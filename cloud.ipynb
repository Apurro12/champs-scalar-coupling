{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.utils.data\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from fastai import *\n",
    "from fastai.basic_train import *\n",
    "from fastai.basic_data import *\n",
    "from fastai.data_block import *\n",
    "from fastai.torch_core import *\n",
    "from fastai.train import *\n",
    "from fastai.callback import *\n",
    "from fastai.distributed import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(fname, type_index=None):\n",
    "    t  = pd.read_csv(fname)\n",
    "    s  = pd.read_csv('structures.csv')\n",
    "    \n",
    "    has_y = 'scalar_coupling_constant' in t.columns\n",
    "\n",
    "    if has_y:\n",
    "        # atom-atom level\n",
    "        # molecule_name,atom_index_0,atom_index_1,type,fc,sd,pso,dso\n",
    "        scalar_couplings = pd.read_csv('scalar_coupling_contributions.csv') # fc,sd,pso,dso\n",
    "\n",
    "        # atom level\n",
    "        # molecule_name,atom_index,XX,YX,ZX,XY,YY,ZY,XZ,YZ,ZZ\n",
    "        magnetic_shielding = pd.read_csv('magnetic_shielding_tensors.csv')\n",
    "        # molecule_name,atom_index,mulliken_charge\n",
    "        mulliken_charges = pd.read_csv('mulliken_charges.csv')\n",
    "\n",
    "        # molecule level\n",
    "        # molecule_name,X,Y,Z\n",
    "        dipole_moments = pd.read_csv('dipole_moments.csv')\n",
    "        # molecule_name,potential_energy\n",
    "        potential_energy = pd.read_csv('potential_energy.csv')\n",
    "\n",
    "    t['molecule_index'] = pd.factorize(t['molecule_name'])[0] + t['id'].min()\n",
    "    # make sure we use the same indexes in train/test (test needs to provide type_index)\n",
    "    if type_index is not None:\n",
    "        t['type_index'] = pd.factorize(pd.concat([pd.Series(type_index),t['type']]))[0][len(type_index):]\n",
    "    else:\n",
    "        t['type_index'] = pd.factorize(t['type'])[0]\n",
    "    s = pd.concat([s,pd.get_dummies(s['atom'])], axis=1)\n",
    "\n",
    "    max_items = 785836 if has_y else 422550\n",
    "    max_atoms = int(s.atom_index.max() + 1)\n",
    "\n",
    "    if has_y:\n",
    "        contributions = ['fc','sd','pso','dso']\n",
    "        magnetic_tensors = ['XX','YX','ZX','XY','YY','ZY','XZ','YZ','ZZ']\n",
    "        XYZ = ['X','Y','Z']\n",
    "    xyz = ['x', 'y', 'z']\n",
    "    a_hot = ['C','F','H','N','O']\n",
    "    \n",
    "    x_xyz   = np.zeros((max_items,len(xyz),  max_atoms), dtype=np.float32)\n",
    "    x_a_hot = np.zeros((max_items,len(a_hot),max_atoms), dtype=np.float32)\n",
    "    x_type  = np.zeros((max_items,1,         max_atoms), dtype=np.float32)\n",
    "\n",
    "    if has_y:\n",
    "        y_scalar   = np.zeros((max_items,len(contributions)   ,max_atoms), dtype=np.float32)\n",
    "        y_magnetic = np.zeros((max_items,len(magnetic_tensors),max_atoms), dtype=np.float32)\n",
    "        y_mulliken = np.zeros((max_items,1                    ,max_atoms), dtype=np.float32)\n",
    "\n",
    "        y_dipole   = np.zeros((max_items,len(XYZ)), dtype=np.float32)\n",
    "        y_potential= np.zeros((max_items,1              ), dtype=np.float32)\n",
    "\n",
    "        y_magnetic[...] = np.nan\n",
    "        y_mulliken[...] = np.nan\n",
    "    else:\n",
    "        xt_ids = np.zeros((max_items, max_atoms), dtype=np.int32)\n",
    "\n",
    "\n",
    "    m = np.zeros((max_items,), dtype=np.int32)\n",
    "    i = j = 0\n",
    "    \n",
    "    for (m_name, m_index) ,m_group in tqdm(t.groupby(['molecule_name', 'molecule_index'])):\n",
    "        ss = s[s.molecule_name==m_name]\n",
    "        n_atoms = len(ss)\n",
    "        if has_y:\n",
    "            magnetic = magnetic_shielding[\n",
    "                    (magnetic_shielding['molecule_name']==m_name)][magnetic_tensors].values.T\n",
    "\n",
    "            mulliken = mulliken_charges[\n",
    "                    (mulliken_charges['molecule_name']==m_name)]['mulliken_charge'].values.T\n",
    "\n",
    "            scs = scalar_couplings[scalar_couplings['molecule_name']==m_name]\n",
    "            \n",
    "            y_dipole[j,:]= dipole_moments[dipole_moments['molecule_name']==m_name][XYZ].values\n",
    "            y_potential[j,:]=potential_energy[\n",
    "                potential_energy['molecule_name']==m_name]['potential_energy'].values\n",
    "        \n",
    "        for a_name,a_group in m_group.groupby('atom_index_0'):\n",
    "            \n",
    "            ref_a = ss[ss['atom_index']==a_name]\n",
    "            \n",
    "            x_xyz[i] = 0.\n",
    "            x_a_hot[i] = ref_a[a_hot].values.T\n",
    "            x_type[i] = -1\n",
    "\n",
    "            x_xyz[i,:,:n_atoms] = (ss[xyz].values-ref_a[xyz].values).T  # xyz \n",
    "            x_a_hot[i,:,:n_atoms] = ss[a_hot].T                  # a_hot\n",
    "            x_type[i,0,a_group['atom_index_1']] = a_group['type_index']  # type \n",
    "            \n",
    "            if has_y:\n",
    "                y_scalar[i,:,a_group['atom_index_1']] = scs[scs['atom_index_0']==a_name][contributions]\n",
    "                y_magnetic[i,:,:n_atoms] = magnetic\n",
    "                y_mulliken[i,:,:n_atoms] = mulliken\n",
    "            else:\n",
    "                xt_ids[i,a_group['atom_index_1']] = a_group['id']  \n",
    "\n",
    "            m[i] = m_index\n",
    "            i+=1\n",
    "        j += 1\n",
    "    assert i == max_items\n",
    "    print(i,max_items)\n",
    "    if has_y:\n",
    "        return x_xyz,x_a_hot,x_type, m , y_scalar, y_magnetic, y_mulliken, y_dipole, y_potential\n",
    "    else:\n",
    "        return x_xyz,x_a_hot,x_type, m, xt_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fname = Path('train.npz')\n",
    "try:\n",
    "    npzfile = np.load(train_fname)\n",
    "    x_xyz = npzfile['x_xyz']\n",
    "    x_a_hot = npzfile['x_a_hot']\n",
    "    x_type = npzfile['x_type']\n",
    "    y_scalar = npzfile['y_scalar']\n",
    "    y_magnetic = npzfile['y_magnetic']\n",
    "    y_mulliken = npzfile['y_mulliken']\n",
    "    y_dipole = npzfile['y_dipole']\n",
    "    y_potential = npzfile['y_potential']\n",
    "    m = npzfile['m']\n",
    "    max_items, max_atoms = x_xyz.shape[0], x_xyz.shape[-1]\n",
    "except:\n",
    "    x_xyz,x_a_hot,x_type, m , y_scalar, y_magnetic, y_mulliken, y_dipole, y_potential = \\\n",
    "        preprocess(train_fname.with_suffix('.csv'))\n",
    "    np.savez(train_fname, \n",
    "             x_xyz=x_xyz,\n",
    "             x_a_hot=x_a_hot,\n",
    "             x_type=x_type,\n",
    "             y_scalar=y_scalar,\n",
    "             y_magnetic=y_magnetic,\n",
    "             y_mulliken=y_mulliken,\n",
    "             y_dipole=y_dipole,\n",
    "             y_potential=y_potential,\n",
    "             m=m)\n",
    "n_types = int(x_type[~np.isnan(x_type)].max() + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_fname = Path('test.npz')\n",
    "try:\n",
    "    npzfile = np.load(test_fname)\n",
    "    xt_xyz = npzfile['x_xyz']\n",
    "    xt_a_hot = npzfile['x_a_hot']\n",
    "    xt_type = npzfile['x_type']\n",
    "    mt = npzfile['m']\n",
    "    xt_ids = npzfile['x_ids']\n",
    "except:\n",
    "    train_csv = pd.read_csv(train_fname.with_suffix('.csv'))\n",
    "    xt_xyz,xt_a_hot,xt_type,mt,xt_ids = \\\n",
    "        preprocess(test_fname.with_suffix('.csv'), type_index = pd.factorize(train_csv['type'])[1])\n",
    "    np.savez(test_fname, \n",
    "             x_xyz=xt_xyz,\n",
    "             x_a_hot=xt_a_hot,\n",
    "             x_type=xt_type,\n",
    "             m=mt,\n",
    "             x_ids=xt_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(785836, 3, 29),\n",
       " (785836, 5, 29),\n",
       " (785836, 1, 29),\n",
       " (785836, 4, 29),\n",
       " (785836, 9, 29),\n",
       " (785836, 1, 29),\n",
       " (785836, 3),\n",
       " (785836, 1),\n",
       " (785836,)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[v.shape for v in [x_xyz,x_a_hot,x_type, y_scalar, y_magnetic, y_mulliken, y_dipole, y_potential, m]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Do norm in FASTAI instead of here\n",
    "# TOCHECK: Filter only valid atoms (otherwise repeated atoms may skew stats)\n",
    "#xyz_mean, xyz_std = x_xyz.mean(axis=(0,2), keepdims=True),  x_xyz.std(axis=(0,2), keepdims=True)\n",
    "#x_xyz  = (x_xyz  - xyz_mean) / xyz_std\n",
    "#xt_xyz = (xt_xyz - xyz_mean) / xyz_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(422550, 3, 29), (422550, 5, 29), (422550, 1, 29), (422550,)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[v.shape for v in [xt_xyz,xt_a_hot,xt_type, mt]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_rate = 0.1\n",
    "# from https://github.com/fxia22/pointnet.pytorch/blob/master/pointnet/model.py\n",
    "\n",
    "class STNkd(nn.Module):\n",
    "    def __init__(self, k=64):\n",
    "        super(STNkd, self).__init__()\n",
    "        self.conv1 = torch.nn.Conv1d(k, 64, 1)\n",
    "        self.conv2 = torch.nn.Conv1d(64, 128, 1)\n",
    "        self.conv3 = torch.nn.Conv1d(128, 1024, 1)\n",
    "        self.fc1 = nn.Linear(1024, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, k*k)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.bn3 = nn.BatchNorm1d(1024)\n",
    "        self.bn4 = nn.BatchNorm1d(512)\n",
    "        self.bn5 = nn.BatchNorm1d(256)\n",
    "\n",
    "        self.k = k\n",
    "\n",
    "    def forward(self, x):\n",
    "        batchsize = x.size()[0]\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = torch.max(x, 2, keepdim=True)[0]\n",
    "        x = x.view(-1, 1024)\n",
    "\n",
    "        x = F.relu(self.bn4(self.fc1(x)))\n",
    "        x = F.relu(self.bn5(self.fc2(x)))\n",
    "        x = self.fc3(x)\n",
    "        #iden = torch.eye(self.k, dtype=x.dtype, device=x.device).view(1,self.k*self.k).expand((batchsize,-1))\n",
    "        #x = x + iden\n",
    "        x = x.view(-1, self.k, self.k)\n",
    "        return x\n",
    "\n",
    "class PointNetfeat(nn.Module):\n",
    "    def __init__(self, global_feat = True, input_transform = True, feature_transform = False):\n",
    "        super(PointNetfeat, self).__init__()\n",
    "        self.stn = STNkd(k=3) if input_transform else None\n",
    "        self.conv1 = torch.nn.Conv1d(9, 64, 1)\n",
    "        self.conv2 = torch.nn.Conv1d(64, 128, 1)\n",
    "        self.conv3 = torch.nn.Conv1d(128, 1024, 1)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.bn3 = nn.BatchNorm1d(1024)\n",
    "        self.global_feat = global_feat\n",
    "        self.feature_transform = feature_transform\n",
    "        if self.feature_transform:\n",
    "            self.fstn = STNkd(k=64)\n",
    "\n",
    "    def forward(self, x):\n",
    "        n_pts = x.size()[2]\n",
    "        if self.stn is not None:\n",
    "            x_xyz = x[:,:3,...].clone()\n",
    "            trans = self.stn(x_xyz)\n",
    "            x[:,:3,:] = torch.bmm(trans, x_xyz)\n",
    "        else:\n",
    "            trans = torch.eye(\n",
    "                self.k, dtype=x.dtype, device=x.device).view(1,self.k*self.k).expand((batchsize,-1))\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "\n",
    "        if self.feature_transform:\n",
    "            trans_feat = self.fstn(x)\n",
    "            x = x.transpose(2,1)\n",
    "            x = torch.bmm(x, trans_feat)\n",
    "            x = x.transpose(2,1)\n",
    "        else:\n",
    "            trans_feat = None\n",
    "\n",
    "        pointfeat = x\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.bn3(self.conv3(x))\n",
    "        x = torch.max(x, 2, keepdim=True)[0]\n",
    "        x = x.view(-1, 1024)\n",
    "        if self.global_feat:\n",
    "            return x, trans, trans_feat\n",
    "        else:\n",
    "            x = x.view(-1, 1024, 1).expand(-1, -1, n_pts)\n",
    "            return torch.cat([x, pointfeat], 1), trans, trans_feat\n",
    "    \n",
    "class PointNetDenseReg(nn.Module):\n",
    "    def __init__(self, k = 2, feature_transform=False):\n",
    "        super(PointNetDenseReg, self).__init__()\n",
    "        self.k = k\n",
    "        self.feature_transform=feature_transform\n",
    "        self.feat = PointNetfeat(global_feat=False, feature_transform=feature_transform)\n",
    "        self.conv1 = torch.nn.Conv1d(1088, 512, 1)\n",
    "        self.conv2 = torch.nn.Conv1d(512, 256, 1)\n",
    "        self.conv3 = torch.nn.Conv1d(256, 128, 1)\n",
    "        self.conv4 = torch.nn.Conv1d(128, self.k, 1)\n",
    "        self.bn1 = nn.BatchNorm1d(512)\n",
    "        self.bn2 = nn.BatchNorm1d(256)\n",
    "        self.bn3 = nn.BatchNorm1d(128)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batchsize = x.size()[0]\n",
    "        n_pts = x.size()[2]\n",
    "        x, trans, trans_feat = self.feat(x)\n",
    "        x = F.dropout(x,p=dropout_rate)\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.conv4(x)\n",
    "        #x = x.transpose(2,1).contiguous()\n",
    "        #x = x.view(batchsize, n_pts, self.k)\n",
    "        return x, trans, trans_feat if self.feature_transform else trans\n",
    "\n",
    "def feature_transform_regularizer(trans):\n",
    "    d = trans.size()[1]\n",
    "    batchsize = trans.size()[0]\n",
    "    I = torch.eye(d, dtype=trans.dtype, device=trans.device).unsqueeze(0)\n",
    "    loss = torch.mean(torch.norm(torch.bmm(trans, trans.transpose(2,1)) - I, dim=(1,2)))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoleculeItem(ItemBase):\n",
    "    def __init__(self,i,xyz,a_hot,type): \n",
    "        self.i, self.xyz,self.a_ahot,self.type = i,xyz,a_hot,type\n",
    "        self.data = torch.cat([Tensor(xyz), Tensor(self.a_ahot),Tensor(self.type)], dim=0)\n",
    "    def __str__(self):\n",
    "        # TODO: count n_atoms correctly. \n",
    "        n_atoms = np.count_nonzero(np.sum(np.absolute(self.xyz), axis=0))+1\n",
    "        n_couplings = np.sum((self.type!=-1))\n",
    "        return f'{self.i} {n_atoms} atoms {n_couplings} couplings'\n",
    "    \n",
    "class ScalarCouplingItem(ItemBase):\n",
    "    def __init__(self,scalar,magnetic,mulliken,dipole,potential,**kwargs): \n",
    "        self.scalar,self.magnetic,self.mulliken,self.dipole,self.potential = \\\n",
    "            scalar,magnetic,mulliken,dipole,potential\n",
    "        self.data = Tensor(np.sum(scalar, axis=0))\n",
    "    def __str__(self):\n",
    "        res, spacer, n_couplings = '', '', 0\n",
    "        for s in self.data:\n",
    "            if s==0.: spacer = ' * '\n",
    "            else: \n",
    "                res += f'{spacer}{s:.4f}'\n",
    "                spacer = ' '\n",
    "                n_couplings +=1\n",
    "        return f'{n_couplings}: {res}'\n",
    "    def __hash__(self): return hash(str(self))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LMAEMaskedLoss(input_outputs,target,feature_transform_regularizer_weight=0.1):\n",
    "    input, output, trans, trans_feat = input_outputs\n",
    "    loss = 0.\n",
    "    n = 0\n",
    "    for type in range(n_types):\n",
    "        mask = (input[:,8,:] == type)\n",
    "        if mask.sum() > 0:\n",
    "            _output,_target = output[:,0,:], target\n",
    "            loss += torch.log((_output[mask] - _target[mask]).abs().mean()+1e-9)\n",
    "            n+=1\n",
    "    return loss/n + feature_transform_regularizer(trans)*feature_transform_regularizer_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScalarCouplingList(ItemList):\n",
    "    def __init__(self, items:Iterator, **kwargs):\n",
    "        super().__init__(items, **kwargs)\n",
    "        self.c = 1\n",
    "        self.loss_func = LMAEMaskedLoss\n",
    "\n",
    "    def get(self, i):\n",
    "        o = super().get(i)\n",
    "        return ScalarCouplingItem(*o)\n",
    "\n",
    "    def reconstruct(self,t): return 0; # TODO for viz !!!! ScalarCouplingItem(t.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ItemList(items=(MoleculeItem(i,*v) for i,v in enumerate(zip(x_xyz,x_a_hot,x_type))),\n",
    "                label_cls=ScalarCouplingItem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "_, idx_valid_split = train_test_split(range(m.max()+1), test_size=0.1, random_state=13)\n",
    "idx_valid_split = np.argwhere(np.isin(m, idx_valid_split)).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ItemLists;\n",
       "\n",
       "Train: ItemList (707619 items)\n",
       "0 5 atoms 4 couplings,1 5 atoms 3 couplings,2 5 atoms 2 couplings,3 5 atoms 1 couplings,4 4 atoms 3 couplings\n",
       "Path: .;\n",
       "\n",
       "Valid: ItemList (78217 items)\n",
       "9 8 atoms 7 couplings,10 8 atoms 6 couplings,11 8 atoms 5 couplings,12 8 atoms 4 couplings,13 8 atoms 3 couplings\n",
       "Path: .;\n",
       "\n",
       "Test: None"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.split_by_idx(idx_valid_split)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.label_from_func(\n",
    "    func=lambda o: (y_scalar[o.i], y_magnetic[o.i], y_mulliken[o.i], y_dipole[o.i], y_potential[o.i]),\n",
    "    label_cls=ScalarCouplingList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.databunch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChemCallback(Callback):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def on_epoch_begin(self,**kwargs):\n",
    "        pass # PAVEL -> at some point do learner.opt.clear() or equivalent reset opt internals \n",
    "    def on_batch_begin(self,**kwargs):\n",
    "        \"Save the last_input (i.e. current input) for on_loss_begin_callback\"\n",
    "        self.last_input = kwargs['last_input']\n",
    "    def on_loss_begin(self, last_output:Tuple[Tensor,Tensor,Tensor], **kwargs):\n",
    "        \"Add last_input to last_output, i.e. input to loss function b/c we need it\"\n",
    "        return {'last_output': (self.last_input, *last_output)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "net, learner = None,None\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "net = PointNetDenseReg(k = 1)\n",
    "learner = Learner(data,net, callbacks=[ChemCallback()], \n",
    "                  loss_func=partial(LMAEMaskedLoss,feature_transform_regularizer_weight=0.),\n",
    "                  metrics=[partial(LMAEMaskedLoss,feature_transform_regularizer_weight=0.)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learner = learner.to_parallel()#.to_fp16() # to_parallel/fp_16 does not converge as well. why?\n",
    "data.batch_size = 4096*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IncompatibleKeys(missing_keys=[], unexpected_keys=[])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.model.load_state_dict(\n",
    "    torch.load('train-1.610453val-1.440965.pth', map_location=\"cpu\")['model']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.lr_find()\n",
    "learner.recorder.plot(suggestion=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.fit_one_cycle(1000, max_lr=5e-3)\n",
    "#learner.fit(20, lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.data.add_test(ItemList(items=(MoleculeItem(i,*v) for i,v in enumerate(zip(xt_xyz,xt_a_hot,xt_type)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.recorder.plot_losses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14aa2d29ae1d48ebbfa8dcd304d9fb84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=104), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "              id  scalar_coupling_constant\n",
      "0        4658147                 17.414852\n",
      "1        4658148                191.917786\n",
      "2        4658149                  3.260659\n",
      "3        4658150                181.777100\n",
      "4        4658151                 13.950219\n",
      "5        4658152                 90.523674\n",
      "6        4658153                  2.810464\n",
      "7        4658154                 -7.536467\n",
      "8        4658155                 -9.768256\n",
      "9        4658156                 90.814972\n",
      "10       4658157                  2.518740\n",
      "11       4658158                 -9.729465\n",
      "12       4658159                 83.171028\n",
      "13       4658160                 11.549896\n",
      "14       4658161                  2.576840\n",
      "15       4658162                 91.002342\n",
      "16       4658163                 -7.551643\n",
      "17       4658164                 -9.669776\n",
      "18       4658165                  2.432697\n",
      "19       4658166                 91.323341\n",
      "20       4658167                 -9.777757\n",
      "21       4658168                 11.068941\n",
      "22       4658169                 83.272224\n",
      "23       4658170                105.161613\n",
      "24       4658171                 -1.054094\n",
      "25       4658172                 -1.053154\n",
      "26       4658173                 -3.505310\n",
      "27       4658174                  3.707601\n",
      "28       4658175                  7.555797\n",
      "29       4658176                  7.555571\n",
      "...          ...                       ...\n",
      "2505512  7163659                116.873131\n",
      "2505513  7163660                  2.855494\n",
      "2505514  7163661                  2.110042\n",
      "2505515  7163662                  2.393236\n",
      "2505516  7163663                  1.132965\n",
      "2505517  7163664                  3.037575\n",
      "2505518  7163665                  3.799144\n",
      "2505519  7163666                  0.981376\n",
      "2505520  7163667                  3.556075\n",
      "2505521  7163668                118.164192\n",
      "2505522  7163669                  5.883613\n",
      "2505523  7163670                  2.915696\n",
      "2505524  7163671                  3.804198\n",
      "2505525  7163672                  2.311715\n",
      "2505526  7163673                  4.676326\n",
      "2505527  7163674                  3.813244\n",
      "2505528  7163675                  8.389427\n",
      "2505529  7163676                  3.489488\n",
      "2505530  7163677                 -0.374250\n",
      "2505531  7163678                  0.693751\n",
      "2505532  7163679                101.656670\n",
      "2505533  7163680                  9.149561\n",
      "2505534  7163681                  0.240959\n",
      "2505535  7163682                  0.018198\n",
      "2505536  7163683                  1.664118\n",
      "2505537  7163684                  0.673955\n",
      "2505538  7163685                  2.313497\n",
      "2505539  7163686                  1.604539\n",
      "2505540  7163687                  5.163481\n",
      "2505541  7163688                116.922188\n",
      "\n",
      "[2505542 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "xt_ids = np.load('xt_ids.npy')\n",
    "learner.model.eval()\n",
    "learner.data.batch_size = 4096\n",
    "\n",
    "test_preds = np.zeros((0, 1, 29), dtype=np.float32)\n",
    "\n",
    "for batch_idx, batch in tqdm(enumerate(learner.dl(DatasetType.Test)), total=len(learner.dl(DatasetType.Test))):\n",
    "    _, preds_, _, _ = learner.pred_batch(ds_type=DatasetType.Test, batch=batch)\n",
    "    test_preds = np.concatenate([test_preds, preds_.data.cpu().numpy()], axis = 0)\n",
    "\n",
    "\n",
    "test_preds = np.squeeze(test_preds, 1)\n",
    "sub = dict()\n",
    "\n",
    "ids = xt_ids[xt_ids!=0]\n",
    "preds = test_preds[xt_ids!=0]\n",
    "\n",
    "for k in range(len(ids)):\n",
    "    sub[int(ids[k])] = preds[k]\n",
    "    \n",
    "sub_df = pd.DataFrame(sub.items(), columns=['id', 'scalar_coupling_constant'])\n",
    "print(sub_df)\n",
    "sub_df.to_csv('train-1.610453val-1.440965.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.data.test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('xt_ids.npy', xt_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
