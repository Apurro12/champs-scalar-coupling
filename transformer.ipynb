{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%load_ext autoreload\n",
    "#%autoreload 2\n",
    "import os\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.utils.data\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.nn import Parameter\n",
    "\n",
    "def _load_from_state_dict(self, state_dict, prefix, local_metadata, strict,\n",
    "                          missing_keys, unexpected_keys, error_msgs):\n",
    "    r\"\"\"Copies parameters and buffers from :attr:`state_dict` into only\n",
    "    this module, but not its descendants. This is called on every submodule\n",
    "    in :meth:`~torch.nn.Module.load_state_dict`. Metadata saved for this\n",
    "    module in input :attr:`state_dict` is provided as :attr:`local_metadata`.\n",
    "    For state dicts without metadata, :attr:`local_metadata` is empty.\n",
    "    Subclasses can achieve class-specific backward compatible loading using\n",
    "    the version number at `local_metadata.get(\"version\", None)`.\n",
    "\n",
    "    .. note::\n",
    "        :attr:`state_dict` is not the same object as the input\n",
    "        :attr:`state_dict` to :meth:`~torch.nn.Module.load_state_dict`. So\n",
    "        it can be modified.\n",
    "\n",
    "    Arguments:\n",
    "        state_dict (dict): a dict containing parameters and\n",
    "            persistent buffers.\n",
    "        prefix (str): the prefix for parameters and buffers used in this\n",
    "            module\n",
    "        local_metadata (dict): a dict containing the metadata for this module.\n",
    "            See\n",
    "        strict (bool): whether to strictly enforce that the keys in\n",
    "            :attr:`state_dict` with :attr:`prefix` match the names of\n",
    "            parameters and buffers in this module\n",
    "        missing_keys (list of str): if ``strict=True``, add missing keys to\n",
    "            this list\n",
    "        unexpected_keys (list of str): if ``strict=True``, add unexpected\n",
    "            keys to this list\n",
    "        error_msgs (list of str): error messages should be added to this\n",
    "            list, and will be reported together in\n",
    "            :meth:`~torch.nn.Module.load_state_dict`\n",
    "    \"\"\"\n",
    "    for hook in self._load_state_dict_pre_hooks.values():\n",
    "        hook(state_dict, prefix, local_metadata, strict, missing_keys,\n",
    "             unexpected_keys, error_msgs)\n",
    "\n",
    "    local_name_params = itertools.chain(self._parameters.items(),\n",
    "                                        self._buffers.items())\n",
    "    local_state = {k: v.data for k, v in local_name_params if v is not None}\n",
    "\n",
    "    for name, param in local_state.items():\n",
    "        key = prefix + name\n",
    "        if key in state_dict:\n",
    "            input_param = state_dict[key]\n",
    "\n",
    "            # Backward compatibility: loading 1-dim tensor from 0.3.* to version 0.4+\n",
    "            if len(param.shape) == 0 and len(input_param.shape) == 1:\n",
    "                input_param = input_param[0]\n",
    "\n",
    "            if input_param.shape != param.shape:\n",
    "                # local shape should match the one in checkpoint\n",
    "                error_msgs.append(\n",
    "                    'size mismatch for {}: copying a param with shape {} from checkpoint, '\n",
    "                    'the shape in current model is {}.'.format(\n",
    "                        key, input_param.shape, param.shape))\n",
    "                #if not strict:\n",
    "                #    continue\n",
    "\n",
    "            if isinstance(input_param, Parameter):\n",
    "                # backwards compatibility for serialized parameters\n",
    "                input_param = input_param.data\n",
    "\n",
    "            try:\n",
    "                param.copy_(input_param)\n",
    "            except Exception:\n",
    "                error_msgs.append(\n",
    "                    'While copying the parameter named \"{}\", '\n",
    "                    'whose dimensions in the model are {} and '\n",
    "                    'whose dimensions in the checkpoint are {}.'.format(\n",
    "                        key, param.size(), input_param.size()))\n",
    "                # PG load partially\n",
    "\n",
    "                if len(input_param.size()) == 3:\n",
    "                    error_msgs.append(\n",
    "                        'Partially copying the parameter named \"{}\", '\n",
    "                        'whose dimensions in the model are {} and '\n",
    "                        'whose dimensions in the checkpoint are {}. - trying {}'\n",
    "                        .format(\n",
    "                            key, param.size(), input_param.size(),\n",
    "                            param[:input_param.size()[0], :input_param.size(\n",
    "                            )[1], :input_param.size()[2]].shape))\n",
    "                else:\n",
    "                    error_msgs.append(\n",
    "                        'Partially copying the parameter named \"{}\", '\n",
    "                        'whose dimensions in the model are {} and '\n",
    "                        'whose dimensions in the checkpoint are {}. - trying {}'\n",
    "                        .format(key, param.size(), input_param.size(),\n",
    "                                param[:input_param.size()[0]].shape))\n",
    "\n",
    "                try:\n",
    "                    new_input_param = torch.empty_like(param)\n",
    "                    new_input_param = torch.nn.init.normal_(new_input_param,\n",
    "                                                            mean=input_param.mean(),\n",
    "                                                            std=input_param.std())\n",
    "\n",
    "                    if len(input_param.size()) == 3:\n",
    "                        new_input_param[:input_param.size()[0], :input_param.\n",
    "                                        size()[1], :input_param.size(\n",
    "                                        )[2]] = input_param\n",
    "                    else:\n",
    "                        new_input_param[:input_param.size()[0]] = input_param\n",
    "                    param.copy_(new_input_param)\n",
    "                except Exception as e:\n",
    "                    assert e\n",
    "                    error_msgs.append(\n",
    "                        'Failed to load weights partially {}'.format(e))\n",
    "        elif strict:\n",
    "            missing_keys.append(key)\n",
    "\n",
    "    if strict:\n",
    "        for key in state_dict.keys():\n",
    "            if key.startswith(prefix):\n",
    "                input_name = key[len(prefix):]\n",
    "                input_name = input_name.split(\n",
    "                    '.', 1)[0]  # get the name of param/buffer/child\n",
    "                if input_name not in self._modules and input_name not in local_state:\n",
    "                    unexpected_keys.append(key)\n",
    "\n",
    "def load_state_dict(self, state_dict, strict=True):\n",
    "    r\"\"\"Copies parameters and buffers from :attr:`state_dict` into\n",
    "    this module and its descendants. If :attr:`strict` is ``True``, then\n",
    "    the keys of :attr:`state_dict` must exactly match the keys returned\n",
    "    by this module's :meth:`~torch.nn.Module.state_dict` function.\n",
    "\n",
    "    Arguments:\n",
    "        state_dict (dict): a dict containing parameters and\n",
    "            persistent buffers.\n",
    "        strict (bool, optional): whether to strictly enforce that the keys\n",
    "            in :attr:`state_dict` match the keys returned by this module's\n",
    "            :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\n",
    "\n",
    "    Returns:\n",
    "        ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n",
    "            * **missing_keys** is a list of str containing the missing keys\n",
    "            * **unexpected_keys** is a list of str containing the unexpected keys\n",
    "    \"\"\"\n",
    "    missing_keys = []\n",
    "    unexpected_keys = []\n",
    "    error_msgs = []\n",
    "\n",
    "    # copy state_dict so _load_from_state_dict can modify it\n",
    "    metadata = getattr(state_dict, '_metadata', None)\n",
    "    state_dict = state_dict.copy()\n",
    "    if metadata is not None:\n",
    "        state_dict._metadata = metadata\n",
    "\n",
    "    def load(module, prefix=''):\n",
    "        local_metadata = {} if metadata is None else metadata.get(\n",
    "            prefix[:-1], {})\n",
    "        module._load_from_state_dict(state_dict, prefix, local_metadata, True,\n",
    "                                     missing_keys, unexpected_keys, error_msgs)\n",
    "        for name, child in module._modules.items():\n",
    "            if child is not None:\n",
    "                load(child, prefix + name + '.')\n",
    "                \n",
    "    load(self)\n",
    "\n",
    "    if strict:\n",
    "        if len(unexpected_keys) > 0:\n",
    "            error_msgs.insert(\n",
    "                0, 'Unexpected key(s) in state_dict: {}. '.format(', '.join(\n",
    "                    '\"{}\"'.format(k) for k in unexpected_keys)))\n",
    "        if len(missing_keys) > 0:\n",
    "            error_msgs.insert(\n",
    "                0, 'Missing key(s) in state_dict: {}. '.format(', '.join(\n",
    "                    '\"{}\"'.format(k) for k in missing_keys)))\n",
    "\n",
    "    if strict and len(error_msgs) > 0:\n",
    "        raise RuntimeError(\n",
    "            'Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n",
    "                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nn.Module._load_from_state_dict = _load_from_state_dict\n",
    "nn.Module.load_state_dict = load_state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from fastai import *\n",
    "from fastai.basic_train import *\n",
    "from fastai.basic_data import *\n",
    "from fastai.data_block import *\n",
    "from fastai.torch_core import *\n",
    "from fastai.train import *\n",
    "from fastai.callback import *\n",
    "from fastai.callbacks import *\n",
    "from fastai.distributed import *\n",
    "from fastai.layers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fname_ext = lambda fname,ext: f'{str(fname)[:-4]}{ext}{str(fname)[-4:]}'\n",
    "\n",
    "def preprocess(fname, type_index=None, ext=''):\n",
    "    t  = pd.read_csv(fname_ext(fname,ext))\n",
    "    s  = pd.read_csv('structures.csv')\n",
    "    \n",
    "    has_y = 'scalar_coupling_constant' in t.columns\n",
    "\n",
    "    if has_y:\n",
    "        # atom-atom level\n",
    "        # molecule_name,atom_index_0,atom_index_1,type,fc,sd,pso,dso\n",
    "        scalar_couplings = pd.read_csv(f'scalar_coupling_contributions{ext}.csv') # fc,sd,pso,dso\n",
    "\n",
    "        # atom level\n",
    "        # molecule_name,atom_index,XX,YX,ZX,XY,YY,ZY,XZ,YZ,ZZ\n",
    "        magnetic_shielding = pd.read_csv('magnetic_shielding_tensors.csv')\n",
    "        # molecule_name,atom_index,mulliken_charge\n",
    "        mulliken_charges = pd.read_csv('mulliken_charges.csv')\n",
    "\n",
    "        # molecule level\n",
    "        # molecule_name,X,Y,Z\n",
    "        dipole_moments = pd.read_csv('dipole_moments.csv')\n",
    "        # molecule_name,potential_energy\n",
    "        potential_energy = pd.read_csv('potential_energy.csv')\n",
    "\n",
    "    t['molecule_index'] = pd.factorize(t['molecule_name'])[0] + t['id'].min()\n",
    "    # make sure we use the same indexes in train/test (test needs to provide type_index)\n",
    "    if type_index is not None:\n",
    "        t['type_idx'] = t['type'].apply(lambda x: type_index.index(x) ) # pd.factorize(pd.concat([pd.Series(type_index),t['type']]))[0][len(type_index):]\n",
    "    else:\n",
    "        t['type_idx'] = pd.factorize(t['type'])[0]\n",
    "\n",
    "    s['atom_idx'] = s['atom'].apply(lambda x: atoms.index(x) )\n",
    "\n",
    "    max_items = len(t.groupby(['molecule_name', 'atom_index_0']))# if has_y else 422550\n",
    "    max_atoms = int(s.atom_index.max() + 1)\n",
    "\n",
    "    if has_y:\n",
    "        contributions = ['fc','sd','pso','dso']\n",
    "        magnetic_tensors = ['XX','YX','ZX','XY','YY','ZY','XZ','YZ','ZZ']\n",
    "        XYZ = ['X','Y','Z']\n",
    "    xyz = ['x', 'y', 'z']\n",
    "    \n",
    "    x_xyz   = np.zeros((max_items,len(xyz),  max_atoms), dtype=np.float32)\n",
    "    x_type  = np.zeros((max_items,1,         max_atoms), dtype=np.int8)\n",
    "    x_ext   = np.zeros((max_items,1,         max_atoms), dtype=np.bool_)\n",
    "    x_atom  = np.empty((max_items,1,         max_atoms), dtype=np.int8)\n",
    "    x_atom[:] = -1\n",
    "\n",
    "    if has_y:\n",
    "        y_scalar   = np.zeros((max_items,len(contributions)   ,max_atoms), dtype=np.float32)\n",
    "        y_magnetic = np.zeros((max_items,len(magnetic_tensors),max_atoms), dtype=np.float32)\n",
    "        y_mulliken = np.zeros((max_items,1                    ,max_atoms), dtype=np.float32)\n",
    "\n",
    "        y_dipole   = np.zeros((max_items,len(XYZ)), dtype=np.float32)\n",
    "        y_potential= np.zeros((max_items,1       ), dtype=np.float32)\n",
    "\n",
    "        y_magnetic[...] = np.nan\n",
    "        y_mulliken[...] = np.nan\n",
    "    else:\n",
    "        xt_ids = np.zeros((max_items, max_atoms), dtype=np.int32)\n",
    "\n",
    "    m = np.zeros((max_items,), dtype=np.int32)\n",
    "    i = j = 0\n",
    "    \n",
    "    for (m_name, m_index) ,m_group in tqdm(t.groupby(['molecule_name', 'molecule_index'])):\n",
    "        ss = s[s.molecule_name==m_name]\n",
    "        n_atoms = len(ss)\n",
    "        if has_y:\n",
    "            magnetic = magnetic_shielding[\n",
    "                    (magnetic_shielding['molecule_name']==m_name)][magnetic_tensors].values.T\n",
    "\n",
    "            mulliken = mulliken_charges[\n",
    "                    (mulliken_charges['molecule_name']==m_name)]['mulliken_charge'].values.T\n",
    "\n",
    "            scs = scalar_couplings[scalar_couplings['molecule_name']==m_name]\n",
    "            \n",
    "            y_dipole[j,:]= dipole_moments[dipole_moments['molecule_name']==m_name][XYZ].values\n",
    "            y_potential[j,:]=potential_energy[\n",
    "                potential_energy['molecule_name']==m_name]['potential_energy'].values\n",
    "        \n",
    "        for a_name,a_group in m_group.groupby('atom_index_0'):\n",
    "            \n",
    "            ref_a = ss[ss['atom_index']==a_name]\n",
    "            \n",
    "            x_xyz[i] = 0.\n",
    "            x_type[i] = -1\n",
    "            x_ext[i] =  True\n",
    "            \n",
    "            x_xyz[i,:,:n_atoms] = (ss[xyz].values-ref_a[xyz].values).T  # xyz \n",
    "            x_type[i,0,a_group['atom_index_1']] = a_group['type_idx']  # type \n",
    "            x_ext[i,0,a_group['atom_index_1']] = a_group['ext']  # ext \n",
    "            x_atom[i,:,:n_atoms] = ss['atom_idx'].T                \n",
    "\n",
    "            if has_y:\n",
    "                y_scalar[i,:,a_group['atom_index_1']] = scs[scs['atom_index_0']==a_name][contributions]\n",
    "                y_magnetic[i,:,:n_atoms] = magnetic\n",
    "                y_mulliken[i,:,:n_atoms] = mulliken\n",
    "            else:\n",
    "                xt_ids[i,a_group['atom_index_1']] = a_group['id']  \n",
    "\n",
    "            m[i] = m_index\n",
    "            i+=1\n",
    "        j += 1\n",
    "    assert i == max_items\n",
    "    print(i,max_items)\n",
    "    if has_y:\n",
    "        return x_xyz,x_type,x_ext,x_atom, m, y_scalar, y_magnetic, y_mulliken, y_dipole, y_potential\n",
    "    else:\n",
    "        return x_xyz,x_type,x_ext,x_atom, m, xt_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define where you want to use original training set '' or extended ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ext = '_ext' # or ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load preprocessed or preprocess and save for later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_fname = Path('train.npz')\n",
    "types = ['1JHC', '2JHH', '1JHN', '2JHN', '2JHC', '3JHH', '3JHC', '3JHN'] \n",
    "atoms = 'CFHNO'\n",
    "\n",
    "try:\n",
    "    npzfile = np.load(fname_ext(train_fname, ext))\n",
    "    x_xyz   = npzfile['x_xyz']\n",
    "    x_type  = npzfile['x_type']\n",
    "    x_ext   = npzfile['x_ext']\n",
    "    x_atom  = npzfile['x_atom']\n",
    "\n",
    "    y_scalar    = npzfile['y_scalar']\n",
    "    y_magnetic  = npzfile['y_magnetic']\n",
    "    y_mulliken  = npzfile['y_mulliken']\n",
    "    y_dipole    = npzfile['y_dipole']\n",
    "    y_potential = npzfile['y_potential']\n",
    "    m = npzfile['m']\n",
    "    max_items, max_atoms = x_xyz.shape[0], x_xyz.shape[-1]\n",
    "except:\n",
    "    x_xyz,x_type,x_ext,x_atom, m, y_scalar, y_magnetic, y_mulliken, y_dipole, y_potential = \\\n",
    "        preprocess(train_fname.with_suffix('.csv'), type_index=types, ext=ext)\n",
    "    np.savez(fname_ext(train_fname, ext), \n",
    "             x_xyz=x_xyz,\n",
    "             x_type=x_type,\n",
    "             x_ext=x_ext,\n",
    "             x_atom=x_atom,\n",
    "             y_scalar=y_scalar,\n",
    "             y_magnetic=y_magnetic,\n",
    "             y_mulliken=y_mulliken,\n",
    "             y_dipole=y_dipole,\n",
    "             y_potential=y_potential,\n",
    "             m=m)\n",
    "n_types = int(x_type[~np.isnan(x_type)].max() + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "use_memmap = True\n",
    "try:\n",
    "    load_fn = np.load if not use_memmap else partial(np.lib.format.open_memmap, mode='r')\n",
    "    x_coulombmat = load_fn(f'x_coulombmat32{ext}.npy')\n",
    "except:\n",
    "    x_coulombmat = np.load(f'x_coulombmat{ext}.npy', allow_pickle=True)\n",
    "    x_coulombmat = np.array(x_coulombmat.tolist()).astype(np.float32)\n",
    "    np.save(f'x_coulombmat32{ext}.npy', x_coulombmat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_qm9_mulliken = load_fn(f'x_qm9_mulliken{ext}.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1405126, 3, 29),\n",
       " (1405126, 1, 29),\n",
       " (1405126, 1, 29),\n",
       " (1405126, 1, 29),\n",
       " (1405126, 1, 29),\n",
       " (1405126, 4, 29),\n",
       " (1405126, 9, 29),\n",
       " (1405126, 1, 29),\n",
       " (1405126, 3),\n",
       " (1405126, 1),\n",
       " (1405126,)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[v.shape for v in [x_xyz,x_type,x_ext,x_atom,x_qm9_mulliken, \n",
    "                   y_scalar, y_magnetic, y_mulliken, y_dipole, y_potential, m]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_xyz_mean, x_xyz_std = Tensor(x_xyz.mean(axis=(0,2),keepdims=True)), Tensor(x_xyz.std(axis=(0,2),keepdims=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_qm9_mulliken_mean = Tensor(x_qm9_mulliken.mean(axis=(0,2),keepdims=True))\n",
    "x_qm9_mulliken_std  = Tensor(x_qm9_mulliken.std( axis=(0,2),keepdims=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 1])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_xyz_std.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fastai classes (this should should be done into its own `application` but who has time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MoleculeItem(ItemBase):\n",
    "    def __init__(self,i,xyz,type,ext,atom,qm9_mulliken,coulomb): \n",
    "        self.i,self.xyz,self.type,self.ext, self.atom,self.qm9_mulliken,self.coulomb = \\\n",
    "            i,xyz,type,ext,atom,qm9_mulliken,coulomb\n",
    "        self.data = [Tensor(xyz), LongTensor((type)), \n",
    "                     Tensor(ext), LongTensor((atom)),Tensor(qm9_mulliken), Tensor(coulomb)]\n",
    "    def __str__(self):\n",
    "        # TODO: count n_atoms correctly. \n",
    "        n_atoms = np.count_nonzero(np.sum(np.absolute(self.xyz), axis=0))+1\n",
    "        n_couplings = np.sum((self.type!=-1))\n",
    "        return f'{self.i} {n_atoms} atoms {n_couplings} couplings'\n",
    "    \n",
    "    def apply_tfms(self, tfms:Collection, **kwargs):\n",
    "        x = self.clone()\n",
    "        for t in tfms:\n",
    "            if t: x.data = t(x.data)\n",
    "        return x\n",
    "    \n",
    "    def clone(self):\n",
    "        return self.__class__(self.i,self.xyz,self.type,self.ext,self.atom,self.qm9_mulliken,self.coulomb)\n",
    "    \n",
    "class ScalarCouplingItem(ItemBase):\n",
    "    def __init__(self,scalar,magnetic,mulliken,dipole,potential,**kwargs): \n",
    "        self.scalar,self.magnetic,self.mulliken,self.dipole,self.potential = \\\n",
    "            scalar,magnetic,mulliken,dipole,potential\n",
    "        self.data = (Tensor(scalar), Tensor(magnetic), Tensor(dipole), Tensor(potential))\n",
    "    def __str__(self):\n",
    "        res, spacer, n_couplings = '', '', 0\n",
    "        for s in self.data[0].sum(dim=0):\n",
    "            if s==0.: spacer = ' * '\n",
    "            else: \n",
    "                res += f'{spacer}{s:.4f}'\n",
    "                spacer = ' '\n",
    "                n_couplings +=1\n",
    "        return f'{n_couplings}: {res}'\n",
    "    def __hash__(self): return hash(str(self))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LMAEMaskedLoss(Module):\n",
    "    def __init__(self,\n",
    "                 contrib_w=0., magnetic_w=0., dipole_w=0., potential_w=0., \n",
    "                 types_w = [1]*n_types, return_all=False, proxy_log=torch.log, exclude_ext=False):\n",
    "        self.contrib_w,self.magnetic_w,self.dipole_w,self.potential_w = contrib_w,magnetic_w,dipole_w,potential_w\n",
    "        self.types_w = types_w\n",
    "        self.return_all = return_all\n",
    "        self.proxy_log = proxy_log\n",
    "        self.exclude_ext = exclude_ext\n",
    "    \n",
    "    def forward(self, input_outputs, t_scalar, t_magnetic, t_dipole, t_potential):    \n",
    "        type, ext, p_scalar, p_magnetic, p_dipole, p_potential = input_outputs\n",
    "        loss = 0.\n",
    "        n = 0\n",
    "        j_loss = [0] * n_types\n",
    "        for t in range(n_types):\n",
    "            mask = (type == t).squeeze(1) if not self.exclude_ext else ((type == t) & (ext == 0)).squeeze(1)\n",
    "            if mask.sum() > 0:\n",
    "                _output,_target = p_scalar.transpose(1,2)[mask], t_scalar.transpose(1,2)[mask] # scalars at the end\n",
    "                # LMAE scalar\n",
    "                s_loss = self.proxy_log((_output.sum(dim=-1) - _target.sum(dim=-1)).abs().mean()+1e-9)\n",
    "                loss += self.types_w[t] * s_loss\n",
    "                j_loss[t] += s_loss\n",
    "                # LMAE scalar contributions\n",
    "                for i_contrib in range(_output.shape[-1]):\n",
    "                    loss += self.contrib_w * \\\n",
    "                        self.proxy_log((_output[...,i_contrib] - _target[...,i_contrib]).abs().mean()+1e-9)\n",
    "                n+=1\n",
    "        loss /= n\n",
    "        \n",
    "        if self.magnetic_w > 0:\n",
    "            mask = ~torch.isnan(t_magnetic)\n",
    "            loss += self.magnetic_w * MSELossFlat()(p_magnetic[mask], t_magnetic[mask])\n",
    "            \n",
    "        if self.dipole_w    > 0: loss += self.dipole_w    * MSELossFlat()(p_dipole,    t_dipole)\n",
    "        if self.potential_w > 0: loss += self.potential_w * MSELossFlat()(p_potential, t_potential)\n",
    "\n",
    "        return loss if not self.return_all else (loss, *j_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ScalarCouplingList(ItemList):\n",
    "    def __init__(self, items:Iterator, **kwargs):\n",
    "        super().__init__(items, **kwargs)\n",
    "        self.loss_func = LMAEMaskedLoss\n",
    "\n",
    "    def get(self, i):\n",
    "        o = super().get(i)\n",
    "        return ScalarCouplingItem(*o)\n",
    "\n",
    "    def reconstruct(self,t): return 0; # TODO for viz !!!! ScalarCouplingItem(t.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quaterions allow us to rotate 3d points randoming with a nice uniform distribution of 3 numbers hece we use them, however it's still to be seen if are useful here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/facebookresearch/QuaterNet/blob/master/common/quaternion.py\n",
    "def qrot(q, v):\n",
    "    \"\"\"\n",
    "    Rotate vector(s) v about the rotation described by quaternion(s) q.\n",
    "    Expects a tensor of shape (*, 4) for q and a tensor of shape (*, 3) for v,\n",
    "    where * denotes any number of dimensions.\n",
    "    Returns a tensor of shape (*, 3).\n",
    "    \"\"\"\n",
    "    assert q.shape[-1] == 4\n",
    "    assert v.shape[-1] == 3\n",
    "    assert q.shape[:-1] == v.shape[:-1]\n",
    "    \n",
    "    original_shape = list(v.shape)\n",
    "    q = q.view(-1, 4)\n",
    "    v = v.view(-1, 3)\n",
    "    \n",
    "    qvec = q[:, 1:]\n",
    "    uv = torch.cross(qvec, v, dim=1)\n",
    "    uuv = torch.cross(qvec, uv, dim=1)\n",
    "    return (v + 2 * (q[:, :1] * uv + uuv)).view(original_shape)\n",
    "\n",
    "def random_rotation(data):\n",
    "    x_xyz = data[0].transpose(0,1)\n",
    "    r = torch.rand(3)\n",
    "    sq1_v1,sqv1,v2_2pi,v3_2pi = torch.sqrt(1-r[:1]),torch.sqrt(r[:1]),2*math.pi*r[1:2],2*math.pi*r[2:3]\n",
    "    q = torch.cat([sq1_v1*torch.sin(v2_2pi), sq1_v1*torch.cos(v2_2pi), \n",
    "                   sqv1  *torch.sin(v3_2pi), sqv1  *torch.cos(v3_2pi)], dim=0).unsqueeze(0)\n",
    "    x_xyz = qrot(q.expand(x_xyz.shape[0],-1), x_xyz).squeeze(0).transpose(0,1)\n",
    "    return (x_xyz, *data[1:])\n",
    "\n",
    "def normalize(data):\n",
    "    sq = False\n",
    "    if data[0].ndim < 3:\n",
    "        data[0].unsqueeze_(0)\n",
    "        data[4].unsqueeze_(0)\n",
    "        sq = True\n",
    "    x_xyz      = (data[0] - x_xyz_mean)          / x_xyz_std\n",
    "    x_mulliken = (data[4] - x_qm9_mulliken_mean) / x_qm9_mulliken_std\n",
    "    if sq:\n",
    "        x_xyz.squeeze_(0)\n",
    "        x_mulliken.squeeze_(0)\n",
    "    return (x_xyz, data[1],data[2],data[3],x_mulliken,data[5])\n",
    "\n",
    "def canonize(data):\n",
    "    xyz,type,ext,atom,mulliken,coulomb = data\n",
    "    mask = (atom == -1).squeeze(0)\n",
    "    mask_atoms = ~mask.unsqueeze(0)\n",
    "    n_atoms = mask_atoms.sum()\n",
    "    i = torch.nonzero(type.squeeze(0) == -1)[0] # pick first one w/o j-coupling\n",
    "    xyz[:,mask], type[:,mask],ext[:,mask],atom[:,mask],mulliken[:,mask] = \\\n",
    "        0,-1,1,-1,0\n",
    "#        xyz[:,i],type[:,i],   ext[:,i],   atom[:,i],   mulliken[:,i]\n",
    "    return (xyz,type,ext,atom,mulliken,coulomb, mask_atoms, n_atoms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build `data` bunch etc. for fastai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = ItemList(items=(MoleculeItem(i,*v) for i,v in \n",
    "                       enumerate(zip(x_xyz,x_type,x_ext,x_atom,x_qm9_mulliken,x_coulombmat))),\n",
    "                label_cls=ScalarCouplingItem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "_, idx_valid_split = train_test_split(range(m.max()+1), test_size=0.1, random_state=13)\n",
    "idx_valid_split = np.argwhere(np.isin(m, idx_valid_split)).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true,
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [],
   "source": [
    "data = data.split_by_idx(idx_valid_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = data.label_from_func(\n",
    "    func=lambda o: (y_scalar[o.i], y_magnetic[o.i], y_mulliken[o.i], y_dipole[o.i], y_potential[o.i]),\n",
    "    label_cls=ScalarCouplingList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    xt_coulombmat = load_fn(f'xt_coulombmat32{ext}.npy')\n",
    "except:\n",
    "    xt_coulombmat = np.load(f'xt_coulombmat{ext}.npy', allow_pickle=True)\n",
    "    xt_coulombmat = np.array(xt_coulombmat.tolist()).astype(np.float32)\n",
    "    np.save(f'xt_coulombmat32{ext}.npy', xt_coulombmat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms = [normalize, canonize]\n",
    "tta_tfms = list(tfms)\n",
    "#tta_tfms.insert(0,random_rotation)\n",
    "data = data.transform((tta_tfms, tfms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data=data.databunch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "Whole model here, self-contained (needs some cleanup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LMAEMetric(LearnerCallback):\n",
    "    _order=-20 # Needs to run before the recorder\n",
    "    def __init__(self, learn, val_only=True):\n",
    "        super().__init__(learn)\n",
    "        self.val_only=val_only\n",
    "        self.metric = LMAEMaskedLoss(return_all=True, exclude_ext=True)\n",
    "\n",
    "    def on_train_begin(self, **kwargs):\n",
    "        if not self.val_only: self.learn.recorder.add_metric_names(['tLMAE'])\n",
    "        self.learn.recorder.add_metric_names(['👉🏻LMAE👈🏻'] + [f'lmae{i}' for i in range(n_types)])\n",
    "            \n",
    "    def on_batch_end(self, train, last_output, last_target, **kwargs):\n",
    "        if self.val_only and train: return \n",
    "        preds,targs = self.preds[int(train)], self.targs[int(train)] # 0 val 1 train\n",
    "        if preds is None:\n",
    "            targs, preds = listify(last_target), listify(last_output)\n",
    "            targs,preds = [t.detach() for t in targs],[t.detach() for t in preds]\n",
    "        else:\n",
    "            for i,(o,t) in enumerate(zip(last_output, last_target)):\n",
    "                preds[i] = torch.cat([preds[i], o.detach()], dim=0)\n",
    "                targs[i] = torch.cat([targs[i], t.detach()], dim=0)\n",
    "        self.preds[int(train)], self.targs[int(train)] = preds,targs\n",
    "        \n",
    "    def on_epoch_begin(self, **kwargs):\n",
    "        self.targs, self.preds = [None, None], [None, None]\n",
    "\n",
    "    def on_epoch_end(self, last_metrics, **kwargs):\n",
    "        mets = []\n",
    "        if self.preds[1]: mets.append(self.metric.forward(self.preds[1], *self.targs[1])[0]) # just tLMAE\n",
    "        if self.preds[0]: mets.extend(self.metric.forward(self.preds[0], *self.targs[0]))\n",
    "        return add_metrics(last_metrics, mets) if mets else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "Activation = Enum('Activation', 'ReLU Swish GeLU')\n",
    "\n",
    "class PositionalEncoding(Module):\n",
    "    \"Encode the position with a sinusoid.\"\n",
    "    def __init__(self, d:int): self.register_buffer('freq', 1 / (10000 ** (torch.arange(0., d, 2.)/d)))\n",
    "\n",
    "    def forward(self, pos:Tensor):\n",
    "        inp = torch.ger(pos, self.freq)\n",
    "        enc = torch.cat([inp.sin(), inp.cos()], dim=-1)\n",
    "        return enc\n",
    "\n",
    "class GeLU(Module):\n",
    "    def forward(self, x): return 0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n",
    "\n",
    "class Swish(Module):\n",
    "    def forward(self, x): return x * torch.sigmoid(x)\n",
    "\n",
    "_activ_func = {Activation.ReLU:nn.ReLU(inplace=True), Activation.GeLU:GeLU(), Activation.Swish: Swish()}\n",
    "\n",
    "def feed_forward(d_model:int, d_ff:int, ff_p:float=0., act:Activation=Activation.ReLU, double_drop:bool=True):\n",
    "    layers = [nn.Linear(d_model, d_ff), _activ_func[act]]\n",
    "    if double_drop: layers.append(nn.Dropout(ff_p))\n",
    "    return SequentialEx(*layers, nn.Linear(d_ff, d_model), nn.Dropout(ff_p), MergeLayer(), nn.LayerNorm(d_model))\n",
    "\n",
    "class MultiHeadAttention(Module):\n",
    "    \"MutiHeadAttention.\"\n",
    "    def __init__(self, n_heads:int, d_model:int, d_head:int=None, resid_p:float=0., attn_p:float=0., bias:bool=True,\n",
    "                 scale:bool=True):\n",
    "        d_head = ifnone(d_head, d_model//n_heads)\n",
    "        self.n_heads,self.d_head,self.scale = n_heads,d_head,scale\n",
    "        self.attention = nn.Linear(d_model, 3 * n_heads * d_head, bias=bias)\n",
    "        self.out = nn.Linear(n_heads * d_head, d_model, bias=bias)\n",
    "        self.drop_att,self.drop_res = nn.Dropout(attn_p),nn.Dropout(resid_p)\n",
    "        self.ln = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x:Tensor, mask:Tensor=None, **kwargs):\n",
    "        return self.ln(x + self.drop_res(self.out(self._apply_attention(x, mask=mask, **kwargs))))\n",
    "\n",
    "    def _apply_attention(self, x:Tensor, mask:Tensor=None):\n",
    "        bs,x_len = x.size(0),x.size(1)\n",
    "        wq,wk,wv = torch.chunk(self.attention(x), 3, dim=-1)\n",
    "        wq,wk,wv = map(lambda x:x.view(bs, x.size(1), self.n_heads, self.d_head), (wq,wk,wv))\n",
    "        wq,wk,wv = wq.permute(0, 2, 1, 3),wk.permute(0, 2, 3, 1),wv.permute(0, 2, 1, 3)\n",
    "        attn_score = torch.matmul(wq, wk)\n",
    "        if self.scale: attn_score.div_(self.d_head ** 0.5)\n",
    "        if mask is not None:\n",
    "            attn_score = attn_score.float().masked_fill(mask, -float('inf')).type_as(attn_score)\n",
    "        attn_prob = self.drop_att(F.softmax(attn_score, dim=-1))\n",
    "        attn_vec = torch.matmul(attn_prob, wv)\n",
    "        return attn_vec.permute(0, 2, 1, 3).contiguous().contiguous().view(bs, x_len, -1)\n",
    "\n",
    "    def _attention_einsum(self, x, mask=None):\n",
    "        # Permute and matmul is a little bit faster but this implementation is more readable\n",
    "        bs,x_len = x.size(0),x.size(1)\n",
    "        wq,wk,wv = torch.chunk(self.attention(x), 3, dim=-1)\n",
    "        wq,wk,wv = map(lambda x:x.view(bs, x.size(1), self.n_heads, self.d_head), (wq,wk,wv))\n",
    "        attn_score = torch.einsum('bind,bjnd->bijn', (wq, wk))\n",
    "        if self.scale: attn_score.mul_(1/(self.d_head ** 0.5))\n",
    "        if mask is not None:\n",
    "            attn_score = attn_score.float().masked_fill(mask, -float('-inf')).type_as(attn_score)\n",
    "        attn_prob = self.drop_att(F.softmax(attn_score, dim=2))\n",
    "        attn_vec = torch.einsum('bijn,bjnd->bind', (attn_prob, wv))\n",
    "        return attn_vec.contiguous().view(bs, x_len, -1)\n",
    "\n",
    "#def _line_shift1(x:Tensor, mask:bool=False):\n",
    "#    \"Shift the line i of `x` by p-i elements to the left, is `mask` puts 0s on the diagonal.\"\n",
    "#    bs,n,p,nh = x.size()\n",
    "#    x_pad = torch.cat([x.new_zeros(bs,n,1,nh), x], dim=2)\n",
    "#    x_shift = x_pad.view(bs,p + 1,n,nh)[:,1:].view_as(x)\n",
    "#    if mask: x_shift.mul_(torch.tril(x.new_ones(n,p), p-n)[None,:,:,None])\n",
    "#    return x_shift\n",
    "\n",
    "def _line_shift(x:Tensor, mask:bool=False):\n",
    "    \"Shift the line i of `x` by p-i elements to the left, is `mask` puts 0s on the diagonal.\"\n",
    "    bs,nh,n,p = x.size()\n",
    "    x_pad = torch.cat([x.new_zeros(bs,nh,n,1), x], dim=3)\n",
    "    x_shift = x_pad.view(bs,nh,p + 1,n)[:,:,1:].view_as(x)\n",
    "    if mask: x_shift.mul_(torch.tril(x.new_ones(n,p), p-n)[None,None,])\n",
    "    return x_shift\n",
    "\n",
    "class MultiHeadRelativeAttention(MultiHeadAttention):\n",
    "    \"MutiHeadAttention with relative positional encoding.\"\n",
    "\n",
    "    def __init__(self, n_heads:int, d_model:int, d_head:int, resid_p:float=0., attn_p:float=0., bias:bool=True,\n",
    "                 scale:bool=True):\n",
    "        super().__init__(n_heads, d_model, d_head, resid_p=resid_p, attn_p=attn_p, bias=bias, scale=scale)\n",
    "        self.r_attn = nn.Linear(d_model, n_heads * d_head, bias=bias)\n",
    "\n",
    "    def _apply_attention(self, x:Tensor, r:Tensor=None, u:Tensor=None, v:Tensor=None, mask:Tensor=None, mem:Tensor=None):\n",
    "        #Notations from the paper: x input, r vector of relative distance between two elements, u et v learnable\n",
    "        #parameters of the model common between all layers, mask to avoid cheating and mem the previous hidden states.\n",
    "        bs,x_len,seq_len = x.size(0),x.size(1),r.size(0)\n",
    "        context = x if mem is None else torch.cat([mem, x], dim=1)\n",
    "        wq,wk,wv = torch.chunk(self.attention(context), 3, dim=-1)\n",
    "        wq = wq[:,-x_len:]\n",
    "        wq,wk,wv = map(lambda x:x.view(bs, x.size(1), self.n_heads, self.d_head), (wq,wk,wv))\n",
    "        wq,wk,wv = wq.permute(0, 2, 1, 3),wk.permute(0, 2, 3, 1),wv.permute(0, 2, 1, 3)\n",
    "        wkr = self.r_attn(r)\n",
    "        wkr = wkr.view(seq_len, self.n_heads, self.d_head)\n",
    "        wkr = wkr.permute(1,2,0)\n",
    "        #### compute attention score (AC is (a) + (c) and BS is (b) + (d) in the paper)\n",
    "        AC = torch.matmul(wq+u,wk)\n",
    "        BD = _line_shift(torch.matmul(wq+v, wkr))\n",
    "        if self.scale: attn_score = (AC + BD).mul_(1/(self.d_head ** 0.5))\n",
    "        if mask is not None:\n",
    "            attn_score = attn_score.float().masked_fill(mask, -float('inf')).type_as(attn_score)\n",
    "        attn_prob = self.drop_att(F.softmax(attn_score, dim=-1))\n",
    "        attn_vec = torch.matmul(attn_prob, wv)\n",
    "        return attn_vec.permute(0, 2, 1, 3).contiguous().view(bs, x_len, -1)\n",
    "\n",
    "    def _attention_einsum(self, x:Tensor, r:Tensor=None, u:Tensor=None, v:Tensor=None, mask:Tensor=None, mem:Tensor=None):\n",
    "        # Permute and matmul is a little bit faster but this implementation is more readable\n",
    "        bs,x_len,seq_len = x.size(0),x.size(1),r.size(0)\n",
    "        context = x if mem is None else torch.cat([mem, x], dim=1)\n",
    "        wq,wk,wv = torch.chunk(self.attention(context), 3, dim=-1)\n",
    "        wq = wq[:,-x_len:]\n",
    "        wkr = self.r_attn(r)\n",
    "        wq,wk,wv = map(lambda x:x.view(bs, x.size(1), self.n_heads, self.d_head), (wq,wk,wv))\n",
    "        wkr = wkr.view(seq_len, self.n_heads, self.d_head)\n",
    "        #### compute attention score (AC is (a) + (c) and BS is (b) + (d) in the paper)\n",
    "        AC = torch.einsum('bind,bjnd->bijn', (wq+u, wk))\n",
    "        BD = _line_shift1(torch.einsum('bind,jnd->bijn', (wq+v, wkr)))\n",
    "        attn_score = (AC + BD).mul_(1/(self.d_head ** 0.5))\n",
    "        if mask is not None:\n",
    "            attn_score = attn_score.float().masked_fill(mask, -float('inf')).type_as(attn_score)\n",
    "        attn_prob = self.drop_att(F.softmax(attn_score, dim=2))\n",
    "        attn_vec = torch.einsum('bijn,bjnd->bind', (attn_prob, wv))\n",
    "        return attn_vec.contiguous().view(bs, x_len, -1)\n",
    "\n",
    "class DecoderLayer(Module):\n",
    "    \"Basic block of a Transformer model.\"\n",
    "    #Can't use Sequential directly cause more than one input...\n",
    "    def __init__(self, n_heads:int, d_model:int, d_head:int, d_inner:int, resid_p:float=0., attn_p:float=0., ff_p:float=0.,\n",
    "                 bias:bool=True, scale:bool=True, act:Activation=Activation.ReLU, double_drop:bool=True,\n",
    "                 attn_cls:Callable=MultiHeadAttention):\n",
    "        self.mhra = attn_cls(n_heads, d_model, d_head, resid_p=resid_p, attn_p=attn_p, bias=bias, scale=scale)\n",
    "        self.ff   = feed_forward(d_model, d_inner, ff_p=ff_p, act=act, double_drop=double_drop)\n",
    "\n",
    "    def forward(self, x:Tensor, mask:Tensor=None, **kwargs): return self.ff(self.mhra(x, mask=mask, **kwargs))\n",
    "\n",
    "class Transformer(Module):\n",
    "    \"Transformer model: https://arxiv.org/abs/1706.03762.\"\n",
    "    def __init__(self, n_layers:int, n_heads:int, d_model:int, d_head:int, d_inner:int,\n",
    "                 resid_p:float=0., attn_p:float=0., ff_p:float=0., embed_p:float=0., bias:bool=True, scale:bool=True,\n",
    "                 act:Activation=Activation.ReLU, double_drop:bool=True, attn_cls:Callable=MultiHeadAttention,\n",
    "                 learned_pos_enc:bool=True, mask:bool=True):\n",
    "        self.mask = mask\n",
    "        #self.encoder = nn.Embedding(vocab_sz, d_model)\n",
    "        #self.pos_enc = nn.Embedding(ctx_len, d_model) if learned_pos_enc else PositionalEncoding(d_model)\n",
    "        self.drop_emb = nn.Dropout(embed_p)\n",
    "        self.layers = nn.ModuleList([DecoderLayer(n_heads, d_model, d_head, d_inner, resid_p=resid_p, attn_p=attn_p,\n",
    "                      ff_p=ff_p, bias=bias, scale=scale, act=act, double_drop=double_drop,\n",
    "                      attn_cls=attn_cls) for k in range(n_layers)])\n",
    "\n",
    "    def reset(self): pass\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        #bs, x_len = x.size()\n",
    "        #pos = torch.arange(0, x_len, device=x.device, dtype=x.dtype)\n",
    "        inp = self.drop_emb(x)# + self.pos_enc(pos)[None]) #.mul_(self.d_model ** 0.5)\n",
    "        #mask = None #torch.triu(x.new_ones(x_len, x_len), diagonal=1).byte()[None,None] if self.mask else None\n",
    "        #[None,:,:None] for einsum implementation of attention\n",
    "        for layer in self.layers: inp = layer(inp, mask=mask)\n",
    "        return inp #For the LinearDecoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AtomTransformer(Module):\n",
    "    def __init__(self,  n_heads,d_model, d_head=None, **kwargs):\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        d_head = ifnone(d_head, d_model//n_heads)\n",
    "        self.transformer = Transformer(n_heads=n_heads,d_model=d_model, d_head=d_head, **kwargs)\n",
    "        \n",
    "        self.scalar    = nn.Conv1d(d_model, 4, 1)\n",
    "        self.magnetic  = nn.Conv1d(d_model, 9, 1)\n",
    "        self.dipole    = nn.Linear(d_model*max_atoms, 3)\n",
    "        self.potential = nn.Linear(d_model*max_atoms, 1)\n",
    "        \n",
    "        n_atom_embedding = d_model//2\n",
    "        n_type_embedding = d_model - n_atom_embedding - 6\n",
    "        self.type_embedding = nn.Embedding(len(types)+1,n_type_embedding)\n",
    "        self.atom_embedding = nn.Embedding(len(atoms)+1,n_atom_embedding)\n",
    "    \n",
    "        #n_pos_encoder = d_model - n_type_embedding - n_atom_embedding\n",
    "        #self.pos_encoder = nn.Sequential(\n",
    "        #    nn.Conv1d(3+1+1,n_pos_encoder, 1), nn.ReLU(), nn.BatchNorm1d(n_pos_encoder),\n",
    "        #)\n",
    "        \n",
    "    def forward(self,xyz,type,ext,atom,mulliken,coulomb, mask_atoms, n_atoms):\n",
    "        bs, _, n_pts = xyz.shape        \n",
    "        t = self.type_embedding((type+1).squeeze(1)) #* math.sqrt(self.d_model) #.transpose(1,2)\n",
    "        a = self.atom_embedding((atom+1).squeeze(1)) #* math.sqrt(self.d_model) #.transpose(1,2)\n",
    "        \n",
    "        x = torch.cat([xyz, mulliken, ext, mask_atoms.float()], dim=1) #* math.sqrt(self.d_model)        \n",
    "        #x = self.pos_encoder(x).transpose(1,2)\n",
    "        \n",
    "        x = torch.cat([x.transpose(1,2), t, a], dim=-1) \n",
    "\n",
    "        mask = None #(coulomb > 0).unsqueeze(1)\n",
    "        #mask = torch.triu(x.new_ones(max_atoms, max_atoms), diagonal=1).byte()[None,:,:,None]#[None,None] \n",
    "        #print(mask.shape, mask)\n",
    "        x = self.transformer(x, mask).transpose(1,2).contiguous()\n",
    "        scalar    = self.scalar(x)\n",
    "        magnetic  = self.magnetic(x) \n",
    "        dipole    = self.dipole(x.view(bs,-1))\n",
    "        potential = self.potential(x.view(bs,-1))\n",
    "                \n",
    "        return type,ext,scalar,magnetic,dipole,potential\n",
    "    \n",
    "    def reset(self): pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This callback allows to insert multiple stateful (not averaged) metrics in one pass. Addditionally we could add metrics for train if we want to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model instantiation: where's all your TPUs/GPUs when you need a decent hyperparam sweep?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "net, learner = None,None\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "net = AtomTransformer(n_layers=6, n_heads=8,d_model=128,d_inner=2048)\n",
    "learner = Learner(data,net, loss_func=LMAEMaskedLoss(),)\n",
    "\n",
    "learner.callbacks.append(LMAEMetric(learner))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AtomTransformer\n",
       "======================================================================\n",
       "Layer (type)         Output Shape         Param #    Trainable \n",
       "======================================================================\n",
       "Dropout              [29, 128]            0          False     \n",
       "______________________________________________________________________\n",
       "Linear               [29, 384]            49,536     True      \n",
       "______________________________________________________________________\n",
       "Linear               [29, 128]            16,512     True      \n",
       "______________________________________________________________________\n",
       "Dropout              [8, 29, 29]          0          False     \n",
       "______________________________________________________________________\n",
       "Dropout              [29, 128]            0          False     \n",
       "______________________________________________________________________\n",
       "LayerNorm            [29, 128]            256        True      \n",
       "______________________________________________________________________\n",
       "Linear               [29, 2048]           264,192    True      \n",
       "______________________________________________________________________\n",
       "ReLU                 [29, 2048]           0          False     \n",
       "______________________________________________________________________\n",
       "Dropout              [29, 2048]           0          False     \n",
       "______________________________________________________________________\n",
       "Linear               [29, 128]            262,272    True      \n",
       "______________________________________________________________________\n",
       "Dropout              [29, 128]            0          False     \n",
       "______________________________________________________________________\n",
       "MergeLayer           [29, 128]            0          False     \n",
       "______________________________________________________________________\n",
       "LayerNorm            [29, 128]            256        True      \n",
       "______________________________________________________________________\n",
       "Linear               [29, 384]            49,536     True      \n",
       "______________________________________________________________________\n",
       "Linear               [29, 128]            16,512     True      \n",
       "______________________________________________________________________\n",
       "Dropout              [8, 29, 29]          0          False     \n",
       "______________________________________________________________________\n",
       "Dropout              [29, 128]            0          False     \n",
       "______________________________________________________________________\n",
       "LayerNorm            [29, 128]            256        True      \n",
       "______________________________________________________________________\n",
       "Linear               [29, 2048]           264,192    True      \n",
       "______________________________________________________________________\n",
       "ReLU                 [29, 2048]           0          False     \n",
       "______________________________________________________________________\n",
       "Dropout              [29, 2048]           0          False     \n",
       "______________________________________________________________________\n",
       "Linear               [29, 128]            262,272    True      \n",
       "______________________________________________________________________\n",
       "Dropout              [29, 128]            0          False     \n",
       "______________________________________________________________________\n",
       "MergeLayer           [29, 128]            0          False     \n",
       "______________________________________________________________________\n",
       "LayerNorm            [29, 128]            256        True      \n",
       "______________________________________________________________________\n",
       "Linear               [29, 384]            49,536     True      \n",
       "______________________________________________________________________\n",
       "Linear               [29, 128]            16,512     True      \n",
       "______________________________________________________________________\n",
       "Dropout              [8, 29, 29]          0          False     \n",
       "______________________________________________________________________\n",
       "Dropout              [29, 128]            0          False     \n",
       "______________________________________________________________________\n",
       "LayerNorm            [29, 128]            256        True      \n",
       "______________________________________________________________________\n",
       "Linear               [29, 2048]           264,192    True      \n",
       "______________________________________________________________________\n",
       "ReLU                 [29, 2048]           0          False     \n",
       "______________________________________________________________________\n",
       "Dropout              [29, 2048]           0          False     \n",
       "______________________________________________________________________\n",
       "Linear               [29, 128]            262,272    True      \n",
       "______________________________________________________________________\n",
       "Dropout              [29, 128]            0          False     \n",
       "______________________________________________________________________\n",
       "MergeLayer           [29, 128]            0          False     \n",
       "______________________________________________________________________\n",
       "LayerNorm            [29, 128]            256        True      \n",
       "______________________________________________________________________\n",
       "Linear               [29, 384]            49,536     True      \n",
       "______________________________________________________________________\n",
       "Linear               [29, 128]            16,512     True      \n",
       "______________________________________________________________________\n",
       "Dropout              [8, 29, 29]          0          False     \n",
       "______________________________________________________________________\n",
       "Dropout              [29, 128]            0          False     \n",
       "______________________________________________________________________\n",
       "LayerNorm            [29, 128]            256        True      \n",
       "______________________________________________________________________\n",
       "Linear               [29, 2048]           264,192    True      \n",
       "______________________________________________________________________\n",
       "ReLU                 [29, 2048]           0          False     \n",
       "______________________________________________________________________\n",
       "Dropout              [29, 2048]           0          False     \n",
       "______________________________________________________________________\n",
       "Linear               [29, 128]            262,272    True      \n",
       "______________________________________________________________________\n",
       "Dropout              [29, 128]            0          False     \n",
       "______________________________________________________________________\n",
       "MergeLayer           [29, 128]            0          False     \n",
       "______________________________________________________________________\n",
       "LayerNorm            [29, 128]            256        True      \n",
       "______________________________________________________________________\n",
       "Linear               [29, 384]            49,536     True      \n",
       "______________________________________________________________________\n",
       "Linear               [29, 128]            16,512     True      \n",
       "______________________________________________________________________\n",
       "Dropout              [8, 29, 29]          0          False     \n",
       "______________________________________________________________________\n",
       "Dropout              [29, 128]            0          False     \n",
       "______________________________________________________________________\n",
       "LayerNorm            [29, 128]            256        True      \n",
       "______________________________________________________________________\n",
       "Linear               [29, 2048]           264,192    True      \n",
       "______________________________________________________________________\n",
       "ReLU                 [29, 2048]           0          False     \n",
       "______________________________________________________________________\n",
       "Dropout              [29, 2048]           0          False     \n",
       "______________________________________________________________________\n",
       "Linear               [29, 128]            262,272    True      \n",
       "______________________________________________________________________\n",
       "Dropout              [29, 128]            0          False     \n",
       "______________________________________________________________________\n",
       "MergeLayer           [29, 128]            0          False     \n",
       "______________________________________________________________________\n",
       "LayerNorm            [29, 128]            256        True      \n",
       "______________________________________________________________________\n",
       "Linear               [29, 384]            49,536     True      \n",
       "______________________________________________________________________\n",
       "Linear               [29, 128]            16,512     True      \n",
       "______________________________________________________________________\n",
       "Dropout              [8, 29, 29]          0          False     \n",
       "______________________________________________________________________\n",
       "Dropout              [29, 128]            0          False     \n",
       "______________________________________________________________________\n",
       "LayerNorm            [29, 128]            256        True      \n",
       "______________________________________________________________________\n",
       "Linear               [29, 2048]           264,192    True      \n",
       "______________________________________________________________________\n",
       "ReLU                 [29, 2048]           0          False     \n",
       "______________________________________________________________________\n",
       "Dropout              [29, 2048]           0          False     \n",
       "______________________________________________________________________\n",
       "Linear               [29, 128]            262,272    True      \n",
       "______________________________________________________________________\n",
       "Dropout              [29, 128]            0          False     \n",
       "______________________________________________________________________\n",
       "MergeLayer           [29, 128]            0          False     \n",
       "______________________________________________________________________\n",
       "LayerNorm            [29, 128]            256        True      \n",
       "______________________________________________________________________\n",
       "Conv1d               [4, 29]              516        True      \n",
       "______________________________________________________________________\n",
       "Conv1d               [9, 29]              1,161      True      \n",
       "______________________________________________________________________\n",
       "Linear               [3]                  11,139     True      \n",
       "______________________________________________________________________\n",
       "Linear               [1]                  3,713      True      \n",
       "______________________________________________________________________\n",
       "Embedding            [29, 58]             522        True      \n",
       "______________________________________________________________________\n",
       "Embedding            [29, 64]             384        True      \n",
       "______________________________________________________________________\n",
       "\n",
       "Total params: 3,575,579\n",
       "Total trainable params: 3,575,579\n",
       "Total non-trainable params: 0\n",
       "Optimized with 'torch.optim.adam.Adam', betas=(0.9, 0.99)\n",
       "Using true weight decay as discussed in https://www.fast.ai/2018/07/02/adam-weight-decay/ \n",
       "Loss function : LMAEMaskedLoss\n",
       "======================================================================\n",
       "Callbacks functions applied \n",
       "    LMAEMetric"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sub_fname = \"loss-3.9585val-2.0609\" # uncomment or set None to skip loading trained net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOT loaded!  name 'sub_fname' is not defined\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print(f\"Attempting to load: {sub_fname}... \", end=\"\")\n",
    "    learner.load(sub_fname, strict=False,with_opt=False)\n",
    "    print(\"Loaded\")\n",
    "except Exception as e:\n",
    "    print(\"NOT loaded! \", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = learner.to_parallel()#.to_fp16() b/c it NaNs loss (probably would need to change fp16 settings)\n",
    "data.batch_size = int(4096//2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Real loss func. Need to test different auxiliary tasks weights: `magnetic_w`, `dipole_w`, `potential`, weights of indivial `lmae`s: `types_w` and maybe `input_transform_w` and `feature_transform_w`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learner.loss_func = LMAEMaskedLoss(magnetic_w=0, dipole_w=0, potential_w=0, \n",
    "                                  types_w = [1] + [1] * (n_types-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n",
      "Min numerical gradient: 1.91E-04\n",
      "Min loss divided by 10: 9.12E-04\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learner.lr_find()\n",
    "learner.recorder.plot(suggestion=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='9' class='' max='10', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      90.00% [9/10 22:07<02:27]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>👉🏻LMAE👈🏻</th>\n",
       "      <th>lmae0</th>\n",
       "      <th>lmae1</th>\n",
       "      <th>lmae2</th>\n",
       "      <th>lmae3</th>\n",
       "      <th>lmae4</th>\n",
       "      <th>lmae5</th>\n",
       "      <th>lmae6</th>\n",
       "      <th>lmae7</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.389318</td>\n",
       "      <td>1.326627</td>\n",
       "      <td>1.319322</td>\n",
       "      <td>4.244818</td>\n",
       "      <td>0.571074</td>\n",
       "      <td>3.087672</td>\n",
       "      <td>0.503308</td>\n",
       "      <td>0.677841</td>\n",
       "      <td>0.888927</td>\n",
       "      <td>0.758651</td>\n",
       "      <td>-0.177716</td>\n",
       "      <td>02:27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.781652</td>\n",
       "      <td>0.730221</td>\n",
       "      <td>0.681081</td>\n",
       "      <td>3.267583</td>\n",
       "      <td>0.275725</td>\n",
       "      <td>1.238660</td>\n",
       "      <td>-0.090420</td>\n",
       "      <td>0.470044</td>\n",
       "      <td>0.265065</td>\n",
       "      <td>0.462664</td>\n",
       "      <td>-0.440676</td>\n",
       "      <td>02:27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.142090</td>\n",
       "      <td>0.072125</td>\n",
       "      <td>0.037374</td>\n",
       "      <td>1.881955</td>\n",
       "      <td>-0.462838</td>\n",
       "      <td>0.566154</td>\n",
       "      <td>-0.571226</td>\n",
       "      <td>0.005154</td>\n",
       "      <td>-0.329306</td>\n",
       "      <td>-0.022762</td>\n",
       "      <td>-0.768138</td>\n",
       "      <td>02:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>-0.226039</td>\n",
       "      <td>-0.241250</td>\n",
       "      <td>-0.258259</td>\n",
       "      <td>1.196239</td>\n",
       "      <td>-0.534954</td>\n",
       "      <td>0.283926</td>\n",
       "      <td>-0.962147</td>\n",
       "      <td>-0.238101</td>\n",
       "      <td>-0.733096</td>\n",
       "      <td>-0.160192</td>\n",
       "      <td>-0.917743</td>\n",
       "      <td>02:27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>-0.478556</td>\n",
       "      <td>-0.510593</td>\n",
       "      <td>-0.536912</td>\n",
       "      <td>0.974719</td>\n",
       "      <td>-0.896174</td>\n",
       "      <td>0.275767</td>\n",
       "      <td>-1.273683</td>\n",
       "      <td>-0.548450</td>\n",
       "      <td>-1.025346</td>\n",
       "      <td>-0.409742</td>\n",
       "      <td>-1.392384</td>\n",
       "      <td>02:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>-0.689598</td>\n",
       "      <td>-0.730736</td>\n",
       "      <td>-0.755678</td>\n",
       "      <td>0.821476</td>\n",
       "      <td>-1.176587</td>\n",
       "      <td>-0.108585</td>\n",
       "      <td>-1.446059</td>\n",
       "      <td>-0.714897</td>\n",
       "      <td>-1.208179</td>\n",
       "      <td>-0.582922</td>\n",
       "      <td>-1.629669</td>\n",
       "      <td>02:27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>-0.871367</td>\n",
       "      <td>-0.871473</td>\n",
       "      <td>-0.890155</td>\n",
       "      <td>0.627620</td>\n",
       "      <td>-1.400815</td>\n",
       "      <td>-0.171945</td>\n",
       "      <td>-1.585806</td>\n",
       "      <td>-0.840630</td>\n",
       "      <td>-1.331746</td>\n",
       "      <td>-0.690403</td>\n",
       "      <td>-1.727517</td>\n",
       "      <td>02:27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>-1.041910</td>\n",
       "      <td>-1.046860</td>\n",
       "      <td>-1.069511</td>\n",
       "      <td>0.529466</td>\n",
       "      <td>-1.557520</td>\n",
       "      <td>-0.624246</td>\n",
       "      <td>-1.734763</td>\n",
       "      <td>-0.922087</td>\n",
       "      <td>-1.511823</td>\n",
       "      <td>-0.799695</td>\n",
       "      <td>-1.935416</td>\n",
       "      <td>02:28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>-1.177634</td>\n",
       "      <td>-1.152874</td>\n",
       "      <td>-1.174615</td>\n",
       "      <td>0.461491</td>\n",
       "      <td>-1.693782</td>\n",
       "      <td>-0.797550</td>\n",
       "      <td>-1.887851</td>\n",
       "      <td>-1.003729</td>\n",
       "      <td>-1.594954</td>\n",
       "      <td>-0.852528</td>\n",
       "      <td>-2.028018</td>\n",
       "      <td>02:27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='28' class='' max='69', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      40.58% [28/69 00:08<00:12]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learner.fit_one_cycle(10, 1e-3)#, moms=(0.75,0.70))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learner.recorder.plot_lr(show_moms=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learner.recorder.plot_losses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learner.recorder.plot_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learner.fit_one_cycle(500, 2e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional Fine tune regular fit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learner.callbacks.append(\n",
    "    ReduceLROnPlateauCallback(learner, monitor='train_loss', mode='min', factor=0.2, patience=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learner.fit(300, 1e-3)# , moms=(0.75,0.70))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learner.fit_one_cycle(300, 5e-4)#, moms=(0.75,0.70))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learner.recorder.plot_lr(show_moms=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learner.recorder.plot_losses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learner.recorder.plot_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learner.fit(10,1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learner.to_fp32()\n",
    "val = learner.validate()[1]\n",
    "print(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    sub_fname = f'loss{learner.recorder.losses[-1]:.04f}val{val:.04f}'\n",
    "except:\n",
    "    sub_fname = f'val{val:.04f}'\n",
    "learner.save(sub_fname)\n",
    "print(sub_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference\n",
    "\n",
    "Make sure `tranforms` are activated to test set otherwise TTA > 1 will be as TTA =1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_fname = Path('test.npz')\n",
    "try:\n",
    "    npzfile  = np.load(fname_ext(test_fname, ext))\n",
    "    xt_xyz   = npzfile['x_xyz']\n",
    "    xt_type  = npzfile['x_type']\n",
    "    xt_ext   = npzfile['x_ext']\n",
    "    xt_atom  = npzfile['x_atom']\n",
    "    mt = npzfile['m']\n",
    "    xt_ids = npzfile['x_ids']\n",
    "except:\n",
    "    xt_xyz,xt_type,xt_ext,xt_atom,mt,xt_ids = \\\n",
    "        preprocess(test_fname.with_suffix('.csv'), type_index=types,ext=ext)\n",
    "    np.savez(fname_ext('_'+test_fname, ext), \n",
    "             x_xyz  = xt_xyz,\n",
    "             x_type = xt_type,\n",
    "             x_ext  = xt_ext,\n",
    "             x_atom = xt_atom,\n",
    "             m=mt,\n",
    "             x_ids=xt_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xt_qm9_mulliken = load_fn(f'xt_qm9_mulliken{ext}.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "[v.shape for v in [xt_xyz,xt_type,xt_ext,xt_atom, xt_qm9_mulliken,xt_ids, mt]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TTA_N = 100\n",
    "\n",
    "learner.data.add_test(ItemList(items=(MoleculeItem(i,*v) for i,v in \n",
    "                              enumerate(zip(xt_xyz,xt_type,xt_ext,xt_atom,xt_qm9_mulliken,xt_coulombmat)))))\n",
    "learner.data.test_ds.tfms = tta_tfms if TTA_N > 1 else tfms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#learner.model = nn.DataParallel(learner.model)\n",
    "data.batch_size = int(4096*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub = defaultdict(int)\n",
    "xt_ids_not_extended = (xt_ids!=0) & (xt_ids<=7163688) # TODO\n",
    "ids = xt_ids[xt_ids_not_extended]\n",
    "\n",
    "mb = master_bar(range(TTA_N))\n",
    "for tta in mb:\n",
    "    test_preds = np.zeros((0, 29), dtype=np.float32)\n",
    "\n",
    "    for batch_idx, batch in progress_bar(\n",
    "        enumerate(learner.dl(DatasetType.Test)), total=len(learner.dl(DatasetType.Test)), parent=mb):\n",
    "        _, _, preds_,_,_,_,_,_ = learner.pred_batch(ds_type=DatasetType.Test, batch=batch)\n",
    "        preds_ = preds_.sum(dim=1)\n",
    "        test_preds = np.concatenate([test_preds, preds_.data.cpu().numpy()], axis = 0)\n",
    "\n",
    "    preds = test_preds[xt_ids_not_extended]\n",
    "    for k in range(len(ids)):\n",
    "        sub[int(ids[k])] += preds[k]\n",
    "    \n",
    "for k in range(len(ids)):\n",
    "    sub[int(ids[k])] = sub[int(ids[k])]/TTA_N\n",
    "\n",
    "sub_df = pd.DataFrame(sub.items(), columns=['id', 'scalar_coupling_constant'])\n",
    "sub_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submit to Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub_df.to_csv(sub_fname, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "comp = 'champs-scalar-coupling'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!kaggle competitions submit -c {comp} -f {sub_fname} -m 'QM9  tta {TTA_N} {ext} transformer'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "time.sleep(60)\n",
    "!kaggle competitions submissions -c {comp} -v > submissions-{comp}.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submissions = pd.read_csv(f'submissions-{comp}.csv')\n",
    "submissions.iloc[0].publicScore"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (root)",
   "language": "python",
   "name": "root"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
