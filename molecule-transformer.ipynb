{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%load_ext autoreload\n",
    "#%autoreload 2\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.utils.data\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from torch.utils.data.sampler import RandomSampler\n",
    "#RandomSampler_num_samples = RandomSampler.num_samples\n",
    "#@property\n",
    "#def RandomSampler_num_samples_x10(self): return  self.RandomSampler_num_samples*10\n",
    "#RandomSampler.num_samples= RandomSampler_num_samples_x10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.nn import Parameter\n",
    "\n",
    "def _load_from_state_dict(self, state_dict, prefix, local_metadata, strict,\n",
    "                          missing_keys, unexpected_keys, error_msgs):\n",
    "    r\"\"\"Copies parameters and buffers from :attr:`state_dict` into only\n",
    "    this module, but not its descendants. This is called on every submodule\n",
    "    in :meth:`~torch.nn.Module.load_state_dict`. Metadata saved for this\n",
    "    module in input :attr:`state_dict` is provided as :attr:`local_metadata`.\n",
    "    For state dicts without metadata, :attr:`local_metadata` is empty.\n",
    "    Subclasses can achieve class-specific backward compatible loading using\n",
    "    the version number at `local_metadata.get(\"version\", None)`.\n",
    "\n",
    "    .. note::\n",
    "        :attr:`state_dict` is not the same object as the input\n",
    "        :attr:`state_dict` to :meth:`~torch.nn.Module.load_state_dict`. So\n",
    "        it can be modified.\n",
    "\n",
    "    Arguments:\n",
    "        state_dict (dict): a dict containing parameters and\n",
    "            persistent buffers.\n",
    "        prefix (str): the prefix for parameters and buffers used in this\n",
    "            module\n",
    "        local_metadata (dict): a dict containing the metadata for this module.\n",
    "            See\n",
    "        strict (bool): whether to strictly enforce that the keys in\n",
    "            :attr:`state_dict` with :attr:`prefix` match the names of\n",
    "            parameters and buffers in this module\n",
    "        missing_keys (list of str): if ``strict=True``, add missing keys to\n",
    "            this list\n",
    "        unexpected_keys (list of str): if ``strict=True``, add unexpected\n",
    "            keys to this list\n",
    "        error_msgs (list of str): error messages should be added to this\n",
    "            list, and will be reported together in\n",
    "            :meth:`~torch.nn.Module.load_state_dict`\n",
    "    \"\"\"\n",
    "    for hook in self._load_state_dict_pre_hooks.values():\n",
    "        hook(state_dict, prefix, local_metadata, strict, missing_keys,\n",
    "             unexpected_keys, error_msgs)\n",
    "\n",
    "    local_name_params = itertools.chain(self._parameters.items(),\n",
    "                                        self._buffers.items())\n",
    "    local_state = {k: v.data for k, v in local_name_params if v is not None}\n",
    "\n",
    "    for name, param in local_state.items():\n",
    "        key = prefix + name\n",
    "        if key in state_dict:\n",
    "            input_param = state_dict[key]\n",
    "\n",
    "            # Backward compatibility: loading 1-dim tensor from 0.3.* to version 0.4+\n",
    "            if len(param.shape) == 0 and len(input_param.shape) == 1:\n",
    "                input_param = input_param[0]\n",
    "\n",
    "            if input_param.shape != param.shape:\n",
    "                # local shape should match the one in checkpoint\n",
    "                error_msgs.append(\n",
    "                    'size mismatch for {}: copying a param with shape {} from checkpoint, '\n",
    "                    'the shape in current model is {}.'.format(\n",
    "                        key, input_param.shape, param.shape))\n",
    "                #if not strict:\n",
    "                #    continue\n",
    "\n",
    "            if isinstance(input_param, Parameter):\n",
    "                # backwards compatibility for serialized parameters\n",
    "                input_param = input_param.data\n",
    "\n",
    "            try:\n",
    "                param.copy_(input_param)\n",
    "            except Exception:\n",
    "                error_msgs.append(\n",
    "                    'While copying the parameter named \"{}\", '\n",
    "                    'whose dimensions in the model are {} and '\n",
    "                    'whose dimensions in the checkpoint are {}.'.format(\n",
    "                        key, param.size(), input_param.size()))\n",
    "                # PG load partially\n",
    "\n",
    "                if len(input_param.size()) == 3:\n",
    "                    error_msgs.append(\n",
    "                        'Partially copying the parameter named \"{}\", '\n",
    "                        'whose dimensions in the model are {} and '\n",
    "                        'whose dimensions in the checkpoint are {}. - trying {}'\n",
    "                        .format(\n",
    "                            key, param.size(), input_param.size(),\n",
    "                            param[:input_param.size()[0], :input_param.size(\n",
    "                            )[1], :input_param.size()[2]].shape))\n",
    "                else:\n",
    "                    error_msgs.append(\n",
    "                        'Partially copying the parameter named \"{}\", '\n",
    "                        'whose dimensions in the model are {} and '\n",
    "                        'whose dimensions in the checkpoint are {}. - trying {}'\n",
    "                        .format(key, param.size(), input_param.size(),\n",
    "                                param[:input_param.size()[0]].shape))\n",
    "\n",
    "                try:\n",
    "                    new_input_param = torch.empty_like(param)\n",
    "                    new_input_param = torch.nn.init.normal_(new_input_param,\n",
    "                                                            mean=input_param.mean(),\n",
    "                                                            std=input_param.std())\n",
    "\n",
    "                    if len(input_param.size()) == 3:\n",
    "                        new_input_param[:input_param.size()[0], :input_param.\n",
    "                                        size()[1], :input_param.size(\n",
    "                                        )[2]] = input_param\n",
    "                    else:\n",
    "                        new_input_param[:input_param.size()[0]] = input_param\n",
    "                    param.copy_(new_input_param)\n",
    "                except Exception as e:\n",
    "                    assert e\n",
    "                    error_msgs.append(\n",
    "                        'Failed to load weights partially {}'.format(e))\n",
    "        elif strict:\n",
    "            missing_keys.append(key)\n",
    "\n",
    "    if strict:\n",
    "        for key in state_dict.keys():\n",
    "            if key.startswith(prefix):\n",
    "                input_name = key[len(prefix):]\n",
    "                input_name = input_name.split(\n",
    "                    '.', 1)[0]  # get the name of param/buffer/child\n",
    "                if input_name not in self._modules and input_name not in local_state:\n",
    "                    unexpected_keys.append(key)\n",
    "\n",
    "def load_state_dict(self, state_dict, strict=True):\n",
    "    r\"\"\"Copies parameters and buffers from :attr:`state_dict` into\n",
    "    this module and its descendants. If :attr:`strict` is ``True``, then\n",
    "    the keys of :attr:`state_dict` must exactly match the keys returned\n",
    "    by this module's :meth:`~torch.nn.Module.state_dict` function.\n",
    "\n",
    "    Arguments:\n",
    "        state_dict (dict): a dict containing parameters and\n",
    "            persistent buffers.\n",
    "        strict (bool, optional): whether to strictly enforce that the keys\n",
    "            in :attr:`state_dict` match the keys returned by this module's\n",
    "            :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\n",
    "\n",
    "    Returns:\n",
    "        ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n",
    "            * **missing_keys** is a list of str containing the missing keys\n",
    "            * **unexpected_keys** is a list of str containing the unexpected keys\n",
    "    \"\"\"\n",
    "    missing_keys = []\n",
    "    unexpected_keys = []\n",
    "    error_msgs = []\n",
    "\n",
    "    # copy state_dict so _load_from_state_dict can modify it\n",
    "    metadata = getattr(state_dict, '_metadata', None)\n",
    "    state_dict = state_dict.copy()\n",
    "    if metadata is not None:\n",
    "        state_dict._metadata = metadata\n",
    "\n",
    "    def load(module, prefix=''):\n",
    "        local_metadata = {} if metadata is None else metadata.get(\n",
    "            prefix[:-1], {})\n",
    "        module._load_from_state_dict(state_dict, prefix, local_metadata, True,\n",
    "                                     missing_keys, unexpected_keys, error_msgs)\n",
    "        for name, child in module._modules.items():\n",
    "            if child is not None:\n",
    "                load(child, prefix + name + '.')\n",
    "                \n",
    "    load(self)\n",
    "\n",
    "    if strict:\n",
    "        if len(unexpected_keys) > 0:\n",
    "            error_msgs.insert(\n",
    "                0, 'Unexpected key(s) in state_dict: {}. '.format(', '.join(\n",
    "                    '\"{}\"'.format(k) for k in unexpected_keys)))\n",
    "        if len(missing_keys) > 0:\n",
    "            error_msgs.insert(\n",
    "                0, 'Missing key(s) in state_dict: {}. '.format(', '.join(\n",
    "                    '\"{}\"'.format(k) for k in missing_keys)))\n",
    "\n",
    "    if strict and len(error_msgs) > 0:\n",
    "        raise RuntimeError(\n",
    "            'Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n",
    "                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.Module._load_from_state_dict = _load_from_state_dict\n",
    "nn.Module.load_state_dict = load_state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from fastai import *\n",
    "from fastai.basic_train import *\n",
    "from fastai.basic_data import *\n",
    "from fastai.data_block import *\n",
    "from fastai.torch_core import *\n",
    "from fastai.train import *\n",
    "from fastai.callback import *\n",
    "from fastai.callbacks import *\n",
    "from fastai.distributed import *\n",
    "from fastai.layers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ext = '_ext' # or ''\n",
    "multi = '_molecule'\n",
    "train_fname = Path('train.npz')\n",
    "type_index=types = ['1JHC', '2JHH', '1JHN', '2JHN', '2JHC', '3JHH', '3JHC', '3JHN'] \n",
    "atoms = 'CFHNO'\n",
    "xyz = ['x', 'y', 'z']\n",
    "max_atoms = 29 #int(s.atom_index.max() + 1)\n",
    "\n",
    "fname_ext = lambda fname,ext,multi: f'{str(fname)[:-4]}{ext}{multi}{str(fname)[-4:]}'\n",
    "s  = pd.read_csv('structures.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_group(process_args,i):\n",
    "    m_group,has_y,x0_xyz,x0_type,x0_ext,x0_atom,x0_id,m0,y0_scalar = process_args\n",
    "    x0_atom[:] = -1\n",
    "    x0_type[:] = -1\n",
    "    x0_id[:] = -1\n",
    "    y0_scalar[:] = np.nan\n",
    "    (m_name, m_index) ,m_group = m_group\n",
    "    ss = s[s.molecule_name==m_name]\n",
    "    n_atoms = len(ss)\n",
    "    ii = 0\n",
    "    x0_xyz[ii] = 0.\n",
    "    x0_type[ii] = -1\n",
    "    x0_ext[ii] =  True\n",
    "    x0_xyz[ii,:,:n_atoms] = ss[xyz].values.T  # xyz \n",
    "    x0_type[ii,0,m_group['atom_index_1'], m_group['atom_index_0']] = m_group['type_idx'].T\n",
    "    x0_ext[ii,0, m_group['atom_index_0']] = m_group['ext'].T\n",
    "    x0_atom[ii,:,:n_atoms] = ss['atom_idx'].T\n",
    "    x0_id[ii,0,m_group['atom_index_1'], m_group['atom_index_0']] = m_group['id'].T\n",
    "    m0[ii] = m_index\n",
    "    res = [x0_xyz,x0_type,x0_ext,x0_atom,x0_id,m0]\n",
    "    if has_y:\n",
    "        y0_scalar[0,0,m_group['atom_index_1'], m_group['atom_index_0']] =  m_group['scalar_coupling_constant']\n",
    "        res.append(y0_scalar)\n",
    "    return i,res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(fname, type_index=None, ext=''):\n",
    "    t  = pd.read_csv(fname_ext(fname,ext,''))\n",
    "    has_y = 'scalar_coupling_constant' in t.columns\n",
    "    t['molecule_index'] = pd.factorize(t['molecule_name'])[0] + t['id'].min()\n",
    "    # make sure we use the same indexes in train/test (test needs to provide type_index)\n",
    "    if type_index is not None:\n",
    "        t['type_idx'] = t['type'].apply(lambda x: type_index.index(x) ) # pd.factorize(pd.concat([pd.Series(type_index),t['type']]))[0][len(type_index):]\n",
    "    else:\n",
    "        t['type_idx'] = pd.factorize(t['type'])[0]\n",
    "\n",
    "    s['atom_idx'] = s['atom'].apply(lambda x: atoms.index(x) )\n",
    "\n",
    "    max_items = t['molecule_name'].nunique()\n",
    "    x_xyz   = np.zeros((max_items,len(xyz),  max_atoms), dtype=np.float32)\n",
    "    x_type  = np.empty((max_items,1,         max_atoms,max_atoms), dtype=np.int8)\n",
    "    x_ext   = np.zeros((max_items,1,         max_atoms), dtype=np.bool_)\n",
    "    x_atom  = np.empty((max_items,1,         max_atoms), dtype=np.int8)\n",
    "    x_ids   = np.zeros((max_items,           max_atoms,max_atoms), dtype=np.int32)\n",
    "    x_atom[:] = -1\n",
    "    x_type[:] = -1\n",
    "    x_ids[:] = -1\n",
    "    m = np.zeros((max_items,), dtype=np.int32)\n",
    "    if has_y:\n",
    "        y_scalar   = np.empty((max_items,1 ,max_atoms,max_atoms), dtype=np.float32)\n",
    "        y_scalar[:] = np.nan\n",
    "        \n",
    "    x0_xyz   = np.zeros((1,len(xyz),  max_atoms), dtype=np.float32)        \n",
    "    x0_type  = np.empty((1,1,         max_atoms,max_atoms), dtype=np.int8)\n",
    "    x0_ext   = np.zeros((1,1,         max_atoms), dtype=np.bool_)\n",
    "    x0_atom  = np.empty((1,1,         max_atoms), dtype=np.int8)\n",
    "    x0_id    = np.zeros((1,1,         max_atoms,max_atoms), dtype=np.int32)\n",
    "    y0_scalar   = np.empty((1,1 ,max_atoms,max_atoms), dtype=np.float32)\n",
    "    m0 = np.zeros((1,), dtype=np.int32)\n",
    "    \n",
    "    process_args = [(m_group,has_y,x0_xyz,x0_type,x0_ext,x0_atom,x0_id,m0,y0_scalar) \n",
    "                    for m_group in t.groupby(['molecule_name', 'molecule_index'])]\n",
    "    res = parallel(process_group,process_args,max_workers=multiprocessing.cpu_count())\n",
    "    for i,r in progress_bar(res):\n",
    "        x_xyz[i],x_type[i],x_ext[i],x_atom[i],x_ids[i],m[i] = r[:6]\n",
    "        if has_y: y_scalar[i]=r[6]\n",
    "        \n",
    "    res = [x_xyz,x_type,x_ext,x_atom,x_ids,m]\n",
    "    if has_y: res.append(y_scalar)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define where you want to use original training set '' or extended ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load preprocessed or preprocess and save for later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fname = Path('train.npz')\n",
    "types = ['1JHC', '2JHH', '1JHN', '2JHN', '2JHC', '3JHH', '3JHC', '3JHN'] \n",
    "atoms = 'CFHNO'\n",
    "\n",
    "try:\n",
    "    npzfile = np.load(fname_ext(train_fname,ext,multi))\n",
    "    x_xyz   = npzfile['x_xyz']\n",
    "    x_type  = npzfile['x_type']\n",
    "    x_ext   = npzfile['x_ext']\n",
    "    x_atom  = npzfile['x_atom']\n",
    "\n",
    "    y_scalar    = npzfile['y_scalar']\n",
    "    m = npzfile['m']\n",
    "    max_items, max_atoms = x_xyz.shape[0], x_xyz.shape[-1]\n",
    "except:\n",
    "    x_xyz,x_type,x_ext,x_atom,x_ids,m,y_scalar=preprocess(train_fname.with_suffix('.csv'),type_index=types,ext=ext)\n",
    "    np.savez(fname_ext(train_fname,ext,multi), \n",
    "             x_xyz=x_xyz,\n",
    "             x_type=x_type,\n",
    "             x_ext=x_ext,\n",
    "             x_atom=x_atom,\n",
    "             x_ids=x_ids,\n",
    "             y_scalar=y_scalar,\n",
    "             m=m)\n",
    "n_types = int(x_type[~np.isnan(x_type)].max() + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(85003, 3, 29),\n",
       " (85003, 1, 29, 29),\n",
       " (85003, 1, 29),\n",
       " (85003, 1, 29),\n",
       " (85003, 1, 29, 29),\n",
       " (85003,)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[v.shape for v in [x_xyz,x_type,x_ext,x_atom,\n",
    "                   y_scalar, m]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1,  2,  2,  2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1], dtype=int8)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_type[1,0,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_xyz_mean, x_xyz_std = Tensor(x_xyz.mean(axis=(0,2),keepdims=True)), Tensor(x_xyz.std(axis=(0,2),keepdims=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 1])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_xyz_std.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fastai classes (this should should be done into its own `application` but who has time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MoleculeItem(ItemBase):\n",
    "    def __init__(self,i,xyz,type,ext,atom):#,qm9_mulliken,coulomb): \n",
    "        self.i,self.xyz,self.type,self.ext, self.atom = \\\n",
    "            i,xyz,type,ext,atom\n",
    "        self.data = [Tensor(xyz), LongTensor((type)), \n",
    "                     Tensor(ext), LongTensor((atom))]\n",
    "    def __str__(self):\n",
    "        # TODO: count n_atoms correctly. \n",
    "        n_atoms = np.count_nonzero(np.sum(np.absolute(self.xyz), axis=0))+1\n",
    "        n_couplings = np.sum((self.type!=-1))\n",
    "        return f'{self.i} {n_atoms} atoms {n_couplings} couplings'\n",
    "    \n",
    "    def apply_tfms(self, tfms:Collection, **kwargs):\n",
    "        x = self.clone()\n",
    "        for t in tfms:\n",
    "            if t: x.data = t(x.data)\n",
    "        return x\n",
    "    \n",
    "    def clone(self):\n",
    "        return self.__class__(self.i,self.xyz,self.type,self.ext,self.atom)#,self.qm9_mulliken,self.coulomb)\n",
    "    \n",
    "class ScalarCouplingItem(ItemBase):\n",
    "    def __init__(self,scalar,**kwargs): \n",
    "        self.scalar = scalar#,magnetic,mulliken,dipole,potential\n",
    "        self.data = Tensor(scalar) #, Tensor(magnetic), Tensor(dipole), Tensor(potential))\n",
    "    def __str__(self):\n",
    "        #res, spacer, n_couplings = '', '', 0\n",
    "        n_couplings = (~torch.isnan(self.data)).sum()\n",
    "        return f'{n_couplings}'\n",
    "    def apply_tfms(self, tfms:Collection, **kwargs):\n",
    "        y = self.clone()\n",
    "        for t in tfms:\n",
    "            if 'label_smoothing' == t.__name__:\n",
    "                if t: y.data = t(y.data)\n",
    "        return y\n",
    "    def clone(self): return self.__class__(self.scalar)#,self.magnetic,self.mulliken,self.dipole,self.potential)\n",
    "    def __hash__(self): return hash(str(self))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LMAEMaskedLoss(Module):\n",
    "    def __init__(self,types_w = [1]*n_types, return_all=False, proxy_log=torch.log, exclude_ext=False):\n",
    "        #self.contrib_w,self.magnetic_w,self.dipole_w,self.potential_w = contrib_w,magnetic_w,dipole_w,potential_w\n",
    "        self.types_w = types_w\n",
    "        self.return_all = return_all\n",
    "        self.proxy_log = proxy_log\n",
    "        self.exclude_ext = exclude_ext\n",
    "    \n",
    "    def forward(self, input_outputs, t_scalar):#, t_magnetic, t_dipole, t_potential):    \n",
    "        type, ext, p_scalar = input_outputs\n",
    "        loss = 0.\n",
    "        n = 0\n",
    "        j_loss = [0] * n_types\n",
    "        t_scalar = t_scalar.view(-1,1)\n",
    "        t_scalar = t_scalar[~torch.isnan(t_scalar)].unsqueeze(-1)\n",
    "\n",
    "        for t in range(n_types):\n",
    "            #if self.exclude_ext:\n",
    "            #    print(type.shape)\n",
    "            mask = (type == t) #if not self.exclude_ext else ((type == t) & (ext == 0)).squeeze(1)\n",
    "            if mask.sum() > 0:\n",
    "                _output,_target = p_scalar[mask], t_scalar[mask]\n",
    "                # LMAE scalar\n",
    "                s_loss = self.proxy_log((_output - _target).abs().mean()+1e-9)\n",
    "                loss += self.types_w[t] * s_loss\n",
    "                j_loss[t] += s_loss\n",
    "                n+=1\n",
    "        loss /= n\n",
    "        \n",
    "        return loss if not self.return_all else (loss, *j_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ScalarCouplingList(ItemList):\n",
    "    def __init__(self, items:Iterator, **kwargs):\n",
    "        super().__init__(items, **kwargs)\n",
    "        self.loss_func = LMAEMaskedLoss\n",
    "\n",
    "    def get(self, i):\n",
    "        o = super().get(i)\n",
    "        #print(len(o),o[0].shape,o[0].dtype)\n",
    "        return ScalarCouplingItem(np.array(o, dtype=np.float32))\n",
    "\n",
    "    def reconstruct(self,t): return 0; # TODO for viz !!!! ScalarCouplingItem(t.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quaterions allow us to rotate 3d points randoming with a nice uniform distribution of 3 numbers hece we use them, however it's still to be seen if are useful here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#https://github.com/facebookresearch/QuaterNet/blob/master/common/quaternion.py\n",
    "def qrot(q, v):\n",
    "    \"\"\"\n",
    "    Rotate vector(s) v about the rotation described by quaternion(s) q.\n",
    "    Expects a tensor of shape (*, 4) for q and a tensor of shape (*, 3) for v,\n",
    "    where * denotes any number of dimensions.\n",
    "    Returns a tensor of shape (*, 3).\n",
    "    \"\"\"\n",
    "    assert q.shape[-1] == 4\n",
    "    assert v.shape[-1] == 3\n",
    "    assert q.shape[:-1] == v.shape[:-1]\n",
    "    \n",
    "    original_shape = list(v.shape)\n",
    "    q = q.view(-1, 4)\n",
    "    v = v.view(-1, 3)\n",
    "    \n",
    "    qvec = q[:, 1:]\n",
    "    uv = torch.cross(qvec, v, dim=1)\n",
    "    uuv = torch.cross(qvec, uv, dim=1)\n",
    "    return (v + 2 * (q[:, :1] * uv + uuv)).view(original_shape)\n",
    "\n",
    "def random_rotation(data):\n",
    "    x_xyz = data[0].transpose(0,1)\n",
    "    r = torch.rand(3)\n",
    "    sq1_v1,sqv1,v2_2pi,v3_2pi = torch.sqrt(1-r[:1]),torch.sqrt(r[:1]),2*math.pi*r[1:2],2*math.pi*r[2:3]\n",
    "    q = torch.cat([sq1_v1*torch.sin(v2_2pi), sq1_v1*torch.cos(v2_2pi), \n",
    "                   sqv1  *torch.sin(v3_2pi), sqv1  *torch.cos(v3_2pi)], dim=0).unsqueeze(0)\n",
    "    x_xyz = qrot(q.expand(x_xyz.shape[0],-1), x_xyz).squeeze(0).transpose(0,1)\n",
    "    return (x_xyz, *data[1:])\n",
    "\n",
    "def normalize(data):\n",
    "    sq = False\n",
    "    if data[0].ndim < 3:\n",
    "        data[0].unsqueeze_(0)\n",
    "        #data[4].unsqueeze_(0)\n",
    "        sq = True\n",
    "    x_xyz      = (data[0] - x_xyz_mean)          / x_xyz_std\n",
    "    #x_mulliken = (data[4] - x_qm9_mulliken_mean) / x_qm9_mulliken_std\n",
    "    if sq:\n",
    "        x_xyz.squeeze_(0)\n",
    "        #x_mulliken.squeeze_(0)\n",
    "    return (x_xyz, *data[1:])\n",
    "\n",
    "def canonize(data):\n",
    "    xyz,type,ext,atom = data\n",
    "    mask = (atom == -1).squeeze(0)\n",
    "    i_max_atom = torch.nonzero(type != -1).max()\n",
    "    mask_atoms = torch.ones((  max_atoms,  max_atoms), dtype=torch.uint8)\n",
    "    zeros      = torch.zeros((i_max_atom, i_max_atom), dtype=torch.uint8)\n",
    "    mask_atoms[:zeros.shape[0],:zeros.shape[1]] = zeros\n",
    "    n_atoms = mask.sum()\n",
    "    xyz[:,mask], type[:,mask],ext[:,mask],atom[:,mask] = 0,-1,1,-1\n",
    "    return (xyz,type,ext,atom,mask_atoms.unsqueeze(0),n_atoms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build `data` bunch etc. for fastai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = ItemList(items=(MoleculeItem(i,*v) for i,v in \n",
    "                       enumerate(zip(x_xyz,x_type,x_ext,x_atom))),\n",
    "                label_cls=ScalarCouplingItem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "_, idx_valid_split = train_test_split(range(m.max()+1), test_size=0.1, random_state=13)\n",
    "idx_valid_split = np.argwhere(np.isin(m, idx_valid_split)).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [],
   "source": [
    "data = data.split_by_idx(idx_valid_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def l(o): return np.array(y_scalar[o.i], dtype=np.float32)\n",
    "data = data.label_from_func(func=l,label_cls=ScalarCouplingList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfms = [normalize, canonize]\n",
    "tta_tfms = list(tfms)\n",
    "tta_tfms.insert(0,random_rotation)\n",
    "data = data.transform((tta_tfms, tfms))#.transform_y(([label_smoothing], None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataBunch;\n",
       "\n",
       "Train: LabelList (76502 items)\n",
       "x: ItemList\n",
       "0 6 atoms 20 couplings,1 5 atoms 12 couplings,2 4 atoms 2 couplings,3 4 atoms 4 couplings,5 7 atoms 20 couplings\n",
       "y: ScalarCouplingList\n",
       "20,12,2,4,20\n",
       "Path: .;\n",
       "\n",
       "Valid: LabelList (8501 items)\n",
       "x: ItemList\n",
       "4 9 atoms 54 couplings,9 7 atoms 18 couplings,37 9 atoms 26 couplings,43 14 atoms 88 couplings,47 14 atoms 90 couplings\n",
       "y: ScalarCouplingList\n",
       "54,18,26,88,90\n",
       "Path: .;\n",
       "\n",
       "Test: None"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=data.databunch()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[     nan,  84.8076,  84.8074,  84.8093,  84.8095,      nan,      nan,\n",
       "                nan,      nan,      nan,      nan,      nan,      nan,      nan,\n",
       "                nan,      nan,      nan,      nan,      nan,      nan,      nan,\n",
       "                nan,      nan,      nan,      nan,      nan,      nan,      nan,\n",
       "                nan],\n",
       "          [ 84.8076,      nan, -11.2570, -11.2548, -11.2543,      nan,      nan,\n",
       "                nan,      nan,      nan,      nan,      nan,      nan,      nan,\n",
       "                nan,      nan,      nan,      nan,      nan,      nan,      nan,\n",
       "                nan,      nan,      nan,      nan,      nan,      nan,      nan,\n",
       "                nan],\n",
       "          [ 84.8074, -11.2570,      nan, -11.2541, -11.2548,      nan,      nan,\n",
       "                nan,      nan,      nan,      nan,      nan,      nan,      nan,\n",
       "                nan,      nan,      nan,      nan,      nan,      nan,      nan,\n",
       "                nan,      nan,      nan,      nan,      nan,      nan,      nan,\n",
       "                nan],\n",
       "          [ 84.8093, -11.2548, -11.2541,      nan, -11.2543,      nan,      nan,\n",
       "                nan,      nan,      nan,      nan,      nan,      nan,      nan,\n",
       "                nan,      nan,      nan,      nan,      nan,      nan,      nan,\n",
       "                nan,      nan,      nan,      nan,      nan,      nan,      nan,\n",
       "                nan],\n",
       "          [ 84.8095, -11.2543, -11.2548, -11.2543,      nan,      nan,      nan,\n",
       "                nan,      nan,      nan,      nan,      nan,      nan,      nan,\n",
       "                nan,      nan,      nan,      nan,      nan,      nan,      nan,\n",
       "                nan,      nan,      nan,      nan,      nan,      nan,      nan,\n",
       "                nan],\n",
       "          [     nan,      nan,      nan,      nan,      nan,      nan,      nan,\n",
       "                nan,      nan,      nan,      nan,      nan,      nan,      nan,\n",
       "                nan,      nan,      nan,      nan,      nan,      nan,      nan,\n",
       "                nan,      nan,      nan,      nan,      nan,      nan,      nan,\n",
       "                nan],\n",
       "          [     nan,      nan,      nan,      nan,      nan,      nan,      nan,\n",
       "                nan,      nan,      nan,      nan,      nan,      nan,      nan,\n",
       "                nan,      nan,      nan,      nan,      nan,      nan,      nan,\n",
       "                nan,      nan,      nan,      nan,      nan,      nan,      nan,\n",
       "                nan],\n",
       "          [     nan,      nan,      nan,      nan,      nan,      nan,      nan,\n",
       "                nan,      nan,      nan,      nan,      nan,      nan,      nan,\n",
       "                nan,      nan,      nan,      nan,      nan,      nan,      nan,\n",
       "                nan,      nan,      nan,      nan,      nan,      nan,      nan,\n",
       "                nan],\n",
       "          [     nan,      nan,      nan,      nan,      nan,      nan,      nan,\n",
       "                nan,      nan,      nan,      nan,      nan,      nan,      nan,\n",
       "                nan,      nan,      nan,      nan,      nan,      nan,      nan,\n",
       "                nan,      nan,      nan,      nan,      nan,      nan,      nan,\n",
       "                nan],\n",
       "          [     nan,      nan,      nan,      nan,      nan,      nan,      nan,\n",
       "                nan,      nan,      nan,      nan,      nan,      nan,      nan,\n",
       "                nan,      nan,      nan,      nan,      nan,      nan,      nan,\n",
       "                nan,      nan,      nan,      nan,      nan,      nan,      nan,\n",
       "                nan],\n",
       "          [     nan,      nan,      nan,      nan,      nan,      nan,      nan,\n",
       "                nan,      nan,      nan,      nan,      nan,      nan,      nan,\n",
       "                nan,      nan,      nan,      nan,      nan,      nan,      nan,\n",
       "                nan,      nan,      nan,      nan,      nan,      nan,      nan,\n",
       "                nan],\n",
       "          [     nan,      nan,      nan,      nan,      nan,      nan,      nan,\n",
       "                nan,      nan,      nan,      nan,      nan,      nan,      nan,\n",
       "                nan,      nan,      nan,      nan,      nan,      nan,      nan,\n",
       "                nan,      nan,      nan,      nan,      nan,      nan,      nan,\n",
       "                nan],\n",
       "          [     nan,      nan,      nan,      nan,      nan,      nan,      nan,\n",
       "                nan,      nan,      nan,      nan,      nan,      nan,      nan,\n",
       "                nan,      nan,      nan,      nan,      nan,      nan,      nan,\n",
       "                nan,      nan,      nan,      nan,      nan,      nan,      nan,\n",
       "                nan],\n",
       "          [     nan,      nan,      nan,      nan,      nan,      nan,      nan,\n",
       "                nan,      nan,      nan,      nan,      nan,      nan,      nan,\n",
       "                nan,      nan,      nan,      nan,      nan,      nan,      nan,\n",
       "                nan,      nan,      nan,      nan,      nan,      nan,      nan,\n",
       "                nan],\n",
       "          [     nan,      nan,      nan,      nan,      nan,      nan,      nan,\n",
       "                nan,      nan,      nan,      nan,      nan,      nan,      nan,\n",
       "                nan,      nan,      nan,      nan,      nan,      nan,      nan,\n",
       "                nan,      nan,      nan,      nan,      nan,      nan,      nan,\n",
       "                nan],\n",
       "          [     nan,      nan,      nan,      nan,      nan,      nan,      nan,\n",
       "                nan,      nan,      nan,      nan,      nan,      nan,      nan,\n",
       "                nan,      nan,      nan,      nan,      nan,      nan,      nan,\n",
       "                nan,      nan,      nan,      nan,      nan,      nan,      nan,\n",
       "                nan],\n",
       "          [     nan,      nan,      nan,      nan,      nan,      nan,      nan,\n",
       "                nan,      nan,      nan,      nan,      nan,      nan,      nan,\n",
       "                nan,      nan,      nan,      nan,      nan,      nan,      nan,\n",
       "                nan,      nan,      nan,      nan,      nan,      nan,      nan,\n",
       "                nan],\n",
       "          [     nan,      nan,      nan,      nan,      nan,      nan,      nan,\n",
       "                nan,      nan,      nan,      nan,      nan,      nan,      nan,\n",
       "                nan,      nan,      nan,      nan,      nan,      nan,      nan,\n",
       "                nan,      nan,      nan,      nan,      nan,      nan,      nan,\n",
       "                nan],\n",
       "          [     nan,      nan,      nan,      nan,      nan,      nan,      nan,\n",
       "                nan,      nan,      nan,      nan,      nan,      nan,      nan,\n",
       "                nan,      nan,      nan,      nan,      nan,      nan,      nan,\n",
       "                nan,      nan,      nan,      nan,      nan,      nan,      nan,\n",
       "                nan],\n",
       "          [     nan,      nan,      nan,      nan,      nan,      nan,      nan,\n",
       "                nan,      nan,      nan,      nan,      nan,      nan,      nan,\n",
       "                nan,      nan,      nan,      nan,      nan,      nan,      nan,\n",
       "                nan,      nan,      nan,      nan,      nan,      nan,      nan,\n",
       "                nan],\n",
       "          [     nan,      nan,      nan,      nan,      nan,      nan,      nan,\n",
       "                nan,      nan,      nan,      nan,      nan,      nan,      nan,\n",
       "                nan,      nan,      nan,      nan,      nan,      nan,      nan,\n",
       "                nan,      nan,      nan,      nan,      nan,      nan,      nan,\n",
       "                nan],\n",
       "          [     nan,      nan,      nan,      nan,      nan,      nan,      nan,\n",
       "                nan,      nan,      nan,      nan,      nan,      nan,      nan,\n",
       "                nan,      nan,      nan,      nan,      nan,      nan,      nan,\n",
       "                nan,      nan,      nan,      nan,      nan,      nan,      nan,\n",
       "                nan],\n",
       "          [     nan,      nan,      nan,      nan,      nan,      nan,      nan,\n",
       "                nan,      nan,      nan,      nan,      nan,      nan,      nan,\n",
       "                nan,      nan,      nan,      nan,      nan,      nan,      nan,\n",
       "                nan,      nan,      nan,      nan,      nan,      nan,      nan,\n",
       "                nan],\n",
       "          [     nan,      nan,      nan,      nan,      nan,      nan,      nan,\n",
       "                nan,      nan,      nan,      nan,      nan,      nan,      nan,\n",
       "                nan,      nan,      nan,      nan,      nan,      nan,      nan,\n",
       "                nan,      nan,      nan,      nan,      nan,      nan,      nan,\n",
       "                nan],\n",
       "          [     nan,      nan,      nan,      nan,      nan,      nan,      nan,\n",
       "                nan,      nan,      nan,      nan,      nan,      nan,      nan,\n",
       "                nan,      nan,      nan,      nan,      nan,      nan,      nan,\n",
       "                nan,      nan,      nan,      nan,      nan,      nan,      nan,\n",
       "                nan],\n",
       "          [     nan,      nan,      nan,      nan,      nan,      nan,      nan,\n",
       "                nan,      nan,      nan,      nan,      nan,      nan,      nan,\n",
       "                nan,      nan,      nan,      nan,      nan,      nan,      nan,\n",
       "                nan,      nan,      nan,      nan,      nan,      nan,      nan,\n",
       "                nan],\n",
       "          [     nan,      nan,      nan,      nan,      nan,      nan,      nan,\n",
       "                nan,      nan,      nan,      nan,      nan,      nan,      nan,\n",
       "                nan,      nan,      nan,      nan,      nan,      nan,      nan,\n",
       "                nan,      nan,      nan,      nan,      nan,      nan,      nan,\n",
       "                nan],\n",
       "          [     nan,      nan,      nan,      nan,      nan,      nan,      nan,\n",
       "                nan,      nan,      nan,      nan,      nan,      nan,      nan,\n",
       "                nan,      nan,      nan,      nan,      nan,      nan,      nan,\n",
       "                nan,      nan,      nan,      nan,      nan,      nan,      nan,\n",
       "                nan],\n",
       "          [     nan,      nan,      nan,      nan,      nan,      nan,      nan,\n",
       "                nan,      nan,      nan,      nan,      nan,      nan,      nan,\n",
       "                nan,      nan,      nan,      nan,      nan,      nan,      nan,\n",
       "                nan,      nan,      nan,      nan,      nan,      nan,      nan,\n",
       "                nan]]]),)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.train_ds[0][1].data,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data.train_ds.filter_by_func(lambda item, _: len(item.data[1].cpu().numpy()[item.data[1].cpu().numpy()==0]) == 0)\n",
    "#data.valid_ds.filter_by_func(lambda item, _: len(item.data[1].cpu().numpy()[item.data[1].cpu().numpy()==0]) == 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "Whole model here, self-contained (needs some cleanup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LMAEMetric(LearnerCallback):\n",
    "    _order=-20 # Needs to run before the recorder\n",
    "    def __init__(self, learn, val_only=True):\n",
    "        super().__init__(learn)\n",
    "        self.val_only=val_only\n",
    "        self.metric = LMAEMaskedLoss(return_all=True, exclude_ext=True)\n",
    "\n",
    "    def on_train_begin(self, **kwargs):\n",
    "        if not self.val_only: self.learn.recorder.add_metric_names(['tLMAE'])\n",
    "        self.learn.recorder.add_metric_names(['👉🏻LMAE👈🏻'] + [f'lmae{i}' for i in range(n_types)])\n",
    "            \n",
    "    def on_batch_end(self, train, last_output, last_target, **kwargs):\n",
    "        if self.val_only and train: return \n",
    "        preds,targs = self.preds[int(train)], self.targs[int(train)] # 0 val 1 train\n",
    "        \n",
    "        if preds is None:\n",
    "            targs, preds = last_target, listify(last_output)\n",
    "            targs,preds = targs.detach(),[t.detach() for t in preds]\n",
    "        else:\n",
    "            targs = torch.cat([targs,last_target.detach()], dim=0)\n",
    "            for i,(o,t) in enumerate(zip(last_output, last_target)):\n",
    "                preds[i] = torch.cat([preds[i], o.detach()], dim=0)\n",
    "        self.preds[int(train)], self.targs[int(train)] = preds,targs\n",
    "        \n",
    "    def on_epoch_begin(self, **kwargs):\n",
    "        self.targs, self.preds = [None, None], [None, None]\n",
    "\n",
    "    def on_epoch_end(self, last_metrics, **kwargs):\n",
    "        if True: #(kwargs['epoch'] % max_atoms) == 0:\n",
    "            mets = []\n",
    "            if self.preds[1]: mets.append(self.metric.forward(self.preds[1], self.targs[1])[0]) # just tLMAE\n",
    "            if self.preds[0]: \n",
    "                mets.extend(self.metric.forward(self.preds[0], self.targs[0]))\n",
    "            return add_metrics(last_metrics, mets) if mets else None\n",
    "        else: return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Activation = Enum('Activation', 'ReLU Swish GeLU')\n",
    "\n",
    "class PositionalEncoding(Module):\n",
    "    \"Encode the position with a sinusoid.\"\n",
    "    def __init__(self, d:int): self.register_buffer('freq', 1 / (10000 ** (torch.arange(0., d, 2.)/d)))\n",
    "\n",
    "    def forward(self, pos:Tensor):\n",
    "        inp = torch.ger(pos, self.freq)\n",
    "        enc = torch.cat([inp.sin(), inp.cos()], dim=-1)\n",
    "        return enc\n",
    "    \n",
    "class GeLU(Module):\n",
    "    def forward(self, x): return 0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n",
    "\n",
    "class Swish(Module):\n",
    "    def forward(self, x): return x * torch.sigmoid(x)\n",
    "\n",
    "_activ_func = {Activation.ReLU:nn.ReLU(inplace=True), Activation.GeLU:GeLU(), Activation.Swish: Swish()}\n",
    "\n",
    "def feed_forward(d_model:int,d_model_out:int, d_ff:int, ff_p:float=0.,\n",
    "                 act:Activation=Activation.ReLU, double_drop:bool=True):\n",
    "    if d_model==d_model_out:\n",
    "        layers = [nn.Linear(d_model, d_ff), _activ_func[act]]\n",
    "        if double_drop: layers.append(nn.Dropout(ff_p))\n",
    "        return SequentialEx(*layers,nn.Linear(d_ff,d_model_out),nn.Dropout(ff_p),MergeLayer(),nn.LayerNorm(d_model_out))\n",
    "    else:\n",
    "        return SequentialEx(nn.Linear(d_model,d_model_out))#,nn.Dropout(ff_p)),nn.LayerNorm(d_model_out))\n",
    "\n",
    "class MultiHeadAttention(Module):\n",
    "    \"MutiHeadAttention.\"\n",
    "    def __init__(self, n_heads:int, d_model:int, d_head:int=None, resid_p:float=0., attn_p:float=0., bias:bool=True,\n",
    "                 scale:bool=True):\n",
    "        d_head = ifnone(d_head, d_model//n_heads)\n",
    "        self.n_heads,self.d_head,self.scale = n_heads,d_head,scale\n",
    "        self.attention = nn.Linear(d_model, 3 * n_heads * d_head, bias=bias)\n",
    "        self.out = nn.Linear(n_heads * d_head, d_model, bias=bias)\n",
    "        self.drop_att,self.drop_res = nn.Dropout(attn_p),nn.Dropout(resid_p)\n",
    "        self.ln = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x:Tensor, mask:Tensor=None, **kwargs):\n",
    "        return self.ln(x + self.drop_res(self.out(self._apply_attention(x, mask=mask, **kwargs))))\n",
    "\n",
    "    def _apply_attention(self, x:Tensor, mask:Tensor=None):\n",
    "        bs,x_len = x.size(0),x.size(1)\n",
    "        wq,wk,wv = torch.chunk(self.attention(x), 3, dim=-1)\n",
    "        wq,wk,wv = map(lambda x:x.view(bs, x.size(1), self.n_heads, self.d_head), (wq,wk,wv))\n",
    "        wq,wk,wv = wq.permute(0, 2, 1, 3),wk.permute(0, 2, 3, 1),wv.permute(0, 2, 1, 3)\n",
    "        attn_score = torch.matmul(wq, wk)\n",
    "        if self.scale: attn_score.div_(self.d_head ** 0.5)\n",
    "        if mask is not None:\n",
    "            minus_inf = -65504 if attn_score.dtype == torch.float16 else -1e9\n",
    "            attn_score = attn_score.masked_fill(mask, minus_inf).type_as(attn_score)\n",
    "        attn_prob = self.drop_att(F.softmax(attn_score, dim=-1))\n",
    "        attn_vec = torch.matmul(attn_prob, wv)\n",
    "        return attn_vec.permute(0, 2, 1, 3).contiguous().contiguous().view(bs, x_len, -1)\n",
    "\n",
    "def _line_shift(x:Tensor, mask:bool=False):\n",
    "    \"Shift the line i of `x` by p-i elements to the left, is `mask` puts 0s on the diagonal.\"\n",
    "    bs,nh,n,p = x.size()\n",
    "    x_pad = torch.cat([x.new_zeros(bs,nh,n,1), x], dim=3)\n",
    "    x_shift = x_pad.view(bs,nh,p + 1,n)[:,:,1:].view_as(x)\n",
    "    if mask: x_shift.mul_(torch.tril(x.new_ones(n,p), p-n)[None,None,])\n",
    "    return x_shift\n",
    "\n",
    "class DecoderLayer(Module):\n",
    "    \"Basic block of a Transformer model.\"\n",
    "    #Can't use Sequential directly cause more than one input...\n",
    "    def __init__(self, n_heads:int, d_model:int, d_model_out:int,d_head:int, d_inner:int, \n",
    "                 resid_p:float=0., attn_p:float=0.,ff_p:float=0.,bias:bool=True, scale:bool=True, \n",
    "                 act:Activation=Activation.ReLU, double_drop:bool=True, attn_cls:Callable=MultiHeadAttention):\n",
    "        self.mhra = attn_cls(n_heads, d_model, d_head, resid_p=resid_p, attn_p=attn_p, bias=bias, scale=scale)\n",
    "        self.ff   = feed_forward(d_model, d_model_out, d_inner, ff_p=ff_p, act=act, double_drop=double_drop)\n",
    "\n",
    "    def forward(self, x:Tensor, mask:Tensor=None, **kwargs): return self.ff(self.mhra(x, mask=mask, **kwargs))\n",
    "\n",
    "class Transformer(Module):\n",
    "    \"Transformer model: https://arxiv.org/abs/1706.03762.\"\n",
    "    def __init__(self, n_layers:int, n_heads:int, d_model:int, d_model_out:int,d_head:int, d_inner:int,\n",
    "                 resid_p:float=0., attn_p:float=0., ff_p:float=0., embed_p:float=0., bias:bool=True, scale:bool=True,\n",
    "                 act:Activation=Activation.ReLU, double_drop:bool=True, attn_cls:Callable=MultiHeadAttention,\n",
    "                 learned_pos_enc:bool=True, mask:bool=True, dense_out:bool=False,final_p:float=0.):\n",
    "        self.mask = mask\n",
    "        self.drop_final = nn.Dropout(final_p)\n",
    "        self.dense_out = dense_out\n",
    "        self.layers = nn.ModuleList([DecoderLayer(\n",
    "            n_heads, d_model, d_model_out if k == n_layers-1 else d_model, d_head, d_inner, \n",
    "            resid_p=resid_p, attn_p=attn_p, ff_p=ff_p, bias=bias, scale=scale, act=act, double_drop=double_drop, \n",
    "            attn_cls=attn_cls) for k in range(n_layers)])\n",
    "\n",
    "    def reset(self): pass\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        inp = x\n",
    "        if self.dense_out: out = x.new()\n",
    "        for i,layer in enumerate(self.layers):\n",
    "            inp = layer(inp, mask=mask)\n",
    "            inp_d = self.drop_final(inp)\n",
    "            if self.dense_out: out = torch.cat([out, inp_d], dim=-1)\n",
    "        return out if self.dense_out else inp_d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoleculeTransformer(Module):\n",
    "    def __init__(self,n_layers,n_heads,d_model,embed_p:float=0,final_p:float=0,d_head=None,deep_decoder=False,\n",
    "                 dense_out=False, **kwargs):\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        d_head = ifnone(d_head, d_model//n_heads)\n",
    "        self.transformer = Transformer(n_layers=n_layers,n_heads=n_heads,d_model=d_model,d_model_out=d_model,\n",
    "                                       d_head=d_head,final_p=final_p,dense_out=dense_out,**kwargs)\n",
    "        \n",
    "        channels_out = d_model*n_layers if dense_out else d_model\n",
    "        channels_out_scalar = channels_out# + n_types + 1\n",
    "        if deep_decoder:\n",
    "            sl = [int(channels_out_scalar/(2**d)) for d in range(int(math.ceil(np.log2(channels_out_scalar/4)-1)))]\n",
    "            self.scalar = nn.Sequential(*(list(itertools.chain.from_iterable(\n",
    "                [[nn.Conv1d(sl[i],sl[i+1],1),nn.ReLU(),nn.BatchNorm1d(sl[i+1])] for i in range(len(sl)-1)])) + \n",
    "                [nn.Conv1d(sl[-1], 1, 1)]))\n",
    "        else:\n",
    "            self.scalar = nn.Conv1d(channels_out_scalar,max_atoms,1)\n",
    "        self.n_type_embedding=n_type_embedding = int((d_model/2)/max_atoms)\n",
    "        n_atom_embedding = d_model - 3 - max_atoms - max_atoms*n_type_embedding\n",
    "\n",
    "        self.atom_embedding = nn.Embedding(len(atoms)+1,n_atom_embedding)\n",
    "        self.type_embedding = nn.Embedding(n_types+1,n_type_embedding)\n",
    "        self.drop_type, self.drop_atom = nn.Dropout(embed_p),nn.Dropout(embed_p)\n",
    "        self.pos_enc = PositionalEncoding(d_model)\n",
    "        \n",
    "        pos = torch.arange(0, max_atoms)\n",
    "        self.pos_one_hot = torch.zeros(1,max_atoms,max_atoms).scatter_(1,pos.expand(1,max_atoms,max_atoms),1.)\n",
    "        # bs,ref0,ref1\n",
    "        # bs,ref0,h\n",
    "\n",
    "    def forward(self,xyz,type,ext,atom,mask_atoms,n_atoms):\n",
    "        bs, _, n_pts = xyz.shape\n",
    "        # idea: count number of t > 0 and wipe out eg 10% of them, to simulate dropout on y (it's equivalent)\n",
    "        t = self.drop_type(self.type_embedding((type+1).squeeze(1))).transpose(1,2).contiguous().view(bs,n_pts,-1) \n",
    "        # b,1,ref1,ref0,n_type_embedding =>  => b,ref0,ref1,n_type_embedding => b,ref0,ref1 * n_type_embedding\n",
    "        # t_one_hot=torch.zeros(bs,n_types+1,n_pts,n_pts,device=type.device,dtype=x.dtype).scatter_(1,type+1,1.)\n",
    "        # bn,n_types+1,max_atoms(target),max_atoms(reference)\n",
    "        #t_one_hot=t_one_hot.view(bs,-1,n_pts)\n",
    "        \n",
    "        a = self.drop_atom(self.atom_embedding((atom+1).squeeze(1)))\n",
    "\n",
    "        pos_one_hot = torch.eye(max_atoms,device=xyz.device,dtype=xyz.dtype).expand(bs,max_atoms,max_atoms) * \\\n",
    "                     (1-mask_atoms.squeeze(1).type_as(xyz)) # eye matrix masked by positive mask\n",
    "        x = torch.cat([pos_one_hot,xyz.transpose(1,2),a,t], dim=-1) \n",
    "        \n",
    "        x *= math.sqrt(self.d_model) # bn,max_atoms,d_model\n",
    "        x = self.transformer(x,mask_atoms).transpose(1,2) # # bn,d_model,max_atoms (for scalar decoder)\n",
    "        \n",
    "        scalar = self.scalar(x).view(bs,-1)\n",
    "        \n",
    "        type = type.transpose(2,3).contiguous().view(bs,-1) \n",
    "        # b,1,ref1,ref0,n_type_embedding -> b,1,ref0,ref1*n_type_embedding\n",
    "        \n",
    "        ext  = ext.view(bs,1,-1) #not used for molecular transformer right now\n",
    "        mask= (type != -1).squeeze(1) #b,ref0,ref1*n_type_embedding\n",
    "        return type[mask],ext,scalar[mask].unsqueeze(-1) # we only pass non-fake couplings to loss fn\n",
    "    \n",
    "    def reset(self): pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This callback allows to insert multiple stateful (not averaged) metrics in one pass. Addditionally we could add metrics for train if we want to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model instantiation: where's all your TPUs/GPUs when you need a decent hyperparam sweep?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "net, learner = None,None\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "net = MoleculeTransformer(n_layers=6, n_heads=max_atoms*8,d_model=256,d_inner=2048,\n",
    "                      resid_p=0., attn_p=0., ff_p=0., embed_p=0, final_p=0.,\n",
    "                      deep_decoder=False, dense_out=False)\n",
    "learner = Learner(data,net, loss_func=LMAEMaskedLoss(),)\n",
    "learner.callbacks.extend([\n",
    "    SaveModelCallback(learner, monitor='👉🏻LMAE👈🏻', mode='min'),\n",
    "    LMAEMetric(learner)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MoleculeTransformer\n",
       "======================================================================\n",
       "Layer (type)         Output Shape         Param #    Trainable \n",
       "======================================================================\n",
       "Dropout              [29, 256]            0          False     \n",
       "______________________________________________________________________\n",
       "Linear               [29, 696]            178,872    True      \n",
       "______________________________________________________________________\n",
       "Linear               [29, 256]            59,648     True      \n",
       "______________________________________________________________________\n",
       "Dropout              [232, 29, 29]        0          False     \n",
       "______________________________________________________________________\n",
       "Dropout              [29, 256]            0          False     \n",
       "______________________________________________________________________\n",
       "LayerNorm            [29, 256]            512        True      \n",
       "______________________________________________________________________\n",
       "Linear               [29, 2048]           526,336    True      \n",
       "______________________________________________________________________\n",
       "ReLU                 [29, 2048]           0          False     \n",
       "______________________________________________________________________\n",
       "Dropout              [29, 2048]           0          False     \n",
       "______________________________________________________________________\n",
       "Linear               [29, 256]            524,544    True      \n",
       "______________________________________________________________________\n",
       "Dropout              [29, 256]            0          False     \n",
       "______________________________________________________________________\n",
       "MergeLayer           [29, 256]            0          False     \n",
       "______________________________________________________________________\n",
       "LayerNorm            [29, 256]            512        True      \n",
       "______________________________________________________________________\n",
       "Linear               [29, 696]            178,872    True      \n",
       "______________________________________________________________________\n",
       "Linear               [29, 256]            59,648     True      \n",
       "______________________________________________________________________\n",
       "Dropout              [232, 29, 29]        0          False     \n",
       "______________________________________________________________________\n",
       "Dropout              [29, 256]            0          False     \n",
       "______________________________________________________________________\n",
       "LayerNorm            [29, 256]            512        True      \n",
       "______________________________________________________________________\n",
       "Linear               [29, 2048]           526,336    True      \n",
       "______________________________________________________________________\n",
       "ReLU                 [29, 2048]           0          False     \n",
       "______________________________________________________________________\n",
       "Dropout              [29, 2048]           0          False     \n",
       "______________________________________________________________________\n",
       "Linear               [29, 256]            524,544    True      \n",
       "______________________________________________________________________\n",
       "Dropout              [29, 256]            0          False     \n",
       "______________________________________________________________________\n",
       "MergeLayer           [29, 256]            0          False     \n",
       "______________________________________________________________________\n",
       "LayerNorm            [29, 256]            512        True      \n",
       "______________________________________________________________________\n",
       "Linear               [29, 696]            178,872    True      \n",
       "______________________________________________________________________\n",
       "Linear               [29, 256]            59,648     True      \n",
       "______________________________________________________________________\n",
       "Dropout              [232, 29, 29]        0          False     \n",
       "______________________________________________________________________\n",
       "Dropout              [29, 256]            0          False     \n",
       "______________________________________________________________________\n",
       "LayerNorm            [29, 256]            512        True      \n",
       "______________________________________________________________________\n",
       "Linear               [29, 2048]           526,336    True      \n",
       "______________________________________________________________________\n",
       "ReLU                 [29, 2048]           0          False     \n",
       "______________________________________________________________________\n",
       "Dropout              [29, 2048]           0          False     \n",
       "______________________________________________________________________\n",
       "Linear               [29, 256]            524,544    True      \n",
       "______________________________________________________________________\n",
       "Dropout              [29, 256]            0          False     \n",
       "______________________________________________________________________\n",
       "MergeLayer           [29, 256]            0          False     \n",
       "______________________________________________________________________\n",
       "LayerNorm            [29, 256]            512        True      \n",
       "______________________________________________________________________\n",
       "Linear               [29, 696]            178,872    True      \n",
       "______________________________________________________________________\n",
       "Linear               [29, 256]            59,648     True      \n",
       "______________________________________________________________________\n",
       "Dropout              [232, 29, 29]        0          False     \n",
       "______________________________________________________________________\n",
       "Dropout              [29, 256]            0          False     \n",
       "______________________________________________________________________\n",
       "LayerNorm            [29, 256]            512        True      \n",
       "______________________________________________________________________\n",
       "Linear               [29, 2048]           526,336    True      \n",
       "______________________________________________________________________\n",
       "ReLU                 [29, 2048]           0          False     \n",
       "______________________________________________________________________\n",
       "Dropout              [29, 2048]           0          False     \n",
       "______________________________________________________________________\n",
       "Linear               [29, 256]            524,544    True      \n",
       "______________________________________________________________________\n",
       "Dropout              [29, 256]            0          False     \n",
       "______________________________________________________________________\n",
       "MergeLayer           [29, 256]            0          False     \n",
       "______________________________________________________________________\n",
       "LayerNorm            [29, 256]            512        True      \n",
       "______________________________________________________________________\n",
       "Linear               [29, 696]            178,872    True      \n",
       "______________________________________________________________________\n",
       "Linear               [29, 256]            59,648     True      \n",
       "______________________________________________________________________\n",
       "Dropout              [232, 29, 29]        0          False     \n",
       "______________________________________________________________________\n",
       "Dropout              [29, 256]            0          False     \n",
       "______________________________________________________________________\n",
       "LayerNorm            [29, 256]            512        True      \n",
       "______________________________________________________________________\n",
       "Linear               [29, 2048]           526,336    True      \n",
       "______________________________________________________________________\n",
       "ReLU                 [29, 2048]           0          False     \n",
       "______________________________________________________________________\n",
       "Dropout              [29, 2048]           0          False     \n",
       "______________________________________________________________________\n",
       "Linear               [29, 256]            524,544    True      \n",
       "______________________________________________________________________\n",
       "Dropout              [29, 256]            0          False     \n",
       "______________________________________________________________________\n",
       "MergeLayer           [29, 256]            0          False     \n",
       "______________________________________________________________________\n",
       "LayerNorm            [29, 256]            512        True      \n",
       "______________________________________________________________________\n",
       "Linear               [29, 696]            178,872    True      \n",
       "______________________________________________________________________\n",
       "Linear               [29, 256]            59,648     True      \n",
       "______________________________________________________________________\n",
       "Dropout              [232, 29, 29]        0          False     \n",
       "______________________________________________________________________\n",
       "Dropout              [29, 256]            0          False     \n",
       "______________________________________________________________________\n",
       "LayerNorm            [29, 256]            512        True      \n",
       "______________________________________________________________________\n",
       "Linear               [29, 2048]           526,336    True      \n",
       "______________________________________________________________________\n",
       "ReLU                 [29, 2048]           0          False     \n",
       "______________________________________________________________________\n",
       "Dropout              [29, 2048]           0          False     \n",
       "______________________________________________________________________\n",
       "Linear               [29, 256]            524,544    True      \n",
       "______________________________________________________________________\n",
       "Dropout              [29, 256]            0          False     \n",
       "______________________________________________________________________\n",
       "MergeLayer           [29, 256]            0          False     \n",
       "______________________________________________________________________\n",
       "LayerNorm            [29, 256]            512        True      \n",
       "______________________________________________________________________\n",
       "Conv1d               [29, 29]             7,453      True      \n",
       "______________________________________________________________________\n",
       "Embedding            [29, 108]            648        True      \n",
       "______________________________________________________________________\n",
       "Embedding            [29, 29, 4]          36         True      \n",
       "______________________________________________________________________\n",
       "Dropout              [29, 29, 4]          0          False     \n",
       "______________________________________________________________________\n",
       "Dropout              [29, 108]            0          False     \n",
       "______________________________________________________________________\n",
       "\n",
       "Total params: 7,750,681\n",
       "Total trainable params: 7,750,681\n",
       "Total non-trainable params: 0\n",
       "Optimized with 'torch.optim.adam.Adam', betas=(0.9, 0.99)\n",
       "Using true weight decay as discussed in https://www.fast.ai/2018/07/02/adam-weight-decay/ \n",
       "Loss function : LMAEMaskedLoss\n",
       "======================================================================\n",
       "Callbacks functions applied \n",
       "    SaveModelCallback\n",
       "    LMAEMetric"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MoleculeTransformer(\n",
       "  (transformer): Transformer(\n",
       "    (drop_final): Dropout(p=0.0)\n",
       "    (layers): ModuleList(\n",
       "      (0): DecoderLayer(\n",
       "        (mhra): MultiHeadAttention(\n",
       "          (attention): Linear(in_features=256, out_features=696, bias=True)\n",
       "          (out): Linear(in_features=232, out_features=256, bias=True)\n",
       "          (drop_att): Dropout(p=0.0)\n",
       "          (drop_res): Dropout(p=0.0)\n",
       "          (ln): LayerNorm(torch.Size([256]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=256, out_features=2048, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.0)\n",
       "            (3): Linear(in_features=2048, out_features=256, bias=True)\n",
       "            (4): Dropout(p=0.0)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm(torch.Size([256]), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): DecoderLayer(\n",
       "        (mhra): MultiHeadAttention(\n",
       "          (attention): Linear(in_features=256, out_features=696, bias=True)\n",
       "          (out): Linear(in_features=232, out_features=256, bias=True)\n",
       "          (drop_att): Dropout(p=0.0)\n",
       "          (drop_res): Dropout(p=0.0)\n",
       "          (ln): LayerNorm(torch.Size([256]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=256, out_features=2048, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.0)\n",
       "            (3): Linear(in_features=2048, out_features=256, bias=True)\n",
       "            (4): Dropout(p=0.0)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm(torch.Size([256]), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): DecoderLayer(\n",
       "        (mhra): MultiHeadAttention(\n",
       "          (attention): Linear(in_features=256, out_features=696, bias=True)\n",
       "          (out): Linear(in_features=232, out_features=256, bias=True)\n",
       "          (drop_att): Dropout(p=0.0)\n",
       "          (drop_res): Dropout(p=0.0)\n",
       "          (ln): LayerNorm(torch.Size([256]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=256, out_features=2048, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.0)\n",
       "            (3): Linear(in_features=2048, out_features=256, bias=True)\n",
       "            (4): Dropout(p=0.0)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm(torch.Size([256]), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): DecoderLayer(\n",
       "        (mhra): MultiHeadAttention(\n",
       "          (attention): Linear(in_features=256, out_features=696, bias=True)\n",
       "          (out): Linear(in_features=232, out_features=256, bias=True)\n",
       "          (drop_att): Dropout(p=0.0)\n",
       "          (drop_res): Dropout(p=0.0)\n",
       "          (ln): LayerNorm(torch.Size([256]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=256, out_features=2048, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.0)\n",
       "            (3): Linear(in_features=2048, out_features=256, bias=True)\n",
       "            (4): Dropout(p=0.0)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm(torch.Size([256]), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): DecoderLayer(\n",
       "        (mhra): MultiHeadAttention(\n",
       "          (attention): Linear(in_features=256, out_features=696, bias=True)\n",
       "          (out): Linear(in_features=232, out_features=256, bias=True)\n",
       "          (drop_att): Dropout(p=0.0)\n",
       "          (drop_res): Dropout(p=0.0)\n",
       "          (ln): LayerNorm(torch.Size([256]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=256, out_features=2048, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.0)\n",
       "            (3): Linear(in_features=2048, out_features=256, bias=True)\n",
       "            (4): Dropout(p=0.0)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm(torch.Size([256]), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): DecoderLayer(\n",
       "        (mhra): MultiHeadAttention(\n",
       "          (attention): Linear(in_features=256, out_features=696, bias=True)\n",
       "          (out): Linear(in_features=232, out_features=256, bias=True)\n",
       "          (drop_att): Dropout(p=0.0)\n",
       "          (drop_res): Dropout(p=0.0)\n",
       "          (ln): LayerNorm(torch.Size([256]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=256, out_features=2048, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.0)\n",
       "            (3): Linear(in_features=2048, out_features=256, bias=True)\n",
       "            (4): Dropout(p=0.0)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm(torch.Size([256]), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (scalar): Conv1d(256, 29, kernel_size=(1,), stride=(1,))\n",
       "  (atom_embedding): Embedding(6, 108)\n",
       "  (type_embedding): Embedding(9, 4)\n",
       "  (drop_type): Dropout(p=0)\n",
       "  (drop_atom): Dropout(p=0)\n",
       "  (pos_enc): PositionalEncoding()\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sub_fname = \"loss-1.4032val-1.4586\" # uncomment or set None to skip loading trained net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOT loaded!  name 'sub_fname' is not defined\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print(f\"Attempting to load: {sub_fname}... \", end=\"\")\n",
    "    learner.load(sub_fname, strict=False,with_opt=False)\n",
    "    print(\"Loaded\")\n",
    "except Exception as e:\n",
    "    print(\"NOT loaded! \", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# neural weight manual transplant \n",
    "if False:\n",
    "    for name,child in learner.model.named_children():\n",
    "        print(\"CHILD: \",name)\n",
    "        if not (name in ['scalar', 'magnetic', 'dipole', 'potential']):\n",
    "            print(\"FREEZING\")\n",
    "            for param in child.parameters(): param.requires_grad = False\n",
    "        else:\n",
    "            for name,param in child.named_parameters(): \n",
    "                param.requires_grad = True\n",
    "    learner.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learner = learner.to_parallel()#.to_fp16()#loss_scale=128, dynamic=False)\n",
    "data.batch_size = int(2048//2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Real loss func. Need to test different auxiliary tasks weights: `magnetic_w`, `dipole_w`, `potential`, weights of indivial `lmae`s: `types_w`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learner.loss_func = LMAEMaskedLoss(types_w = [1] + [1] * (n_types-1))#, proxy_log=lambda x: x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n",
      "Min numerical gradient: 2.09E-05\n",
      "Min loss divided by 10: 4.79E-03\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "! rm models/bestmodel.pth\n",
    "learner.lr_find()\n",
    "learner.recorder.plot(suggestion=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='82' class='' max='100', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      82.00% [82/100 48:23<10:37]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>👉🏻LMAE👈🏻</th>\n",
       "      <th>lmae0</th>\n",
       "      <th>lmae1</th>\n",
       "      <th>lmae2</th>\n",
       "      <th>lmae3</th>\n",
       "      <th>lmae4</th>\n",
       "      <th>lmae5</th>\n",
       "      <th>lmae6</th>\n",
       "      <th>lmae7</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.905666</td>\n",
       "      <td>1.859223</td>\n",
       "      <td>1.870605</td>\n",
       "      <td>4.546057</td>\n",
       "      <td>2.376478</td>\n",
       "      <td>3.830608</td>\n",
       "      <td>1.040156</td>\n",
       "      <td>1.046630</td>\n",
       "      <td>1.167702</td>\n",
       "      <td>1.026674</td>\n",
       "      <td>-0.069464</td>\n",
       "      <td>00:34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.827244</td>\n",
       "      <td>1.766647</td>\n",
       "      <td>1.775967</td>\n",
       "      <td>4.540597</td>\n",
       "      <td>2.103292</td>\n",
       "      <td>3.806127</td>\n",
       "      <td>0.923722</td>\n",
       "      <td>0.903475</td>\n",
       "      <td>1.101794</td>\n",
       "      <td>0.940813</td>\n",
       "      <td>-0.112081</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.740159</td>\n",
       "      <td>1.677849</td>\n",
       "      <td>1.684329</td>\n",
       "      <td>4.535095</td>\n",
       "      <td>1.525932</td>\n",
       "      <td>3.789322</td>\n",
       "      <td>0.821037</td>\n",
       "      <td>0.872838</td>\n",
       "      <td>1.106949</td>\n",
       "      <td>0.931750</td>\n",
       "      <td>-0.108293</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.650856</td>\n",
       "      <td>1.594077</td>\n",
       "      <td>1.597968</td>\n",
       "      <td>4.526427</td>\n",
       "      <td>1.114435</td>\n",
       "      <td>3.767586</td>\n",
       "      <td>0.724629</td>\n",
       "      <td>0.843907</td>\n",
       "      <td>1.050063</td>\n",
       "      <td>0.901835</td>\n",
       "      <td>-0.145139</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.572899</td>\n",
       "      <td>1.526586</td>\n",
       "      <td>1.528076</td>\n",
       "      <td>4.518251</td>\n",
       "      <td>0.798673</td>\n",
       "      <td>3.744238</td>\n",
       "      <td>0.623540</td>\n",
       "      <td>0.809794</td>\n",
       "      <td>1.008402</td>\n",
       "      <td>0.889991</td>\n",
       "      <td>-0.168282</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.508008</td>\n",
       "      <td>1.480635</td>\n",
       "      <td>1.480324</td>\n",
       "      <td>4.509372</td>\n",
       "      <td>0.619437</td>\n",
       "      <td>3.720711</td>\n",
       "      <td>0.541503</td>\n",
       "      <td>0.785871</td>\n",
       "      <td>0.983742</td>\n",
       "      <td>0.867914</td>\n",
       "      <td>-0.185958</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.455273</td>\n",
       "      <td>1.442232</td>\n",
       "      <td>1.441452</td>\n",
       "      <td>4.494233</td>\n",
       "      <td>0.496351</td>\n",
       "      <td>3.682512</td>\n",
       "      <td>0.503628</td>\n",
       "      <td>0.767169</td>\n",
       "      <td>0.929889</td>\n",
       "      <td>0.846440</td>\n",
       "      <td>-0.188604</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.414461</td>\n",
       "      <td>1.400113</td>\n",
       "      <td>1.398948</td>\n",
       "      <td>4.483895</td>\n",
       "      <td>0.398871</td>\n",
       "      <td>3.648844</td>\n",
       "      <td>0.396601</td>\n",
       "      <td>0.728027</td>\n",
       "      <td>0.908696</td>\n",
       "      <td>0.831052</td>\n",
       "      <td>-0.204403</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.378006</td>\n",
       "      <td>1.358339</td>\n",
       "      <td>1.356891</td>\n",
       "      <td>4.468681</td>\n",
       "      <td>0.283400</td>\n",
       "      <td>3.597817</td>\n",
       "      <td>0.327198</td>\n",
       "      <td>0.706252</td>\n",
       "      <td>0.879043</td>\n",
       "      <td>0.822539</td>\n",
       "      <td>-0.229805</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.340880</td>\n",
       "      <td>1.333972</td>\n",
       "      <td>1.332705</td>\n",
       "      <td>4.449141</td>\n",
       "      <td>0.278329</td>\n",
       "      <td>3.537078</td>\n",
       "      <td>0.271552</td>\n",
       "      <td>0.683047</td>\n",
       "      <td>0.866503</td>\n",
       "      <td>0.817405</td>\n",
       "      <td>-0.241417</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.304962</td>\n",
       "      <td>1.294892</td>\n",
       "      <td>1.293647</td>\n",
       "      <td>4.430522</td>\n",
       "      <td>0.196925</td>\n",
       "      <td>3.467276</td>\n",
       "      <td>0.231830</td>\n",
       "      <td>0.653613</td>\n",
       "      <td>0.847578</td>\n",
       "      <td>0.800519</td>\n",
       "      <td>-0.279089</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.271363</td>\n",
       "      <td>1.265318</td>\n",
       "      <td>1.264286</td>\n",
       "      <td>4.405361</td>\n",
       "      <td>0.162298</td>\n",
       "      <td>3.382707</td>\n",
       "      <td>0.189191</td>\n",
       "      <td>0.625477</td>\n",
       "      <td>0.839611</td>\n",
       "      <td>0.792755</td>\n",
       "      <td>-0.283114</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.235290</td>\n",
       "      <td>1.221458</td>\n",
       "      <td>1.220667</td>\n",
       "      <td>4.380414</td>\n",
       "      <td>0.097539</td>\n",
       "      <td>3.297477</td>\n",
       "      <td>0.132082</td>\n",
       "      <td>0.591013</td>\n",
       "      <td>0.810754</td>\n",
       "      <td>0.778380</td>\n",
       "      <td>-0.322324</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.199027</td>\n",
       "      <td>1.202635</td>\n",
       "      <td>1.202578</td>\n",
       "      <td>4.349931</td>\n",
       "      <td>0.095012</td>\n",
       "      <td>3.208610</td>\n",
       "      <td>0.138351</td>\n",
       "      <td>0.558681</td>\n",
       "      <td>0.806477</td>\n",
       "      <td>0.770597</td>\n",
       "      <td>-0.307032</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.161816</td>\n",
       "      <td>1.175538</td>\n",
       "      <td>1.176709</td>\n",
       "      <td>4.315828</td>\n",
       "      <td>0.139635</td>\n",
       "      <td>3.088686</td>\n",
       "      <td>0.101089</td>\n",
       "      <td>0.537464</td>\n",
       "      <td>0.801140</td>\n",
       "      <td>0.754104</td>\n",
       "      <td>-0.324275</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.122247</td>\n",
       "      <td>1.121948</td>\n",
       "      <td>1.124607</td>\n",
       "      <td>4.274641</td>\n",
       "      <td>0.048972</td>\n",
       "      <td>2.932811</td>\n",
       "      <td>0.074051</td>\n",
       "      <td>0.489268</td>\n",
       "      <td>0.784012</td>\n",
       "      <td>0.741518</td>\n",
       "      <td>-0.348419</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.078261</td>\n",
       "      <td>1.066749</td>\n",
       "      <td>1.071532</td>\n",
       "      <td>4.221581</td>\n",
       "      <td>0.001417</td>\n",
       "      <td>2.728584</td>\n",
       "      <td>0.022347</td>\n",
       "      <td>0.468653</td>\n",
       "      <td>0.755045</td>\n",
       "      <td>0.729177</td>\n",
       "      <td>-0.354543</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.031122</td>\n",
       "      <td>1.034135</td>\n",
       "      <td>1.040773</td>\n",
       "      <td>4.163990</td>\n",
       "      <td>0.011566</td>\n",
       "      <td>2.530597</td>\n",
       "      <td>0.023850</td>\n",
       "      <td>0.462296</td>\n",
       "      <td>0.746903</td>\n",
       "      <td>0.729678</td>\n",
       "      <td>-0.342694</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.981817</td>\n",
       "      <td>0.971961</td>\n",
       "      <td>0.977796</td>\n",
       "      <td>4.094580</td>\n",
       "      <td>-0.044662</td>\n",
       "      <td>2.324116</td>\n",
       "      <td>-0.022198</td>\n",
       "      <td>0.415497</td>\n",
       "      <td>0.731009</td>\n",
       "      <td>0.716487</td>\n",
       "      <td>-0.392462</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.931503</td>\n",
       "      <td>0.935122</td>\n",
       "      <td>0.940145</td>\n",
       "      <td>4.011613</td>\n",
       "      <td>0.014585</td>\n",
       "      <td>2.023726</td>\n",
       "      <td>-0.015677</td>\n",
       "      <td>0.418142</td>\n",
       "      <td>0.738147</td>\n",
       "      <td>0.711594</td>\n",
       "      <td>-0.380970</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.867600</td>\n",
       "      <td>0.853997</td>\n",
       "      <td>0.857668</td>\n",
       "      <td>3.892682</td>\n",
       "      <td>-0.067105</td>\n",
       "      <td>1.600758</td>\n",
       "      <td>-0.009495</td>\n",
       "      <td>0.404561</td>\n",
       "      <td>0.731544</td>\n",
       "      <td>0.704840</td>\n",
       "      <td>-0.396442</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.802154</td>\n",
       "      <td>0.809770</td>\n",
       "      <td>0.808164</td>\n",
       "      <td>3.792315</td>\n",
       "      <td>-0.007617</td>\n",
       "      <td>1.274889</td>\n",
       "      <td>-0.039780</td>\n",
       "      <td>0.393826</td>\n",
       "      <td>0.718168</td>\n",
       "      <td>0.700707</td>\n",
       "      <td>-0.367197</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.747399</td>\n",
       "      <td>0.765398</td>\n",
       "      <td>0.762067</td>\n",
       "      <td>3.727558</td>\n",
       "      <td>-0.085048</td>\n",
       "      <td>1.149136</td>\n",
       "      <td>-0.087382</td>\n",
       "      <td>0.369366</td>\n",
       "      <td>0.713579</td>\n",
       "      <td>0.694339</td>\n",
       "      <td>-0.385013</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.706357</td>\n",
       "      <td>0.738757</td>\n",
       "      <td>0.735834</td>\n",
       "      <td>3.665965</td>\n",
       "      <td>-0.044895</td>\n",
       "      <td>0.978258</td>\n",
       "      <td>-0.077311</td>\n",
       "      <td>0.348867</td>\n",
       "      <td>0.706281</td>\n",
       "      <td>0.698586</td>\n",
       "      <td>-0.389074</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.670372</td>\n",
       "      <td>0.711915</td>\n",
       "      <td>0.708347</td>\n",
       "      <td>3.616460</td>\n",
       "      <td>-0.033894</td>\n",
       "      <td>0.902228</td>\n",
       "      <td>-0.101067</td>\n",
       "      <td>0.354161</td>\n",
       "      <td>0.699080</td>\n",
       "      <td>0.686460</td>\n",
       "      <td>-0.456653</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.639665</td>\n",
       "      <td>0.684379</td>\n",
       "      <td>0.680947</td>\n",
       "      <td>3.549757</td>\n",
       "      <td>-0.128027</td>\n",
       "      <td>0.836985</td>\n",
       "      <td>-0.098337</td>\n",
       "      <td>0.345496</td>\n",
       "      <td>0.694366</td>\n",
       "      <td>0.683345</td>\n",
       "      <td>-0.436007</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.612792</td>\n",
       "      <td>0.678584</td>\n",
       "      <td>0.674656</td>\n",
       "      <td>3.495118</td>\n",
       "      <td>-0.085195</td>\n",
       "      <td>0.818501</td>\n",
       "      <td>-0.116289</td>\n",
       "      <td>0.359073</td>\n",
       "      <td>0.686743</td>\n",
       "      <td>0.673734</td>\n",
       "      <td>-0.434437</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.587457</td>\n",
       "      <td>0.651411</td>\n",
       "      <td>0.646046</td>\n",
       "      <td>3.447323</td>\n",
       "      <td>-0.115197</td>\n",
       "      <td>0.785959</td>\n",
       "      <td>-0.149524</td>\n",
       "      <td>0.305694</td>\n",
       "      <td>0.686510</td>\n",
       "      <td>0.663898</td>\n",
       "      <td>-0.456298</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.561663</td>\n",
       "      <td>0.654501</td>\n",
       "      <td>0.651876</td>\n",
       "      <td>3.361189</td>\n",
       "      <td>-0.006892</td>\n",
       "      <td>0.812152</td>\n",
       "      <td>-0.151146</td>\n",
       "      <td>0.310980</td>\n",
       "      <td>0.672800</td>\n",
       "      <td>0.661501</td>\n",
       "      <td>-0.445574</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.537112</td>\n",
       "      <td>0.616941</td>\n",
       "      <td>0.612525</td>\n",
       "      <td>3.303470</td>\n",
       "      <td>-0.053992</td>\n",
       "      <td>0.696421</td>\n",
       "      <td>-0.182121</td>\n",
       "      <td>0.306587</td>\n",
       "      <td>0.659640</td>\n",
       "      <td>0.649613</td>\n",
       "      <td>-0.479414</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.508010</td>\n",
       "      <td>0.579549</td>\n",
       "      <td>0.576217</td>\n",
       "      <td>3.194506</td>\n",
       "      <td>-0.160629</td>\n",
       "      <td>0.690249</td>\n",
       "      <td>-0.193107</td>\n",
       "      <td>0.274938</td>\n",
       "      <td>0.655705</td>\n",
       "      <td>0.643355</td>\n",
       "      <td>-0.495284</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.478900</td>\n",
       "      <td>0.554242</td>\n",
       "      <td>0.550956</td>\n",
       "      <td>3.050521</td>\n",
       "      <td>-0.163909</td>\n",
       "      <td>0.700278</td>\n",
       "      <td>-0.217022</td>\n",
       "      <td>0.264333</td>\n",
       "      <td>0.650119</td>\n",
       "      <td>0.638939</td>\n",
       "      <td>-0.515608</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.444296</td>\n",
       "      <td>0.524649</td>\n",
       "      <td>0.520973</td>\n",
       "      <td>2.869186</td>\n",
       "      <td>-0.148514</td>\n",
       "      <td>0.634420</td>\n",
       "      <td>-0.218704</td>\n",
       "      <td>0.261518</td>\n",
       "      <td>0.641677</td>\n",
       "      <td>0.633292</td>\n",
       "      <td>-0.505090</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.407525</td>\n",
       "      <td>0.492512</td>\n",
       "      <td>0.489120</td>\n",
       "      <td>2.675177</td>\n",
       "      <td>-0.191619</td>\n",
       "      <td>0.690586</td>\n",
       "      <td>-0.243190</td>\n",
       "      <td>0.252630</td>\n",
       "      <td>0.639021</td>\n",
       "      <td>0.630133</td>\n",
       "      <td>-0.539774</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.367982</td>\n",
       "      <td>0.462486</td>\n",
       "      <td>0.458660</td>\n",
       "      <td>2.453750</td>\n",
       "      <td>-0.179658</td>\n",
       "      <td>0.637106</td>\n",
       "      <td>-0.242755</td>\n",
       "      <td>0.258225</td>\n",
       "      <td>0.623768</td>\n",
       "      <td>0.625931</td>\n",
       "      <td>-0.507091</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.327552</td>\n",
       "      <td>0.415995</td>\n",
       "      <td>0.413764</td>\n",
       "      <td>2.289676</td>\n",
       "      <td>-0.225962</td>\n",
       "      <td>0.538820</td>\n",
       "      <td>-0.240639</td>\n",
       "      <td>0.236213</td>\n",
       "      <td>0.616774</td>\n",
       "      <td>0.623756</td>\n",
       "      <td>-0.528527</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.294985</td>\n",
       "      <td>0.393351</td>\n",
       "      <td>0.391046</td>\n",
       "      <td>2.151922</td>\n",
       "      <td>-0.227706</td>\n",
       "      <td>0.570847</td>\n",
       "      <td>-0.285764</td>\n",
       "      <td>0.237534</td>\n",
       "      <td>0.622549</td>\n",
       "      <td>0.611981</td>\n",
       "      <td>-0.552995</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.261597</td>\n",
       "      <td>0.378924</td>\n",
       "      <td>0.375658</td>\n",
       "      <td>2.022102</td>\n",
       "      <td>-0.227776</td>\n",
       "      <td>0.606008</td>\n",
       "      <td>-0.281110</td>\n",
       "      <td>0.223783</td>\n",
       "      <td>0.606215</td>\n",
       "      <td>0.610775</td>\n",
       "      <td>-0.554736</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.226032</td>\n",
       "      <td>0.335918</td>\n",
       "      <td>0.333638</td>\n",
       "      <td>1.906760</td>\n",
       "      <td>-0.285252</td>\n",
       "      <td>0.537986</td>\n",
       "      <td>-0.297226</td>\n",
       "      <td>0.202373</td>\n",
       "      <td>0.594684</td>\n",
       "      <td>0.593579</td>\n",
       "      <td>-0.583797</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.194325</td>\n",
       "      <td>0.330033</td>\n",
       "      <td>0.329237</td>\n",
       "      <td>1.809945</td>\n",
       "      <td>-0.245463</td>\n",
       "      <td>0.575449</td>\n",
       "      <td>-0.311719</td>\n",
       "      <td>0.198631</td>\n",
       "      <td>0.586793</td>\n",
       "      <td>0.590520</td>\n",
       "      <td>-0.570261</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.167045</td>\n",
       "      <td>0.303894</td>\n",
       "      <td>0.300995</td>\n",
       "      <td>1.724689</td>\n",
       "      <td>-0.287035</td>\n",
       "      <td>0.538869</td>\n",
       "      <td>-0.324856</td>\n",
       "      <td>0.177901</td>\n",
       "      <td>0.577863</td>\n",
       "      <td>0.583446</td>\n",
       "      <td>-0.582915</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.140528</td>\n",
       "      <td>0.272293</td>\n",
       "      <td>0.268987</td>\n",
       "      <td>1.653569</td>\n",
       "      <td>-0.307014</td>\n",
       "      <td>0.477995</td>\n",
       "      <td>-0.321865</td>\n",
       "      <td>0.169626</td>\n",
       "      <td>0.520757</td>\n",
       "      <td>0.573607</td>\n",
       "      <td>-0.614780</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.101849</td>\n",
       "      <td>0.247846</td>\n",
       "      <td>0.242518</td>\n",
       "      <td>1.589351</td>\n",
       "      <td>-0.305024</td>\n",
       "      <td>0.502104</td>\n",
       "      <td>-0.375417</td>\n",
       "      <td>0.151798</td>\n",
       "      <td>0.458591</td>\n",
       "      <td>0.552310</td>\n",
       "      <td>-0.633565</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.068597</td>\n",
       "      <td>0.227215</td>\n",
       "      <td>0.220787</td>\n",
       "      <td>1.545624</td>\n",
       "      <td>-0.343418</td>\n",
       "      <td>0.473517</td>\n",
       "      <td>-0.382902</td>\n",
       "      <td>0.153303</td>\n",
       "      <td>0.408810</td>\n",
       "      <td>0.545158</td>\n",
       "      <td>-0.633799</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.040968</td>\n",
       "      <td>0.217589</td>\n",
       "      <td>0.212503</td>\n",
       "      <td>1.566738</td>\n",
       "      <td>-0.360079</td>\n",
       "      <td>0.461161</td>\n",
       "      <td>-0.384588</td>\n",
       "      <td>0.138996</td>\n",
       "      <td>0.380329</td>\n",
       "      <td>0.537217</td>\n",
       "      <td>-0.639752</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.009687</td>\n",
       "      <td>0.181076</td>\n",
       "      <td>0.176199</td>\n",
       "      <td>1.459513</td>\n",
       "      <td>-0.393161</td>\n",
       "      <td>0.432858</td>\n",
       "      <td>-0.418990</td>\n",
       "      <td>0.123119</td>\n",
       "      <td>0.342659</td>\n",
       "      <td>0.520772</td>\n",
       "      <td>-0.657175</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>-0.016570</td>\n",
       "      <td>0.166692</td>\n",
       "      <td>0.161661</td>\n",
       "      <td>1.411480</td>\n",
       "      <td>-0.402675</td>\n",
       "      <td>0.422776</td>\n",
       "      <td>-0.418760</td>\n",
       "      <td>0.125441</td>\n",
       "      <td>0.322639</td>\n",
       "      <td>0.512296</td>\n",
       "      <td>-0.679907</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>-0.043625</td>\n",
       "      <td>0.139756</td>\n",
       "      <td>0.135269</td>\n",
       "      <td>1.385877</td>\n",
       "      <td>-0.446158</td>\n",
       "      <td>0.400724</td>\n",
       "      <td>-0.460393</td>\n",
       "      <td>0.102623</td>\n",
       "      <td>0.292530</td>\n",
       "      <td>0.497661</td>\n",
       "      <td>-0.690710</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>-0.068776</td>\n",
       "      <td>0.129215</td>\n",
       "      <td>0.124511</td>\n",
       "      <td>1.355608</td>\n",
       "      <td>-0.450157</td>\n",
       "      <td>0.387212</td>\n",
       "      <td>-0.467483</td>\n",
       "      <td>0.096734</td>\n",
       "      <td>0.282488</td>\n",
       "      <td>0.488764</td>\n",
       "      <td>-0.697079</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>-0.083703</td>\n",
       "      <td>0.125356</td>\n",
       "      <td>0.120217</td>\n",
       "      <td>1.341866</td>\n",
       "      <td>-0.402673</td>\n",
       "      <td>0.385717</td>\n",
       "      <td>-0.474795</td>\n",
       "      <td>0.082752</td>\n",
       "      <td>0.262507</td>\n",
       "      <td>0.477456</td>\n",
       "      <td>-0.711093</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>-0.110345</td>\n",
       "      <td>0.105004</td>\n",
       "      <td>0.099928</td>\n",
       "      <td>1.310849</td>\n",
       "      <td>-0.474370</td>\n",
       "      <td>0.375517</td>\n",
       "      <td>-0.495002</td>\n",
       "      <td>0.074782</td>\n",
       "      <td>0.243791</td>\n",
       "      <td>0.476926</td>\n",
       "      <td>-0.713066</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>-0.134654</td>\n",
       "      <td>0.099749</td>\n",
       "      <td>0.093545</td>\n",
       "      <td>1.295333</td>\n",
       "      <td>-0.490636</td>\n",
       "      <td>0.391503</td>\n",
       "      <td>-0.497166</td>\n",
       "      <td>0.069465</td>\n",
       "      <td>0.237986</td>\n",
       "      <td>0.470157</td>\n",
       "      <td>-0.728281</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>-0.154042</td>\n",
       "      <td>0.081450</td>\n",
       "      <td>0.077371</td>\n",
       "      <td>1.277487</td>\n",
       "      <td>-0.503035</td>\n",
       "      <td>0.351773</td>\n",
       "      <td>-0.506054</td>\n",
       "      <td>0.052758</td>\n",
       "      <td>0.218374</td>\n",
       "      <td>0.460796</td>\n",
       "      <td>-0.733128</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>-0.177073</td>\n",
       "      <td>0.076183</td>\n",
       "      <td>0.070879</td>\n",
       "      <td>1.270345</td>\n",
       "      <td>-0.509079</td>\n",
       "      <td>0.331439</td>\n",
       "      <td>-0.512282</td>\n",
       "      <td>0.055074</td>\n",
       "      <td>0.213359</td>\n",
       "      <td>0.449932</td>\n",
       "      <td>-0.731758</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>-0.197158</td>\n",
       "      <td>0.067438</td>\n",
       "      <td>0.063427</td>\n",
       "      <td>1.253956</td>\n",
       "      <td>-0.484785</td>\n",
       "      <td>0.320405</td>\n",
       "      <td>-0.523398</td>\n",
       "      <td>0.046193</td>\n",
       "      <td>0.194659</td>\n",
       "      <td>0.449045</td>\n",
       "      <td>-0.748658</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>-0.223583</td>\n",
       "      <td>0.051973</td>\n",
       "      <td>0.046516</td>\n",
       "      <td>1.243448</td>\n",
       "      <td>-0.527391</td>\n",
       "      <td>0.297129</td>\n",
       "      <td>-0.552137</td>\n",
       "      <td>0.040201</td>\n",
       "      <td>0.184895</td>\n",
       "      <td>0.443425</td>\n",
       "      <td>-0.757447</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>-0.243862</td>\n",
       "      <td>0.046066</td>\n",
       "      <td>0.041076</td>\n",
       "      <td>1.229412</td>\n",
       "      <td>-0.528554</td>\n",
       "      <td>0.329255</td>\n",
       "      <td>-0.555928</td>\n",
       "      <td>0.022808</td>\n",
       "      <td>0.167493</td>\n",
       "      <td>0.433153</td>\n",
       "      <td>-0.769034</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>-0.264053</td>\n",
       "      <td>0.032719</td>\n",
       "      <td>0.028233</td>\n",
       "      <td>1.211926</td>\n",
       "      <td>-0.534953</td>\n",
       "      <td>0.285383</td>\n",
       "      <td>-0.569212</td>\n",
       "      <td>0.018476</td>\n",
       "      <td>0.162375</td>\n",
       "      <td>0.427054</td>\n",
       "      <td>-0.775189</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>-0.284882</td>\n",
       "      <td>0.022948</td>\n",
       "      <td>0.018195</td>\n",
       "      <td>1.190027</td>\n",
       "      <td>-0.551993</td>\n",
       "      <td>0.270752</td>\n",
       "      <td>-0.575584</td>\n",
       "      <td>0.012741</td>\n",
       "      <td>0.151443</td>\n",
       "      <td>0.424331</td>\n",
       "      <td>-0.776158</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>-0.308849</td>\n",
       "      <td>0.016524</td>\n",
       "      <td>0.013017</td>\n",
       "      <td>1.189646</td>\n",
       "      <td>-0.562868</td>\n",
       "      <td>0.296789</td>\n",
       "      <td>-0.583460</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.141257</td>\n",
       "      <td>0.417289</td>\n",
       "      <td>-0.794579</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>-0.333968</td>\n",
       "      <td>0.010293</td>\n",
       "      <td>0.005423</td>\n",
       "      <td>1.189021</td>\n",
       "      <td>-0.571184</td>\n",
       "      <td>0.255969</td>\n",
       "      <td>-0.597920</td>\n",
       "      <td>0.001886</td>\n",
       "      <td>0.138219</td>\n",
       "      <td>0.416532</td>\n",
       "      <td>-0.789141</td>\n",
       "      <td>00:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>-0.356150</td>\n",
       "      <td>0.000614</td>\n",
       "      <td>-0.005246</td>\n",
       "      <td>1.177368</td>\n",
       "      <td>-0.576240</td>\n",
       "      <td>0.229185</td>\n",
       "      <td>-0.603545</td>\n",
       "      <td>-0.006899</td>\n",
       "      <td>0.131439</td>\n",
       "      <td>0.408379</td>\n",
       "      <td>-0.801658</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>-0.379510</td>\n",
       "      <td>-0.002470</td>\n",
       "      <td>-0.006362</td>\n",
       "      <td>1.184882</td>\n",
       "      <td>-0.576787</td>\n",
       "      <td>0.241837</td>\n",
       "      <td>-0.602468</td>\n",
       "      <td>-0.011970</td>\n",
       "      <td>0.117948</td>\n",
       "      <td>0.403452</td>\n",
       "      <td>-0.807792</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>-0.399105</td>\n",
       "      <td>-0.017445</td>\n",
       "      <td>-0.022554</td>\n",
       "      <td>1.153181</td>\n",
       "      <td>-0.589694</td>\n",
       "      <td>0.230436</td>\n",
       "      <td>-0.630677</td>\n",
       "      <td>-0.021752</td>\n",
       "      <td>0.101512</td>\n",
       "      <td>0.400198</td>\n",
       "      <td>-0.823632</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>-0.424252</td>\n",
       "      <td>-0.018442</td>\n",
       "      <td>-0.023357</td>\n",
       "      <td>1.153623</td>\n",
       "      <td>-0.586199</td>\n",
       "      <td>0.238734</td>\n",
       "      <td>-0.633681</td>\n",
       "      <td>-0.023838</td>\n",
       "      <td>0.096786</td>\n",
       "      <td>0.396037</td>\n",
       "      <td>-0.828314</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>-0.449008</td>\n",
       "      <td>-0.026898</td>\n",
       "      <td>-0.031561</td>\n",
       "      <td>1.141994</td>\n",
       "      <td>-0.595207</td>\n",
       "      <td>0.206956</td>\n",
       "      <td>-0.629704</td>\n",
       "      <td>-0.026934</td>\n",
       "      <td>0.091044</td>\n",
       "      <td>0.389866</td>\n",
       "      <td>-0.830505</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>-0.472690</td>\n",
       "      <td>-0.034597</td>\n",
       "      <td>-0.039131</td>\n",
       "      <td>1.135026</td>\n",
       "      <td>-0.611347</td>\n",
       "      <td>0.208442</td>\n",
       "      <td>-0.636151</td>\n",
       "      <td>-0.037647</td>\n",
       "      <td>0.083688</td>\n",
       "      <td>0.385573</td>\n",
       "      <td>-0.840636</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>-0.498060</td>\n",
       "      <td>-0.040115</td>\n",
       "      <td>-0.045105</td>\n",
       "      <td>1.123962</td>\n",
       "      <td>-0.609491</td>\n",
       "      <td>0.208894</td>\n",
       "      <td>-0.653766</td>\n",
       "      <td>-0.041960</td>\n",
       "      <td>0.076046</td>\n",
       "      <td>0.383001</td>\n",
       "      <td>-0.847525</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>-0.524058</td>\n",
       "      <td>-0.047947</td>\n",
       "      <td>-0.052504</td>\n",
       "      <td>1.117340</td>\n",
       "      <td>-0.626766</td>\n",
       "      <td>0.185167</td>\n",
       "      <td>-0.644236</td>\n",
       "      <td>-0.043081</td>\n",
       "      <td>0.067825</td>\n",
       "      <td>0.378476</td>\n",
       "      <td>-0.854754</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>-0.547401</td>\n",
       "      <td>-0.052502</td>\n",
       "      <td>-0.056647</td>\n",
       "      <td>1.108351</td>\n",
       "      <td>-0.624954</td>\n",
       "      <td>0.184358</td>\n",
       "      <td>-0.655043</td>\n",
       "      <td>-0.052338</td>\n",
       "      <td>0.064590</td>\n",
       "      <td>0.375902</td>\n",
       "      <td>-0.854046</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>-0.572909</td>\n",
       "      <td>-0.056937</td>\n",
       "      <td>-0.061278</td>\n",
       "      <td>1.112028</td>\n",
       "      <td>-0.627621</td>\n",
       "      <td>0.172722</td>\n",
       "      <td>-0.667733</td>\n",
       "      <td>-0.056899</td>\n",
       "      <td>0.059433</td>\n",
       "      <td>0.374504</td>\n",
       "      <td>-0.856662</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>-0.598846</td>\n",
       "      <td>-0.060054</td>\n",
       "      <td>-0.064070</td>\n",
       "      <td>1.103041</td>\n",
       "      <td>-0.636561</td>\n",
       "      <td>0.185246</td>\n",
       "      <td>-0.670787</td>\n",
       "      <td>-0.056402</td>\n",
       "      <td>0.051408</td>\n",
       "      <td>0.372474</td>\n",
       "      <td>-0.860980</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>-0.624220</td>\n",
       "      <td>-0.064915</td>\n",
       "      <td>-0.068902</td>\n",
       "      <td>1.099859</td>\n",
       "      <td>-0.633196</td>\n",
       "      <td>0.174824</td>\n",
       "      <td>-0.679197</td>\n",
       "      <td>-0.063423</td>\n",
       "      <td>0.047494</td>\n",
       "      <td>0.369258</td>\n",
       "      <td>-0.866838</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>-0.651180</td>\n",
       "      <td>-0.066566</td>\n",
       "      <td>-0.070989</td>\n",
       "      <td>1.095588</td>\n",
       "      <td>-0.643541</td>\n",
       "      <td>0.160237</td>\n",
       "      <td>-0.676899</td>\n",
       "      <td>-0.064352</td>\n",
       "      <td>0.056842</td>\n",
       "      <td>0.369813</td>\n",
       "      <td>-0.865595</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>-0.679363</td>\n",
       "      <td>-0.073611</td>\n",
       "      <td>-0.077988</td>\n",
       "      <td>1.091887</td>\n",
       "      <td>-0.647923</td>\n",
       "      <td>0.156974</td>\n",
       "      <td>-0.688141</td>\n",
       "      <td>-0.073318</td>\n",
       "      <td>0.043145</td>\n",
       "      <td>0.364808</td>\n",
       "      <td>-0.871341</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>-0.707368</td>\n",
       "      <td>-0.077064</td>\n",
       "      <td>-0.081555</td>\n",
       "      <td>1.079797</td>\n",
       "      <td>-0.648646</td>\n",
       "      <td>0.156766</td>\n",
       "      <td>-0.692867</td>\n",
       "      <td>-0.072328</td>\n",
       "      <td>0.037939</td>\n",
       "      <td>0.363981</td>\n",
       "      <td>-0.877084</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>-0.735144</td>\n",
       "      <td>-0.079397</td>\n",
       "      <td>-0.083306</td>\n",
       "      <td>1.084176</td>\n",
       "      <td>-0.653613</td>\n",
       "      <td>0.156420</td>\n",
       "      <td>-0.689830</td>\n",
       "      <td>-0.075966</td>\n",
       "      <td>0.034200</td>\n",
       "      <td>0.360344</td>\n",
       "      <td>-0.882177</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>-0.766681</td>\n",
       "      <td>-0.081164</td>\n",
       "      <td>-0.085714</td>\n",
       "      <td>1.080463</td>\n",
       "      <td>-0.651437</td>\n",
       "      <td>0.146317</td>\n",
       "      <td>-0.696995</td>\n",
       "      <td>-0.080151</td>\n",
       "      <td>0.038095</td>\n",
       "      <td>0.360509</td>\n",
       "      <td>-0.882510</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>-0.795276</td>\n",
       "      <td>-0.087139</td>\n",
       "      <td>-0.091187</td>\n",
       "      <td>1.074153</td>\n",
       "      <td>-0.658117</td>\n",
       "      <td>0.142984</td>\n",
       "      <td>-0.701779</td>\n",
       "      <td>-0.082317</td>\n",
       "      <td>0.029007</td>\n",
       "      <td>0.356860</td>\n",
       "      <td>-0.890285</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>-0.826730</td>\n",
       "      <td>-0.089608</td>\n",
       "      <td>-0.093828</td>\n",
       "      <td>1.073153</td>\n",
       "      <td>-0.658679</td>\n",
       "      <td>0.132581</td>\n",
       "      <td>-0.702932</td>\n",
       "      <td>-0.085525</td>\n",
       "      <td>0.028351</td>\n",
       "      <td>0.355089</td>\n",
       "      <td>-0.892665</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>-0.856556</td>\n",
       "      <td>-0.090485</td>\n",
       "      <td>-0.094841</td>\n",
       "      <td>1.070626</td>\n",
       "      <td>-0.664007</td>\n",
       "      <td>0.141093</td>\n",
       "      <td>-0.707673</td>\n",
       "      <td>-0.086767</td>\n",
       "      <td>0.026830</td>\n",
       "      <td>0.355109</td>\n",
       "      <td>-0.893937</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>-0.886517</td>\n",
       "      <td>-0.093215</td>\n",
       "      <td>-0.097437</td>\n",
       "      <td>1.068281</td>\n",
       "      <td>-0.664989</td>\n",
       "      <td>0.133440</td>\n",
       "      <td>-0.709771</td>\n",
       "      <td>-0.089438</td>\n",
       "      <td>0.023657</td>\n",
       "      <td>0.353718</td>\n",
       "      <td>-0.894395</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='8' class='' max='74', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      10.81% [8/74 00:04<00:41 -0.8943]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 0 with 👉🏻LMAE👈🏻 value: 1.870605230331421.\n",
      "Better model found at epoch 1 with 👉🏻LMAE👈🏻 value: 1.7759674787521362.\n",
      "Better model found at epoch 2 with 👉🏻LMAE👈🏻 value: 1.6843287944793701.\n",
      "Better model found at epoch 3 with 👉🏻LMAE👈🏻 value: 1.5979678630828857.\n",
      "Better model found at epoch 4 with 👉🏻LMAE👈🏻 value: 1.5280756950378418.\n",
      "Better model found at epoch 5 with 👉🏻LMAE👈🏻 value: 1.4803237915039062.\n",
      "Better model found at epoch 6 with 👉🏻LMAE👈🏻 value: 1.441452145576477.\n",
      "Better model found at epoch 7 with 👉🏻LMAE👈🏻 value: 1.3989479541778564.\n",
      "Better model found at epoch 8 with 👉🏻LMAE👈🏻 value: 1.3568905591964722.\n",
      "Better model found at epoch 9 with 👉🏻LMAE👈🏻 value: 1.3327049016952515.\n",
      "Better model found at epoch 10 with 👉🏻LMAE👈🏻 value: 1.2936468124389648.\n",
      "Better model found at epoch 11 with 👉🏻LMAE👈🏻 value: 1.2642858028411865.\n",
      "Better model found at epoch 12 with 👉🏻LMAE👈🏻 value: 1.2206670045852661.\n",
      "Better model found at epoch 13 with 👉🏻LMAE👈🏻 value: 1.2025784254074097.\n",
      "Better model found at epoch 14 with 👉🏻LMAE👈🏻 value: 1.1767089366912842.\n",
      "Better model found at epoch 15 with 👉🏻LMAE👈🏻 value: 1.124606728553772.\n",
      "Better model found at epoch 16 with 👉🏻LMAE👈🏻 value: 1.0715324878692627.\n",
      "Better model found at epoch 17 with 👉🏻LMAE👈🏻 value: 1.0407732725143433.\n",
      "Better model found at epoch 18 with 👉🏻LMAE👈🏻 value: 0.9777957201004028.\n",
      "Better model found at epoch 19 with 👉🏻LMAE👈🏻 value: 0.9401451349258423.\n",
      "Better model found at epoch 20 with 👉🏻LMAE👈🏻 value: 0.8576678037643433.\n",
      "Better model found at epoch 21 with 👉🏻LMAE👈🏻 value: 0.8081638813018799.\n",
      "Better model found at epoch 22 with 👉🏻LMAE👈🏻 value: 0.7620669603347778.\n",
      "Better model found at epoch 23 with 👉🏻LMAE👈🏻 value: 0.7358344793319702.\n",
      "Better model found at epoch 24 with 👉🏻LMAE👈🏻 value: 0.7083467245101929.\n",
      "Better model found at epoch 25 with 👉🏻LMAE👈🏻 value: 0.6809473037719727.\n",
      "Better model found at epoch 26 with 👉🏻LMAE👈🏻 value: 0.6746559143066406.\n",
      "Better model found at epoch 27 with 👉🏻LMAE👈🏻 value: 0.6460455656051636.\n",
      "Better model found at epoch 29 with 👉🏻LMAE👈🏻 value: 0.612525463104248.\n",
      "Better model found at epoch 30 with 👉🏻LMAE👈🏻 value: 0.5762168765068054.\n",
      "Better model found at epoch 31 with 👉🏻LMAE👈🏻 value: 0.5509563684463501.\n",
      "Better model found at epoch 32 with 👉🏻LMAE👈🏻 value: 0.5209730863571167.\n",
      "Better model found at epoch 33 with 👉🏻LMAE👈🏻 value: 0.4891204535961151.\n",
      "Better model found at epoch 34 with 👉🏻LMAE👈🏻 value: 0.4586595594882965.\n",
      "Better model found at epoch 35 with 👉🏻LMAE👈🏻 value: 0.4137638211250305.\n",
      "Better model found at epoch 36 with 👉🏻LMAE👈🏻 value: 0.3910459876060486.\n",
      "Better model found at epoch 37 with 👉🏻LMAE👈🏻 value: 0.3756576180458069.\n",
      "Better model found at epoch 38 with 👉🏻LMAE👈🏻 value: 0.3336383104324341.\n",
      "Better model found at epoch 39 with 👉🏻LMAE👈🏻 value: 0.3292369246482849.\n",
      "Better model found at epoch 40 with 👉🏻LMAE👈🏻 value: 0.30099523067474365.\n",
      "Better model found at epoch 41 with 👉🏻LMAE👈🏻 value: 0.2689867615699768.\n",
      "Better model found at epoch 42 with 👉🏻LMAE👈🏻 value: 0.24251842498779297.\n",
      "Better model found at epoch 43 with 👉🏻LMAE👈🏻 value: 0.22078680992126465.\n",
      "Better model found at epoch 44 with 👉🏻LMAE👈🏻 value: 0.21250268816947937.\n",
      "Better model found at epoch 45 with 👉🏻LMAE👈🏻 value: 0.176199272274971.\n",
      "Better model found at epoch 46 with 👉🏻LMAE👈🏻 value: 0.16166123747825623.\n",
      "Better model found at epoch 47 with 👉🏻LMAE👈🏻 value: 0.13526922464370728.\n",
      "Better model found at epoch 48 with 👉🏻LMAE👈🏻 value: 0.12451065331697464.\n",
      "Better model found at epoch 49 with 👉🏻LMAE👈🏻 value: 0.12021701037883759.\n",
      "Better model found at epoch 50 with 👉🏻LMAE👈🏻 value: 0.0999283641576767.\n",
      "Better model found at epoch 51 with 👉🏻LMAE👈🏻 value: 0.09354516863822937.\n",
      "Better model found at epoch 52 with 👉🏻LMAE👈🏻 value: 0.0773712694644928.\n",
      "Better model found at epoch 53 with 👉🏻LMAE👈🏻 value: 0.070878766477108.\n",
      "Better model found at epoch 54 with 👉🏻LMAE👈🏻 value: 0.0634271651506424.\n",
      "Better model found at epoch 55 with 👉🏻LMAE👈🏻 value: 0.046515509486198425.\n",
      "Better model found at epoch 56 with 👉🏻LMAE👈🏻 value: 0.04107558727264404.\n",
      "Better model found at epoch 57 with 👉🏻LMAE👈🏻 value: 0.02823261171579361.\n",
      "Better model found at epoch 58 with 👉🏻LMAE👈🏻 value: 0.018194809556007385.\n",
      "Better model found at epoch 59 with 👉🏻LMAE👈🏻 value: 0.013017058372497559.\n",
      "Better model found at epoch 60 with 👉🏻LMAE👈🏻 value: 0.005422726273536682.\n",
      "Better model found at epoch 61 with 👉🏻LMAE👈🏻 value: -0.005246452987194061.\n",
      "Better model found at epoch 62 with 👉🏻LMAE👈🏻 value: -0.006362341344356537.\n",
      "Better model found at epoch 63 with 👉🏻LMAE👈🏻 value: -0.02255350351333618.\n",
      "Better model found at epoch 64 with 👉🏻LMAE👈🏻 value: -0.02335657924413681.\n",
      "Better model found at epoch 65 with 👉🏻LMAE👈🏻 value: -0.03156128525733948.\n",
      "Better model found at epoch 66 with 👉🏻LMAE👈🏻 value: -0.03913147747516632.\n",
      "Better model found at epoch 67 with 👉🏻LMAE👈🏻 value: -0.045104801654815674.\n",
      "Better model found at epoch 68 with 👉🏻LMAE👈🏻 value: -0.052503518760204315.\n",
      "Better model found at epoch 69 with 👉🏻LMAE👈🏻 value: -0.05664748325943947.\n",
      "Better model found at epoch 70 with 👉🏻LMAE👈🏻 value: -0.061278488487005234.\n",
      "Better model found at epoch 71 with 👉🏻LMAE👈🏻 value: -0.06407009065151215.\n",
      "Better model found at epoch 72 with 👉🏻LMAE👈🏻 value: -0.06890249252319336.\n",
      "Better model found at epoch 73 with 👉🏻LMAE👈🏻 value: -0.07098853588104248.\n",
      "Better model found at epoch 74 with 👉🏻LMAE👈🏻 value: -0.07798844575881958.\n",
      "Better model found at epoch 75 with 👉🏻LMAE👈🏻 value: -0.08155523985624313.\n",
      "Better model found at epoch 76 with 👉🏻LMAE👈🏻 value: -0.08330576121807098.\n",
      "Better model found at epoch 77 with 👉🏻LMAE👈🏻 value: -0.08571376651525497.\n",
      "Better model found at epoch 78 with 👉🏻LMAE👈🏻 value: -0.09118684381246567.\n",
      "Better model found at epoch 79 with 👉🏻LMAE👈🏻 value: -0.09382820129394531.\n",
      "Better model found at epoch 80 with 👉🏻LMAE👈🏻 value: -0.0948406532406807.\n",
      "Better model found at epoch 81 with 👉🏻LMAE👈🏻 value: -0.09743714332580566.\n"
     ]
    }
   ],
   "source": [
    "learner.fit_one_cycle(100,2e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.recorder.plot_losses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learner.recorder.plot_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional Fine tune regular fit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learner.callbacks.append(\n",
    "    ReduceLROnPlateauCallback(learner, monitor='train_loss', mode='min', factor=0.2, patience=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learner.fit(100, 5e-5)# , moms=(0.75,0.70))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learner.fit_one_cycle(300, 5e-4)#, moms=(0.75,0.70))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learner.recorder.plot_lr(show_moms=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learner.recorder.plot_losses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learner.recorder.plot_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learner.fit(10,1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learner.to_fp32()\n",
    "val = learner.validate()[1]\n",
    "print(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    sub_fname = f'loss{learner.recorder.losses[-1]:.04f}val{val:.04f}'\n",
    "except:\n",
    "    sub_fname = f'val{val:.04f}'\n",
    "learner.save(sub_fname)\n",
    "print(sub_fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference\n",
    "\n",
    "Make sure `tranforms` are activated to test set otherwise TTA > 1 will be as TTA =1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    xt_coulombmat = load_fn(f'xt_coulombmat32{ext}.npy')\n",
    "except:\n",
    "    xt_coulombmat = np.load(f'xt_coulombmat{ext}.npy', allow_pickle=True)\n",
    "    xt_coulombmat = np.array(xt_coulombmat.tolist()).astype(np.float32)\n",
    "    np.save(f'xt_coulombmat32{ext}.npy', xt_coulombmat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_fname = Path('test.npz')\n",
    "try:\n",
    "    npzfile  = np.load(fname_ext(test_fname, ext))\n",
    "    xt_xyz   = npzfile['x_xyz']\n",
    "    xt_type  = npzfile['x_type']\n",
    "    xt_ext   = npzfile['x_ext']\n",
    "    xt_atom  = npzfile['x_atom']\n",
    "    mt = npzfile['m']\n",
    "    xt_ids = npzfile['x_ids']\n",
    "except:\n",
    "    xt_xyz,xt_type,xt_ext,xt_atom,mt,xt_ids = \\\n",
    "        preprocess(test_fname.with_suffix('.csv'), type_index=types,ext=ext)\n",
    "    np.savez(fname_ext('_'+test_fname, ext), \n",
    "             x_xyz  = xt_xyz,\n",
    "             x_type = xt_type,\n",
    "             x_ext  = xt_ext,\n",
    "             x_atom = xt_atom,\n",
    "             m=mt,\n",
    "             x_ids=xt_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xt_qm9_mulliken = load_fn(f'xt_qm9_mulliken{ext}.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "[v.shape for v in [xt_xyz,xt_type,xt_ext,xt_atom, xt_qm9_mulliken,xt_ids, mt]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learner.data.add_test(ItemList(items=(MoleculeItem(i,*v) for i,v in \n",
    "                              enumerate(zip(xt_xyz,xt_type,xt_ext,xt_atom,xt_qm9_mulliken,xt_coulombmat)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TTA_N = 1\n",
    "learner.data.test_ds.tfms = tta_tfms if TTA_N > 1 else tfms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#learner.model = learner.model.module\n",
    "#data.batch_size = 4096*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#learner.model = nn.DataParallel(learner.model)\n",
    "old_bs = data.batch_size\n",
    "data.batch_size *= 2\n",
    "\n",
    "sub = defaultdict(int)\n",
    "xt_ids_not_extended = (xt_ids!=0) & (xt_ids<=7163688) # TODO\n",
    "ids = xt_ids[xt_ids_not_extended]\n",
    "\n",
    "mb = master_bar(range(TTA_N))\n",
    "for tta in mb:\n",
    "    test_preds = np.zeros((0, 29), dtype=np.float32)\n",
    "\n",
    "    for batch_idx, batch in progress_bar(\n",
    "        enumerate(learner.dl(DatasetType.Test)), total=len(learner.dl(DatasetType.Test)), parent=mb):\n",
    "        _, _, preds_,_,_,_ = learner.pred_batch(ds_type=DatasetType.Test, batch=batch)\n",
    "        preds_ = preds_.sum(dim=1)\n",
    "        test_preds = np.concatenate([test_preds, preds_.data.cpu().numpy()], axis = 0)\n",
    "\n",
    "    preds = test_preds[xt_ids_not_extended]\n",
    "    for k in range(len(ids)):\n",
    "        sub[int(ids[k])] += preds[k]\n",
    "    \n",
    "for k in range(len(ids)):\n",
    "    sub[int(ids[k])] = sub[int(ids[k])]/TTA_N\n",
    "\n",
    "learner.model = learner.model.module\n",
    "data.batch_size = old_bs\n",
    "\n",
    "sub_df = pd.DataFrame(sub.items(), columns=['id', 'scalar_coupling_constant'])\n",
    "sub_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submit to Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "csv_fname = f'{sub_fname}_tta{TTA_N}.csv'\n",
    "sub_df.to_csv(csv_fname, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "comp = 'champs-scalar-coupling'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!kaggle competitions submit -c {comp} -f {csv_fname} -m 'QM9 tta {TTA_N} {ext}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "time.sleep(60)\n",
    "!kaggle competitions submissions -c {comp} -v > submissions-{comp}.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submissions = pd.read_csv(f'submissions-{comp}.csv')\n",
    "submissions.iloc[0].publicScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (root)",
   "language": "python",
   "name": "root"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
