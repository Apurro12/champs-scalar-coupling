{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%load_ext autoreload\n",
    "#%autoreload 2\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.utils.data\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from torch.utils.data.sampler import RandomSampler\n",
    "#RandomSampler_num_samples = RandomSampler.num_samples\n",
    "#@property\n",
    "#def RandomSampler_num_samples_x10(self): return  self.RandomSampler_num_samples*10\n",
    "#RandomSampler.num_samples= RandomSampler_num_samples_x10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.nn import Parameter\n",
    "\n",
    "def _load_from_state_dict(self, state_dict, prefix, local_metadata, strict,\n",
    "                          missing_keys, unexpected_keys, error_msgs):\n",
    "    r\"\"\"Copies parameters and buffers from :attr:`state_dict` into only\n",
    "    this module, but not its descendants. This is called on every submodule\n",
    "    in :meth:`~torch.nn.Module.load_state_dict`. Metadata saved for this\n",
    "    module in input :attr:`state_dict` is provided as :attr:`local_metadata`.\n",
    "    For state dicts without metadata, :attr:`local_metadata` is empty.\n",
    "    Subclasses can achieve class-specific backward compatible loading using\n",
    "    the version number at `local_metadata.get(\"version\", None)`.\n",
    "\n",
    "    .. note::\n",
    "        :attr:`state_dict` is not the same object as the input\n",
    "        :attr:`state_dict` to :meth:`~torch.nn.Module.load_state_dict`. So\n",
    "        it can be modified.\n",
    "\n",
    "    Arguments:\n",
    "        state_dict (dict): a dict containing parameters and\n",
    "            persistent buffers.\n",
    "        prefix (str): the prefix for parameters and buffers used in this\n",
    "            module\n",
    "        local_metadata (dict): a dict containing the metadata for this module.\n",
    "            See\n",
    "        strict (bool): whether to strictly enforce that the keys in\n",
    "            :attr:`state_dict` with :attr:`prefix` match the names of\n",
    "            parameters and buffers in this module\n",
    "        missing_keys (list of str): if ``strict=True``, add missing keys to\n",
    "            this list\n",
    "        unexpected_keys (list of str): if ``strict=True``, add unexpected\n",
    "            keys to this list\n",
    "        error_msgs (list of str): error messages should be added to this\n",
    "            list, and will be reported together in\n",
    "            :meth:`~torch.nn.Module.load_state_dict`\n",
    "    \"\"\"\n",
    "    for hook in self._load_state_dict_pre_hooks.values():\n",
    "        hook(state_dict, prefix, local_metadata, strict, missing_keys,\n",
    "             unexpected_keys, error_msgs)\n",
    "\n",
    "    local_name_params = itertools.chain(self._parameters.items(),\n",
    "                                        self._buffers.items())\n",
    "    local_state = {k: v.data for k, v in local_name_params if v is not None}\n",
    "\n",
    "    for name, param in local_state.items():\n",
    "        key = prefix + name\n",
    "        if key in state_dict:\n",
    "            input_param = state_dict[key]\n",
    "\n",
    "            # Backward compatibility: loading 1-dim tensor from 0.3.* to version 0.4+\n",
    "            if len(param.shape) == 0 and len(input_param.shape) == 1:\n",
    "                input_param = input_param[0]\n",
    "\n",
    "            if input_param.shape != param.shape:\n",
    "                # local shape should match the one in checkpoint\n",
    "                error_msgs.append(\n",
    "                    'size mismatch for {}: copying a param with shape {} from checkpoint, '\n",
    "                    'the shape in current model is {}.'.format(\n",
    "                        key, input_param.shape, param.shape))\n",
    "                #if not strict:\n",
    "                #    continue\n",
    "\n",
    "            if isinstance(input_param, Parameter):\n",
    "                # backwards compatibility for serialized parameters\n",
    "                input_param = input_param.data\n",
    "\n",
    "            try:\n",
    "                param.copy_(input_param)\n",
    "            except Exception:\n",
    "                error_msgs.append(\n",
    "                    'While copying the parameter named \"{}\", '\n",
    "                    'whose dimensions in the model are {} and '\n",
    "                    'whose dimensions in the checkpoint are {}.'.format(\n",
    "                        key, param.size(), input_param.size()))\n",
    "                # PG load partially\n",
    "\n",
    "                if len(input_param.size()) == 3:\n",
    "                    error_msgs.append(\n",
    "                        'Partially copying the parameter named \"{}\", '\n",
    "                        'whose dimensions in the model are {} and '\n",
    "                        'whose dimensions in the checkpoint are {}. - trying {}'\n",
    "                        .format(\n",
    "                            key, param.size(), input_param.size(),\n",
    "                            param[:input_param.size()[0], :input_param.size(\n",
    "                            )[1], :input_param.size()[2]].shape))\n",
    "                else:\n",
    "                    error_msgs.append(\n",
    "                        'Partially copying the parameter named \"{}\", '\n",
    "                        'whose dimensions in the model are {} and '\n",
    "                        'whose dimensions in the checkpoint are {}. - trying {}'\n",
    "                        .format(key, param.size(), input_param.size(),\n",
    "                                param[:input_param.size()[0]].shape))\n",
    "\n",
    "                try:\n",
    "                    new_input_param = torch.empty_like(param)\n",
    "                    new_input_param = torch.nn.init.normal_(new_input_param,\n",
    "                                                            mean=input_param.mean(),\n",
    "                                                            std=input_param.std())\n",
    "\n",
    "                    if len(input_param.size()) == 3:\n",
    "                        new_input_param[:input_param.size()[0], :input_param.\n",
    "                                        size()[1], :input_param.size(\n",
    "                                        )[2]] = input_param\n",
    "                    else:\n",
    "                        new_input_param[:input_param.size()[0]] = input_param\n",
    "                    param.copy_(new_input_param)\n",
    "                except Exception as e:\n",
    "                    assert e\n",
    "                    error_msgs.append(\n",
    "                        'Failed to load weights partially {}'.format(e))\n",
    "        elif strict:\n",
    "            missing_keys.append(key)\n",
    "\n",
    "    if strict:\n",
    "        for key in state_dict.keys():\n",
    "            if key.startswith(prefix):\n",
    "                input_name = key[len(prefix):]\n",
    "                input_name = input_name.split(\n",
    "                    '.', 1)[0]  # get the name of param/buffer/child\n",
    "                if input_name not in self._modules and input_name not in local_state:\n",
    "                    unexpected_keys.append(key)\n",
    "\n",
    "def load_state_dict(self, state_dict, strict=True):\n",
    "    r\"\"\"Copies parameters and buffers from :attr:`state_dict` into\n",
    "    this module and its descendants. If :attr:`strict` is ``True``, then\n",
    "    the keys of :attr:`state_dict` must exactly match the keys returned\n",
    "    by this module's :meth:`~torch.nn.Module.state_dict` function.\n",
    "\n",
    "    Arguments:\n",
    "        state_dict (dict): a dict containing parameters and\n",
    "            persistent buffers.\n",
    "        strict (bool, optional): whether to strictly enforce that the keys\n",
    "            in :attr:`state_dict` match the keys returned by this module's\n",
    "            :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\n",
    "\n",
    "    Returns:\n",
    "        ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n",
    "            * **missing_keys** is a list of str containing the missing keys\n",
    "            * **unexpected_keys** is a list of str containing the unexpected keys\n",
    "    \"\"\"\n",
    "    missing_keys = []\n",
    "    unexpected_keys = []\n",
    "    error_msgs = []\n",
    "\n",
    "    # copy state_dict so _load_from_state_dict can modify it\n",
    "    metadata = getattr(state_dict, '_metadata', None)\n",
    "    state_dict = state_dict.copy()\n",
    "    if metadata is not None:\n",
    "        state_dict._metadata = metadata\n",
    "\n",
    "    def load(module, prefix=''):\n",
    "        local_metadata = {} if metadata is None else metadata.get(\n",
    "            prefix[:-1], {})\n",
    "        module._load_from_state_dict(state_dict, prefix, local_metadata, True,\n",
    "                                     missing_keys, unexpected_keys, error_msgs)\n",
    "        for name, child in module._modules.items():\n",
    "            if child is not None:\n",
    "                load(child, prefix + name + '.')\n",
    "                \n",
    "    load(self)\n",
    "\n",
    "    if strict:\n",
    "        if len(unexpected_keys) > 0:\n",
    "            error_msgs.insert(\n",
    "                0, 'Unexpected key(s) in state_dict: {}. '.format(', '.join(\n",
    "                    '\"{}\"'.format(k) for k in unexpected_keys)))\n",
    "        if len(missing_keys) > 0:\n",
    "            error_msgs.insert(\n",
    "                0, 'Missing key(s) in state_dict: {}. '.format(', '.join(\n",
    "                    '\"{}\"'.format(k) for k in missing_keys)))\n",
    "\n",
    "    if strict and len(error_msgs) > 0:\n",
    "        raise RuntimeError(\n",
    "            'Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n",
    "                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.Module._load_from_state_dict = _load_from_state_dict\n",
    "nn.Module.load_state_dict = load_state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from fastai import *\n",
    "from fastai.basic_train import *\n",
    "from fastai.basic_data import *\n",
    "from fastai.data_block import *\n",
    "from fastai.torch_core import *\n",
    "from fastai.train import *\n",
    "from fastai.callback import *\n",
    "from fastai.callbacks import *\n",
    "from fastai.distributed import *\n",
    "from fastai.layers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ext = '_ext' # or ''\n",
    "multi = '_molecule'\n",
    "train_fname = Path('train.npz')\n",
    "type_index=types = ['1JHC', '2JHH', '1JHN', '2JHN', '2JHC', '3JHH', '3JHC', '3JHN'] \n",
    "atoms = 'CFHNO'\n",
    "xyz = ['x', 'y', 'z']\n",
    "max_atoms = 29 #int(s.atom_index.max() + 1)\n",
    "\n",
    "fname_ext = lambda fname,ext,multi: f'{str(fname)[:-4]}{ext}{multi}{str(fname)[-4:]}'\n",
    "s  = pd.read_csv('structures.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_group(process_args,i):\n",
    "    m_group,has_y,x0_xyz,x0_type,x0_ext,x0_atom,x0_id,m0,y0_scalar = process_args\n",
    "    x0_atom[:] = -1\n",
    "    x0_type[:] = -1\n",
    "    x0_id[:] = -1\n",
    "    y0_scalar[:] = np.nan\n",
    "    (m_name, m_index) ,m_group = m_group\n",
    "    ss = s[s.molecule_name==m_name]\n",
    "    n_atoms = len(ss)\n",
    "    ii = 0\n",
    "    x0_xyz[ii] = 0.\n",
    "    x0_type[ii] = -1\n",
    "    x0_ext[ii] =  True\n",
    "    x0_xyz[ii,:,:n_atoms] = ss[xyz].values.T  # xyz \n",
    "    x0_type[ii,0,m_group['atom_index_1'], m_group['atom_index_0']] = m_group['type_idx'].T\n",
    "    x0_ext[ii,0, m_group['atom_index_0']] = m_group['ext'].T\n",
    "    x0_atom[ii,:,:n_atoms] = ss['atom_idx'].T\n",
    "    x0_id[ii,0,m_group['atom_index_1'], m_group['atom_index_0']] = m_group['id'].T\n",
    "    m0[ii] = m_index\n",
    "    res = [x0_xyz,x0_type,x0_ext,x0_atom,x0_id,m0]\n",
    "    if has_y:\n",
    "        y0_scalar[0,0,m_group['atom_index_1'], m_group['atom_index_0']] =  m_group['scalar_coupling_constant']\n",
    "        res.append(y0_scalar)\n",
    "    return i,res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(fname, type_index=None, ext=''):\n",
    "    t  = pd.read_csv(fname_ext(fname,ext,''))\n",
    "    has_y = 'scalar_coupling_constant' in t.columns\n",
    "    t['molecule_index'] = pd.factorize(t['molecule_name'])[0] + t['id'].min()\n",
    "    # make sure we use the same indexes in train/test (test needs to provide type_index)\n",
    "    if type_index is not None:\n",
    "        t['type_idx'] = t['type'].apply(lambda x: type_index.index(x) ) # pd.factorize(pd.concat([pd.Series(type_index),t['type']]))[0][len(type_index):]\n",
    "    else:\n",
    "        t['type_idx'] = pd.factorize(t['type'])[0]\n",
    "\n",
    "    s['atom_idx'] = s['atom'].apply(lambda x: atoms.index(x) )\n",
    "\n",
    "    max_items = t['molecule_name'].nunique()\n",
    "    x_xyz   = np.zeros((max_items,len(xyz),  max_atoms), dtype=np.float32)\n",
    "    x_type  = np.empty((max_items,1,         max_atoms,max_atoms), dtype=np.int8)\n",
    "    x_ext   = np.zeros((max_items,1,         max_atoms), dtype=np.bool_)\n",
    "    x_atom  = np.empty((max_items,1,         max_atoms), dtype=np.int8)\n",
    "    x_ids   = np.zeros((max_items,           max_atoms,max_atoms), dtype=np.int32)\n",
    "    x_atom[:] = -1\n",
    "    x_type[:] = -1\n",
    "    x_ids[:] = -1\n",
    "    m = np.zeros((max_items,), dtype=np.int32)\n",
    "    if has_y:\n",
    "        y_scalar   = np.empty((max_items,1 ,max_atoms,max_atoms), dtype=np.float32)\n",
    "        y_scalar[:] = np.nan\n",
    "        \n",
    "    x0_xyz   = np.zeros((1,len(xyz),  max_atoms), dtype=np.float32)        \n",
    "    x0_type  = np.empty((1,1,         max_atoms,max_atoms), dtype=np.int8)\n",
    "    x0_ext   = np.zeros((1,1,         max_atoms), dtype=np.bool_)\n",
    "    x0_atom  = np.empty((1,1,         max_atoms), dtype=np.int8)\n",
    "    x0_id    = np.zeros((1,1,         max_atoms,max_atoms), dtype=np.int32)\n",
    "    y0_scalar   = np.empty((1,1 ,max_atoms,max_atoms), dtype=np.float32)\n",
    "    m0 = np.zeros((1,), dtype=np.int32)\n",
    "    \n",
    "    process_args = [(m_group,has_y,x0_xyz,x0_type,x0_ext,x0_atom,x0_id,m0,y0_scalar) \n",
    "                    for m_group in t.groupby(['molecule_name', 'molecule_index'])]\n",
    "    res = parallel(process_group,process_args,max_workers=multiprocessing.cpu_count())\n",
    "    for i,r in progress_bar(res):\n",
    "        x_xyz[i],x_type[i],x_ext[i],x_atom[i],x_ids[i],m[i] = r[:6]\n",
    "        if has_y: y_scalar[i]=r[6]\n",
    "        \n",
    "    res = [x_xyz,x_type,x_ext,x_atom,x_ids,m]\n",
    "    if has_y: res.append(y_scalar)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define where you want to use original training set '' or extended ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load preprocessed or preprocess and save for later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fname = Path('train.npz')\n",
    "types = ['1JHC', '2JHH', '1JHN', '2JHN', '2JHC', '3JHH', '3JHC', '3JHN'] \n",
    "atoms = 'CFHNO'\n",
    "\n",
    "try:\n",
    "    npzfile = np.load(fname_ext(train_fname,ext,multi))\n",
    "    x_xyz   = npzfile['x_xyz']\n",
    "    x_type  = npzfile['x_type']\n",
    "    x_ext   = npzfile['x_ext']\n",
    "    x_atom  = npzfile['x_atom']\n",
    "\n",
    "    y_scalar    = npzfile['y_scalar']\n",
    "    m = npzfile['m']\n",
    "    max_items, max_atoms = x_xyz.shape[0], x_xyz.shape[-1]\n",
    "except:\n",
    "    x_xyz,x_type,x_ext,x_atom,x_ids,m,y_scalar=preprocess(train_fname.with_suffix('.csv'),type_index=types,ext=ext)\n",
    "    np.savez(fname_ext(train_fname,ext,multi), \n",
    "             x_xyz=x_xyz,\n",
    "             x_type=x_type,\n",
    "             x_ext=x_ext,\n",
    "             x_atom=x_atom,\n",
    "             x_ids=x_ids,\n",
    "             y_scalar=y_scalar,\n",
    "             m=m)\n",
    "n_types = int(x_type[~np.isnan(x_type)].max() + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(85003, 3, 29),\n",
       " (85003, 1, 29, 29),\n",
       " (85003, 1, 29),\n",
       " (85003, 1, 29),\n",
       " (85003, 1, 29, 29),\n",
       " (85003,)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[v.shape for v in [x_xyz,x_type,x_ext,x_atom,\n",
    "                   y_scalar, m]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_xyz_mean, x_xyz_std = Tensor(x_xyz.mean(axis=(0,2),keepdims=True)), Tensor(x_xyz.std(axis=(0,2),keepdims=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 1])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_xyz_std.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fastai classes (this should should be done into its own `application` but who has time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MoleculeItem(ItemBase):\n",
    "    def __init__(self,i,xyz,type,ext,atom):#,qm9_mulliken,coulomb): \n",
    "        self.i,self.xyz,self.type,self.ext, self.atom = \\\n",
    "            i,xyz,type,ext,atom\n",
    "        self.data = [Tensor(xyz), LongTensor((type)), \n",
    "                     Tensor(ext), LongTensor((atom))]\n",
    "    def __str__(self):\n",
    "        # TODO: count n_atoms correctly. \n",
    "        n_atoms = np.count_nonzero(np.sum(np.absolute(self.xyz), axis=0))+1\n",
    "        n_couplings = np.sum((self.type!=-1))\n",
    "        return f'{self.i} {n_atoms} atoms {n_couplings} couplings'\n",
    "    \n",
    "    def apply_tfms(self, tfms:Collection, **kwargs):\n",
    "        x = self.clone()\n",
    "        for t in tfms:\n",
    "            if t: x.data = t(x.data)\n",
    "        return x\n",
    "    \n",
    "    def clone(self):\n",
    "        return self.__class__(self.i,self.xyz,self.type,self.ext,self.atom)#,self.qm9_mulliken,self.coulomb)\n",
    "    \n",
    "class ScalarCouplingItem(ItemBase):\n",
    "    def __init__(self,scalar,**kwargs): \n",
    "        self.scalar = scalar#,magnetic,mulliken,dipole,potential\n",
    "        self.data = Tensor(scalar) #, Tensor(magnetic), Tensor(dipole), Tensor(potential))\n",
    "    def __str__(self):\n",
    "        #res, spacer, n_couplings = '', '', 0\n",
    "        n_couplings = (~torch.isnan(self.data)).sum()\n",
    "        return f'{n_couplings}'\n",
    "    def apply_tfms(self, tfms:Collection, **kwargs):\n",
    "        y = self.clone()\n",
    "        for t in tfms:\n",
    "            if 'label_smoothing' == t.__name__:\n",
    "                if t: y.data = t(y.data)\n",
    "        return y\n",
    "    def clone(self): return self.__class__(self.scalar)#,self.magnetic,self.mulliken,self.dipole,self.potential)\n",
    "    def __hash__(self): return hash(str(self))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LMAEMaskedLoss(Module):\n",
    "    def __init__(self,types_w = [1]*n_types, return_all=False, proxy_log=torch.log, exclude_ext=False):\n",
    "        self.types_w = types_w\n",
    "        self.return_all = return_all\n",
    "        self.proxy_log = proxy_log\n",
    "        self.exclude_ext = exclude_ext\n",
    "        self.loss = torch.nn.L1Loss()\n",
    "    \n",
    "    def forward(self, input_outputs, t_scalar):#, t_magnetic, t_dipole, t_potential):    \n",
    "        t_mask, type, ext, p_scalar = input_outputs\n",
    "        loss = 0.\n",
    "        n = 0\n",
    "        j_loss = [0] * n_types\n",
    "        p_scalar = p_scalar.squeeze(-1)\n",
    "        #print(t_scalar.shape, type.shape)\n",
    "        t_scalar = t_scalar.view(t_scalar.shape[0],-1)[t_mask] #t_scalar[~torch.isnan(t_scalar)].unsqueeze(-1)\n",
    "        #t_scalar = t_scalar.view(-1,1)\n",
    "        #print(t_scalar.shape)\n",
    "        for t in range(n_types):\n",
    "            #if self.exclude_ext:\n",
    "            #    print(type.shape)\n",
    "            mask = (type == t) #if not self.exclude_ext else ((type == t) & (ext == 0)).squeeze(1)\n",
    "            if mask.sum() > 0:\n",
    "               # print(mask.shape)\n",
    "                _output,_target = p_scalar[mask], t_scalar[mask]\n",
    "               # print(mask.shape,p_scalar.shape,t_scalar.shape)\n",
    "\n",
    "                # LMAE scalar\n",
    "#                s_loss = self.proxy_log((_output - _target).abs().mean()+1e-9)\n",
    "                s_loss = self.proxy_log(torch.nn.L1Loss()(_output,_target)+1e-9)\n",
    "                loss += self.types_w[t] * s_loss\n",
    "                j_loss[t] += s_loss\n",
    "                n+=1\n",
    "        loss /= n\n",
    "        \n",
    "        return loss if not self.return_all else (loss, *j_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ScalarCouplingList(ItemList):\n",
    "    def __init__(self, items:Iterator, **kwargs):\n",
    "        super().__init__(items, **kwargs)\n",
    "        self.loss_func = LMAEMaskedLoss\n",
    "\n",
    "    def get(self, i):\n",
    "        o = super().get(i)\n",
    "        #print(len(o),o[0].shape,o[0].dtype)\n",
    "        return ScalarCouplingItem(np.array(o, dtype=np.float32))\n",
    "\n",
    "    def reconstruct(self,t): return 0; # TODO for viz !!!! ScalarCouplingItem(t.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quaterions allow us to rotate 3d points randoming with a nice uniform distribution of 3 numbers hece we use them, however it's still to be seen if are useful here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#https://github.com/facebookresearch/QuaterNet/blob/master/common/quaternion.py\n",
    "def qrot(q, v):\n",
    "    \"\"\"\n",
    "    Rotate vector(s) v about the rotation described by quaternion(s) q.\n",
    "    Expects a tensor of shape (*, 4) for q and a tensor of shape (*, 3) for v,\n",
    "    where * denotes any number of dimensions.\n",
    "    Returns a tensor of shape (*, 3).\n",
    "    \"\"\"\n",
    "    assert q.shape[-1] == 4\n",
    "    assert v.shape[-1] == 3\n",
    "    assert q.shape[:-1] == v.shape[:-1]\n",
    "    \n",
    "    original_shape = list(v.shape)\n",
    "    q = q.view(-1, 4)\n",
    "    v = v.view(-1, 3)\n",
    "    \n",
    "    qvec = q[:, 1:]\n",
    "    uv = torch.cross(qvec, v, dim=1)\n",
    "    uuv = torch.cross(qvec, uv, dim=1)\n",
    "    return (v + 2 * (q[:, :1] * uv + uuv)).view(original_shape)\n",
    "\n",
    "def random_rotation(data):\n",
    "    x_xyz = data[0].transpose(0,1)\n",
    "    r = torch.rand(3)\n",
    "    sq1_v1,sqv1,v2_2pi,v3_2pi = torch.sqrt(1-r[:1]),torch.sqrt(r[:1]),2*math.pi*r[1:2],2*math.pi*r[2:3]\n",
    "    q = torch.cat([sq1_v1*torch.sin(v2_2pi), sq1_v1*torch.cos(v2_2pi), \n",
    "                   sqv1  *torch.sin(v3_2pi), sqv1  *torch.cos(v3_2pi)], dim=0).unsqueeze(0)\n",
    "    x_xyz = qrot(q.expand(x_xyz.shape[0],-1), x_xyz).squeeze(0).transpose(0,1)\n",
    "    return (x_xyz, *data[1:])\n",
    "\n",
    "def normalize(data):\n",
    "    sq = False\n",
    "    if data[0].ndim < 3:\n",
    "        data[0].unsqueeze_(0)\n",
    "        #data[4].unsqueeze_(0)\n",
    "        sq = True\n",
    "    x_xyz      = (data[0] - x_xyz_mean)          / x_xyz_std\n",
    "    #x_mulliken = (data[4] - x_qm9_mulliken_mean) / x_qm9_mulliken_std\n",
    "    if sq:\n",
    "        x_xyz.squeeze_(0)\n",
    "        #x_mulliken.squeeze_(0)\n",
    "    return (x_xyz, *data[1:])\n",
    "\n",
    "def canonize(data):\n",
    "    xyz,type,ext,atom = data\n",
    "    mask = (atom == -1).squeeze(0)\n",
    "    i_max_atom = torch.nonzero(type != -1).max()\n",
    "    mask_atoms = torch.ones((  max_atoms,  max_atoms), dtype=torch.uint8)\n",
    "    zeros      = torch.zeros((i_max_atom, i_max_atom), dtype=torch.uint8)\n",
    "    mask_atoms[:zeros.shape[0],:zeros.shape[1]] = zeros\n",
    "    n_atoms = mask.sum()\n",
    "    xyz[:,mask], type[:,mask],ext[:,mask],atom[:,mask] = 0,-1,1,-1\n",
    "    return (xyz,type,ext,atom,mask_atoms.unsqueeze(0),n_atoms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build `data` bunch etc. for fastai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = ItemList(items=(MoleculeItem(i,*v) for i,v in \n",
    "                       enumerate(zip(x_xyz,x_type,x_ext,x_atom))),\n",
    "                label_cls=ScalarCouplingItem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "_, idx_valid_split = train_test_split(range(m.max()+1), test_size=0.1, random_state=13)\n",
    "idx_valid_split = np.argwhere(np.isin(m, idx_valid_split)).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [],
   "source": [
    "data = data.split_by_idx(idx_valid_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def l(o): return np.array(y_scalar[o.i], dtype=np.float32)\n",
    "data = data.label_from_func(func=l,label_cls=ScalarCouplingList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfms = [normalize, canonize]\n",
    "tta_tfms = list(tfms)\n",
    "tta_tfms.insert(0,random_rotation)\n",
    "data = data.transform((tta_tfms, tfms))#.transform_y(([label_smoothing], None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataBunch;\n",
       "\n",
       "Train: LabelList (76502 items)\n",
       "x: ItemList\n",
       "0 6 atoms 20 couplings,1 5 atoms 12 couplings,2 4 atoms 2 couplings,3 4 atoms 4 couplings,5 7 atoms 20 couplings\n",
       "y: ScalarCouplingList\n",
       "20,12,2,4,20\n",
       "Path: .;\n",
       "\n",
       "Valid: LabelList (8501 items)\n",
       "x: ItemList\n",
       "4 9 atoms 54 couplings,9 7 atoms 18 couplings,37 9 atoms 26 couplings,43 14 atoms 88 couplings,47 14 atoms 90 couplings\n",
       "y: ScalarCouplingList\n",
       "54,18,26,88,90\n",
       "Path: .;\n",
       "\n",
       "Test: None"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=data.databunch()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data.train_ds.filter_by_func(lambda item, _: len(item.data[1].cpu().numpy()[item.data[1].cpu().numpy()==0]) == 0)\n",
    "#data.valid_ds.filter_by_func(lambda item, _: len(item.data[1].cpu().numpy()[item.data[1].cpu().numpy()==0]) == 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "Whole model here, self-contained (needs some cleanup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LMAEMetric(LearnerCallback):\n",
    "    _order=-20 # Needs to run before the recorder\n",
    "    def __init__(self, learn, val_only=True):\n",
    "        super().__init__(learn)\n",
    "        self.val_only=val_only\n",
    "        self.metric = LMAEMaskedLoss(return_all=True, exclude_ext=True)\n",
    "\n",
    "    def on_train_begin(self, **kwargs):\n",
    "        if not self.val_only: self.learn.recorder.add_metric_names(['tLMAE'])\n",
    "        self.learn.recorder.add_metric_names(['👉🏻LMAE👈🏻'] + [f'lmae{i}' for i in range(n_types)])\n",
    "            \n",
    "    def on_batch_end(self, train, last_output, last_target, **kwargs):\n",
    "        if self.val_only and train: return \n",
    "        preds,targs = self.preds[int(train)], self.targs[int(train)] # 0 val 1 train\n",
    "        \n",
    "        if preds is None:\n",
    "            targs, preds = last_target.detach(),[t.detach() for t in listify(last_output)]\n",
    "        else:\n",
    "            targs = torch.cat([targs,last_target.detach()], dim=0)\n",
    "            for i,(o,t) in enumerate(zip(last_output, last_target)):\n",
    "                preds[i] = torch.cat([preds[i], o.detach()], dim=0)\n",
    "        self.preds[int(train)], self.targs[int(train)] = preds,targs\n",
    "        \n",
    "    def on_epoch_begin(self, **kwargs):\n",
    "        self.targs, self.preds = [None, None], [None, None]\n",
    "\n",
    "    def on_epoch_end(self, last_metrics, **kwargs):\n",
    "        if True: #(kwargs['epoch'] % max_atoms) == 0:\n",
    "            mets = []\n",
    "            if self.preds[1]: mets.append(self.metric.forward(self.preds[1], self.targs[1])[0]) # just tLMAE\n",
    "            if self.preds[0]: \n",
    "                mets.extend(self.metric.forward(self.preds[0], self.targs[0]))\n",
    "            return add_metrics(last_metrics, mets) if mets else None\n",
    "        else: return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Activation = Enum('Activation', 'ReLU Swish GeLU')\n",
    "\n",
    "class PositionalEncoding(Module):\n",
    "    \"Encode the position with a sinusoid.\"\n",
    "    def __init__(self, d:int): self.register_buffer('freq', 1 / (10000 ** (torch.arange(0., d, 2.)/d)))\n",
    "\n",
    "    def forward(self, pos:Tensor):\n",
    "        inp = torch.ger(pos, self.freq)\n",
    "        enc = torch.cat([inp.sin(), inp.cos()], dim=-1)\n",
    "        return enc\n",
    "    \n",
    "class GeLU(Module):\n",
    "    def forward(self, x): return 0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n",
    "\n",
    "class Swish(Module):\n",
    "    def forward(self, x): return x * torch.sigmoid(x)\n",
    "\n",
    "_activ_func = {Activation.ReLU:nn.ReLU(inplace=True), Activation.GeLU:GeLU(), Activation.Swish: Swish()}\n",
    "\n",
    "def feed_forward(d_model:int,d_model_out:int, d_ff:int, ff_p:float=0.,\n",
    "                 act:Activation=Activation.ReLU, double_drop:bool=True):\n",
    "    if d_model==d_model_out:\n",
    "        layers = [nn.Linear(d_model, d_ff), _activ_func[act]]\n",
    "        if double_drop: layers.append(nn.Dropout(ff_p))\n",
    "        return SequentialEx(*layers,nn.Linear(d_ff,d_model_out),nn.Dropout(ff_p),MergeLayer(),nn.LayerNorm(d_model_out))\n",
    "    else:\n",
    "        return SequentialEx(nn.Linear(d_model,d_model_out))#,nn.Dropout(ff_p)),nn.LayerNorm(d_model_out))\n",
    "\n",
    "class MultiHeadAttention(Module):\n",
    "    \"MutiHeadAttention.\"\n",
    "    def __init__(self, n_heads:int, d_model:int, d_head:int=None, resid_p:float=0., attn_p:float=0., bias:bool=True,\n",
    "                 scale:bool=True):\n",
    "        d_head = ifnone(d_head, d_model//n_heads)\n",
    "        self.n_heads,self.d_head,self.scale = n_heads,d_head,scale\n",
    "        self.attention = nn.Linear(d_model, 3 * n_heads * d_head, bias=bias)\n",
    "        self.out = nn.Linear(n_heads * d_head, d_model, bias=bias)\n",
    "        self.drop_att,self.drop_res = nn.Dropout(attn_p),nn.Dropout(resid_p)\n",
    "        self.ln = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x:Tensor, mask:Tensor=None, **kwargs):\n",
    "        return self.ln(x + self.drop_res(self.out(self._apply_attention(x, mask=mask, **kwargs))))\n",
    "\n",
    "    def _apply_attention(self, x:Tensor, mask:Tensor=None):\n",
    "        bs,x_len = x.size(0),x.size(1)\n",
    "        wq,wk,wv = torch.chunk(self.attention(x), 3, dim=-1)\n",
    "        wq,wk,wv = map(lambda x:x.view(bs, x.size(1), self.n_heads, self.d_head), (wq,wk,wv))\n",
    "        wq,wk,wv = wq.permute(0, 2, 1, 3),wk.permute(0, 2, 3, 1),wv.permute(0, 2, 1, 3)\n",
    "        attn_score = torch.matmul(wq, wk)\n",
    "        if self.scale: attn_score.div_(self.d_head ** 0.5)\n",
    "        if mask is not None:\n",
    "            minus_inf = -65504 if attn_score.dtype == torch.float16 else -1e9\n",
    "            attn_score = attn_score.masked_fill(mask, minus_inf).type_as(attn_score)\n",
    "        attn_prob = self.drop_att(F.softmax(attn_score, dim=-1))\n",
    "        attn_vec = torch.matmul(attn_prob, wv)\n",
    "        return attn_vec.permute(0, 2, 1, 3).contiguous().contiguous().view(bs, x_len, -1)\n",
    "\n",
    "def _line_shift(x:Tensor, mask:bool=False):\n",
    "    \"Shift the line i of `x` by p-i elements to the left, is `mask` puts 0s on the diagonal.\"\n",
    "    bs,nh,n,p = x.size()\n",
    "    x_pad = torch.cat([x.new_zeros(bs,nh,n,1), x], dim=3)\n",
    "    x_shift = x_pad.view(bs,nh,p + 1,n)[:,:,1:].view_as(x)\n",
    "    if mask: x_shift.mul_(torch.tril(x.new_ones(n,p), p-n)[None,None,])\n",
    "    return x_shift\n",
    "\n",
    "class DecoderLayer(Module):\n",
    "    \"Basic block of a Transformer model.\"\n",
    "    #Can't use Sequential directly cause more than one input...\n",
    "    def __init__(self, n_heads:int, d_model:int, d_model_out:int,d_head:int, d_inner:int, \n",
    "                 resid_p:float=0., attn_p:float=0.,ff_p:float=0.,bias:bool=True, scale:bool=True, \n",
    "                 act:Activation=Activation.ReLU, double_drop:bool=True, attn_cls:Callable=MultiHeadAttention):\n",
    "        self.mhra = attn_cls(n_heads, d_model, d_head, resid_p=resid_p, attn_p=attn_p, bias=bias, scale=scale)\n",
    "        self.ff   = feed_forward(d_model, d_model_out, d_inner, ff_p=ff_p, act=act, double_drop=double_drop)\n",
    "\n",
    "    def forward(self, x:Tensor, mask:Tensor=None, **kwargs): return self.ff(self.mhra(x, mask=mask, **kwargs))\n",
    "\n",
    "class Transformer(Module):\n",
    "    \"Transformer model: https://arxiv.org/abs/1706.03762.\"\n",
    "    def __init__(self, n_layers:int, n_heads:int, d_model:int, d_model_out:int,d_head:int, d_inner:int,\n",
    "                 resid_p:float=0., attn_p:float=0., ff_p:float=0., embed_p:float=0., bias:bool=True, scale:bool=True,\n",
    "                 act:Activation=Activation.ReLU, double_drop:bool=True, attn_cls:Callable=MultiHeadAttention,\n",
    "                 learned_pos_enc:bool=True, mask:bool=True, dense_out:bool=False,final_p:float=0.):\n",
    "        self.mask = mask\n",
    "        self.drop_final = nn.Dropout(final_p)\n",
    "        self.dense_out = dense_out\n",
    "        self.layers = nn.ModuleList([DecoderLayer(\n",
    "            n_heads, d_model, d_model_out if k == n_layers-1 else d_model, d_head, d_inner, \n",
    "            resid_p=resid_p, attn_p=attn_p, ff_p=ff_p, bias=bias, scale=scale, act=act, double_drop=double_drop, \n",
    "            attn_cls=attn_cls) for k in range(n_layers)])\n",
    "\n",
    "    def reset(self): pass\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        inp = x\n",
    "        if self.dense_out: out = x.new()\n",
    "        for i,layer in enumerate(self.layers):\n",
    "            inp = layer(inp, mask=mask)\n",
    "            inp_d = self.drop_final(inp)\n",
    "            if self.dense_out: out = torch.cat([out, inp_d], dim=-1)\n",
    "        return out if self.dense_out else inp_d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoleculeTransformer(Module):\n",
    "    def __init__(self,n_layers,n_heads,d_model,embed_p:float=0,final_p:float=0, scalar_p:float=0,\n",
    "                 d_head=None,deep_decoder=False, dense_out=False, **kwargs):\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        d_head = ifnone(d_head, d_model//n_heads)\n",
    "        self.transformer = Transformer(n_layers=n_layers,n_heads=n_heads,d_model=d_model,d_model_out=d_model,\n",
    "                                       d_head=d_head,final_p=final_p,dense_out=dense_out,**kwargs)\n",
    "        \n",
    "        channels_out = d_model*n_layers if dense_out else d_model\n",
    "        channels_out_scalar = channels_out# + n_types + 1\n",
    "        if deep_decoder:\n",
    "            sl = [int(channels_out_scalar/(2**d)) for d in range(int(math.ceil(np.log2(channels_out_scalar/4)-1)))]\n",
    "            self.scalar = nn.Sequential(*(list(itertools.chain.from_iterable(\n",
    "                [[nn.Conv1d(sl[i],sl[i+1],1),nn.ReLU(),nn.BatchNorm1d(sl[i+1])] for i in range(len(sl)-1)])) + \n",
    "                [nn.Conv1d(sl[-1], 1, 1)]))\n",
    "        else:\n",
    "            self.scalar = nn.Conv1d(channels_out_scalar,max_atoms,1)\n",
    "        self.n_type_embedding=n_type_embedding = int((d_model/2)/max_atoms)\n",
    "        n_atom_embedding = d_model - 3 - max_atoms - max_atoms*n_type_embedding\n",
    "\n",
    "        self.atom_embedding = nn.Embedding(len(atoms)+1,n_atom_embedding)\n",
    "        self.type_embedding = nn.Embedding(n_types+1,n_type_embedding)\n",
    "        self.drop_type, self.drop_atom = nn.Dropout(embed_p),nn.Dropout(embed_p)\n",
    "        self.pos_enc = PositionalEncoding(d_model)\n",
    "        \n",
    "        pos = torch.arange(0, max_atoms)\n",
    "        self.pos_one_hot = torch.zeros(1,max_atoms,max_atoms).scatter_(1,pos.expand(1,max_atoms,max_atoms),1.)\n",
    "        # bs,ref0,ref1\n",
    "        # bs,ref0,h\n",
    "        self.drop_mask_type = torch.distributions.Bernoulli(torch.ones(max_atoms,max_atoms)*scalar_p)\n",
    "        \n",
    "    def forward(self,xyz,type,ext,atom,mask_atoms,n_atoms):\n",
    "        bs, _, n_pts = xyz.shape\n",
    "        # idea: count number of t > 0 and wipe out eg 10% of them, to simulate dropout on y (it's equivalent)        \n",
    "        if self.training:\n",
    "            type = type.masked_fill(self.drop_mask_type.sample().to(dtype=torch.uint8,device=xyz.device),-1)\n",
    "\n",
    "        t = self.drop_type(self.type_embedding((type+1).squeeze(1))).transpose(1,2).contiguous().view(bs,n_pts,-1) \n",
    "        # b,1,ref1,ref0,n_type_embedding =>  => b,ref0,ref1,n_type_embedding => b,ref0,ref1 * n_type_embedding\n",
    "        # t_one_hot=torch.zeros(bs,n_types+1,n_pts,n_pts,device=type.device,dtype=x.dtype).scatter_(1,type+1,1.)\n",
    "        # bn,n_types+1,max_atoms(target),max_atoms(reference)\n",
    "        #t_one_hot=t_one_hot.view(bs,-1,n_pts)\n",
    "        \n",
    "        a = self.drop_atom(self.atom_embedding((atom+1).squeeze(1)))\n",
    "\n",
    "        pos_one_hot = torch.eye(max_atoms,device=xyz.device,dtype=xyz.dtype).expand(bs,max_atoms,max_atoms) * \\\n",
    "                     (1-mask_atoms.squeeze(1).type_as(xyz)) # eye matrix masked by positive mask\n",
    "        x = torch.cat([pos_one_hot,xyz.transpose(1,2),a,t], dim=-1) \n",
    "        \n",
    "        x *= math.sqrt(self.d_model) # bn,max_atoms,d_model\n",
    "        x = self.transformer(x,mask_atoms).transpose(1,2) #  bn,max_atoms,d_model > bn,d_model,max_atoms\n",
    "        \n",
    "        scalar = self.scalar(x).view(bs,-1)\n",
    "        \n",
    "        type = type.transpose(2,3).contiguous().view(bs,-1) \n",
    "        # b,1,ref1,ref0,n_type_embedding -> b,1,ref0,ref1*n_type_embedding\n",
    "        \n",
    "        ext  = ext.view(bs,1,-1) #not used for molecular transformer right now\n",
    "        mask= (type != -1).squeeze(1) #b,ref0,ref1*n_type_embedding\n",
    "\n",
    "        return mask.detach(),type[mask].detach(),ext.detach(),scalar[mask].unsqueeze(-1) \n",
    "        # we only pass non-fake couplings to loss fn\n",
    "    \n",
    "    def reset(self): pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This callback allows to insert multiple stateful (not averaged) metrics in one pass. Addditionally we could add metrics for train if we want to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model instantiation: where's all your TPUs/GPUs when you need a decent hyperparam sweep?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "net, learner = None,None\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "net = MoleculeTransformer(n_layers=6, n_heads=max_atoms*8,d_model=256,d_inner=2048,\n",
    "                      resid_p=0., attn_p=0., ff_p=0., embed_p=0., final_p=0.,scalar_p=0.5,\n",
    "                      deep_decoder=False, dense_out=False)\n",
    "learner = Learner(data,net, loss_func=LMAEMaskedLoss(),)\n",
    "learner.callbacks.extend([\n",
    "    SaveModelCallback(learner, monitor='👉🏻LMAE👈🏻', mode='min'),\n",
    "    LMAEMetric(learner)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MoleculeTransformer\n",
       "======================================================================\n",
       "Layer (type)         Output Shape         Param #    Trainable \n",
       "======================================================================\n",
       "Dropout              [29, 256]            0          False     \n",
       "______________________________________________________________________\n",
       "Linear               [29, 696]            178,872    True      \n",
       "______________________________________________________________________\n",
       "Linear               [29, 256]            59,648     True      \n",
       "______________________________________________________________________\n",
       "Dropout              [232, 29, 29]        0          False     \n",
       "______________________________________________________________________\n",
       "Dropout              [29, 256]            0          False     \n",
       "______________________________________________________________________\n",
       "LayerNorm            [29, 256]            512        True      \n",
       "______________________________________________________________________\n",
       "Linear               [29, 2048]           526,336    True      \n",
       "______________________________________________________________________\n",
       "ReLU                 [29, 2048]           0          False     \n",
       "______________________________________________________________________\n",
       "Dropout              [29, 2048]           0          False     \n",
       "______________________________________________________________________\n",
       "Linear               [29, 256]            524,544    True      \n",
       "______________________________________________________________________\n",
       "Dropout              [29, 256]            0          False     \n",
       "______________________________________________________________________\n",
       "MergeLayer           [29, 256]            0          False     \n",
       "______________________________________________________________________\n",
       "LayerNorm            [29, 256]            512        True      \n",
       "______________________________________________________________________\n",
       "Linear               [29, 696]            178,872    True      \n",
       "______________________________________________________________________\n",
       "Linear               [29, 256]            59,648     True      \n",
       "______________________________________________________________________\n",
       "Dropout              [232, 29, 29]        0          False     \n",
       "______________________________________________________________________\n",
       "Dropout              [29, 256]            0          False     \n",
       "______________________________________________________________________\n",
       "LayerNorm            [29, 256]            512        True      \n",
       "______________________________________________________________________\n",
       "Linear               [29, 2048]           526,336    True      \n",
       "______________________________________________________________________\n",
       "ReLU                 [29, 2048]           0          False     \n",
       "______________________________________________________________________\n",
       "Dropout              [29, 2048]           0          False     \n",
       "______________________________________________________________________\n",
       "Linear               [29, 256]            524,544    True      \n",
       "______________________________________________________________________\n",
       "Dropout              [29, 256]            0          False     \n",
       "______________________________________________________________________\n",
       "MergeLayer           [29, 256]            0          False     \n",
       "______________________________________________________________________\n",
       "LayerNorm            [29, 256]            512        True      \n",
       "______________________________________________________________________\n",
       "Linear               [29, 696]            178,872    True      \n",
       "______________________________________________________________________\n",
       "Linear               [29, 256]            59,648     True      \n",
       "______________________________________________________________________\n",
       "Dropout              [232, 29, 29]        0          False     \n",
       "______________________________________________________________________\n",
       "Dropout              [29, 256]            0          False     \n",
       "______________________________________________________________________\n",
       "LayerNorm            [29, 256]            512        True      \n",
       "______________________________________________________________________\n",
       "Linear               [29, 2048]           526,336    True      \n",
       "______________________________________________________________________\n",
       "ReLU                 [29, 2048]           0          False     \n",
       "______________________________________________________________________\n",
       "Dropout              [29, 2048]           0          False     \n",
       "______________________________________________________________________\n",
       "Linear               [29, 256]            524,544    True      \n",
       "______________________________________________________________________\n",
       "Dropout              [29, 256]            0          False     \n",
       "______________________________________________________________________\n",
       "MergeLayer           [29, 256]            0          False     \n",
       "______________________________________________________________________\n",
       "LayerNorm            [29, 256]            512        True      \n",
       "______________________________________________________________________\n",
       "Linear               [29, 696]            178,872    True      \n",
       "______________________________________________________________________\n",
       "Linear               [29, 256]            59,648     True      \n",
       "______________________________________________________________________\n",
       "Dropout              [232, 29, 29]        0          False     \n",
       "______________________________________________________________________\n",
       "Dropout              [29, 256]            0          False     \n",
       "______________________________________________________________________\n",
       "LayerNorm            [29, 256]            512        True      \n",
       "______________________________________________________________________\n",
       "Linear               [29, 2048]           526,336    True      \n",
       "______________________________________________________________________\n",
       "ReLU                 [29, 2048]           0          False     \n",
       "______________________________________________________________________\n",
       "Dropout              [29, 2048]           0          False     \n",
       "______________________________________________________________________\n",
       "Linear               [29, 256]            524,544    True      \n",
       "______________________________________________________________________\n",
       "Dropout              [29, 256]            0          False     \n",
       "______________________________________________________________________\n",
       "MergeLayer           [29, 256]            0          False     \n",
       "______________________________________________________________________\n",
       "LayerNorm            [29, 256]            512        True      \n",
       "______________________________________________________________________\n",
       "Linear               [29, 696]            178,872    True      \n",
       "______________________________________________________________________\n",
       "Linear               [29, 256]            59,648     True      \n",
       "______________________________________________________________________\n",
       "Dropout              [232, 29, 29]        0          False     \n",
       "______________________________________________________________________\n",
       "Dropout              [29, 256]            0          False     \n",
       "______________________________________________________________________\n",
       "LayerNorm            [29, 256]            512        True      \n",
       "______________________________________________________________________\n",
       "Linear               [29, 2048]           526,336    True      \n",
       "______________________________________________________________________\n",
       "ReLU                 [29, 2048]           0          False     \n",
       "______________________________________________________________________\n",
       "Dropout              [29, 2048]           0          False     \n",
       "______________________________________________________________________\n",
       "Linear               [29, 256]            524,544    True      \n",
       "______________________________________________________________________\n",
       "Dropout              [29, 256]            0          False     \n",
       "______________________________________________________________________\n",
       "MergeLayer           [29, 256]            0          False     \n",
       "______________________________________________________________________\n",
       "LayerNorm            [29, 256]            512        True      \n",
       "______________________________________________________________________\n",
       "Linear               [29, 696]            178,872    True      \n",
       "______________________________________________________________________\n",
       "Linear               [29, 256]            59,648     True      \n",
       "______________________________________________________________________\n",
       "Dropout              [232, 29, 29]        0          False     \n",
       "______________________________________________________________________\n",
       "Dropout              [29, 256]            0          False     \n",
       "______________________________________________________________________\n",
       "LayerNorm            [29, 256]            512        True      \n",
       "______________________________________________________________________\n",
       "Linear               [29, 2048]           526,336    True      \n",
       "______________________________________________________________________\n",
       "ReLU                 [29, 2048]           0          False     \n",
       "______________________________________________________________________\n",
       "Dropout              [29, 2048]           0          False     \n",
       "______________________________________________________________________\n",
       "Linear               [29, 256]            524,544    True      \n",
       "______________________________________________________________________\n",
       "Dropout              [29, 256]            0          False     \n",
       "______________________________________________________________________\n",
       "MergeLayer           [29, 256]            0          False     \n",
       "______________________________________________________________________\n",
       "LayerNorm            [29, 256]            512        True      \n",
       "______________________________________________________________________\n",
       "Conv1d               [29, 29]             7,453      True      \n",
       "______________________________________________________________________\n",
       "Embedding            [29, 108]            648        True      \n",
       "______________________________________________________________________\n",
       "Embedding            [29, 29, 4]          36         True      \n",
       "______________________________________________________________________\n",
       "Dropout              [29, 29, 4]          0          False     \n",
       "______________________________________________________________________\n",
       "Dropout              [29, 108]            0          False     \n",
       "______________________________________________________________________\n",
       "\n",
       "Total params: 7,750,681\n",
       "Total trainable params: 7,750,681\n",
       "Total non-trainable params: 0\n",
       "Optimized with 'torch.optim.adam.Adam', betas=(0.9, 0.99)\n",
       "Using true weight decay as discussed in https://www.fast.ai/2018/07/02/adam-weight-decay/ \n",
       "Loss function : LMAEMaskedLoss\n",
       "======================================================================\n",
       "Callbacks functions applied \n",
       "    SaveModelCallback\n",
       "    LMAEMetric"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MoleculeTransformer(\n",
       "  (transformer): Transformer(\n",
       "    (drop_final): Dropout(p=0.0)\n",
       "    (layers): ModuleList(\n",
       "      (0): DecoderLayer(\n",
       "        (mhra): MultiHeadAttention(\n",
       "          (attention): Linear(in_features=256, out_features=696, bias=True)\n",
       "          (out): Linear(in_features=232, out_features=256, bias=True)\n",
       "          (drop_att): Dropout(p=0.0)\n",
       "          (drop_res): Dropout(p=0.0)\n",
       "          (ln): LayerNorm(torch.Size([256]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=256, out_features=2048, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.0)\n",
       "            (3): Linear(in_features=2048, out_features=256, bias=True)\n",
       "            (4): Dropout(p=0.0)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm(torch.Size([256]), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): DecoderLayer(\n",
       "        (mhra): MultiHeadAttention(\n",
       "          (attention): Linear(in_features=256, out_features=696, bias=True)\n",
       "          (out): Linear(in_features=232, out_features=256, bias=True)\n",
       "          (drop_att): Dropout(p=0.0)\n",
       "          (drop_res): Dropout(p=0.0)\n",
       "          (ln): LayerNorm(torch.Size([256]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=256, out_features=2048, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.0)\n",
       "            (3): Linear(in_features=2048, out_features=256, bias=True)\n",
       "            (4): Dropout(p=0.0)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm(torch.Size([256]), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): DecoderLayer(\n",
       "        (mhra): MultiHeadAttention(\n",
       "          (attention): Linear(in_features=256, out_features=696, bias=True)\n",
       "          (out): Linear(in_features=232, out_features=256, bias=True)\n",
       "          (drop_att): Dropout(p=0.0)\n",
       "          (drop_res): Dropout(p=0.0)\n",
       "          (ln): LayerNorm(torch.Size([256]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=256, out_features=2048, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.0)\n",
       "            (3): Linear(in_features=2048, out_features=256, bias=True)\n",
       "            (4): Dropout(p=0.0)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm(torch.Size([256]), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): DecoderLayer(\n",
       "        (mhra): MultiHeadAttention(\n",
       "          (attention): Linear(in_features=256, out_features=696, bias=True)\n",
       "          (out): Linear(in_features=232, out_features=256, bias=True)\n",
       "          (drop_att): Dropout(p=0.0)\n",
       "          (drop_res): Dropout(p=0.0)\n",
       "          (ln): LayerNorm(torch.Size([256]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=256, out_features=2048, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.0)\n",
       "            (3): Linear(in_features=2048, out_features=256, bias=True)\n",
       "            (4): Dropout(p=0.0)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm(torch.Size([256]), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): DecoderLayer(\n",
       "        (mhra): MultiHeadAttention(\n",
       "          (attention): Linear(in_features=256, out_features=696, bias=True)\n",
       "          (out): Linear(in_features=232, out_features=256, bias=True)\n",
       "          (drop_att): Dropout(p=0.0)\n",
       "          (drop_res): Dropout(p=0.0)\n",
       "          (ln): LayerNorm(torch.Size([256]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=256, out_features=2048, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.0)\n",
       "            (3): Linear(in_features=2048, out_features=256, bias=True)\n",
       "            (4): Dropout(p=0.0)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm(torch.Size([256]), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): DecoderLayer(\n",
       "        (mhra): MultiHeadAttention(\n",
       "          (attention): Linear(in_features=256, out_features=696, bias=True)\n",
       "          (out): Linear(in_features=232, out_features=256, bias=True)\n",
       "          (drop_att): Dropout(p=0.0)\n",
       "          (drop_res): Dropout(p=0.0)\n",
       "          (ln): LayerNorm(torch.Size([256]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=256, out_features=2048, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.0)\n",
       "            (3): Linear(in_features=2048, out_features=256, bias=True)\n",
       "            (4): Dropout(p=0.0)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm(torch.Size([256]), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (scalar): Conv1d(256, 29, kernel_size=(1,), stride=(1,))\n",
       "  (atom_embedding): Embedding(6, 108)\n",
       "  (type_embedding): Embedding(9, 4)\n",
       "  (drop_type): Dropout(p=0)\n",
       "  (drop_atom): Dropout(p=0)\n",
       "  (pos_enc): PositionalEncoding()\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sub_fname = \"loss-1.4032val-1.4586\" # uncomment or set None to skip loading trained net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOT loaded!  name 'sub_fname' is not defined\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print(f\"Attempting to load: {sub_fname}... \", end=\"\")\n",
    "    learner.load(sub_fname, strict=False,with_opt=False)\n",
    "    print(\"Loaded\")\n",
    "except Exception as e:\n",
    "    print(\"NOT loaded! \", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# neural weight manual transplant \n",
    "if False:\n",
    "    for name,child in learner.model.named_children():\n",
    "        print(\"CHILD: \",name)\n",
    "        if not (name in ['scalar', 'magnetic', 'dipole', 'potential']):\n",
    "            print(\"FREEZING\")\n",
    "            for param in child.parameters(): param.requires_grad = False\n",
    "        else:\n",
    "            for name,param in child.named_parameters(): \n",
    "                param.requires_grad = True\n",
    "    learner.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learner = learner.to_parallel()#.to_fp16()#loss_scale=128, dynamic=False)\n",
    "data.batch_size = int(2048//2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Real loss func. Need to test different auxiliary tasks weights: `magnetic_w`, `dipole_w`, `potential`, weights of indivial `lmae`s: `types_w`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learner.loss_func = LMAEMaskedLoss(types_w = [1] + [1] * (n_types-1))#, proxy_log=lambda x: x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n",
      "Min numerical gradient: 1.74E-05\n",
      "Min loss divided by 10: 3.31E-03\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "! rm models/bestmodel.pth\n",
    "learner.lr_find()\n",
    "learner.recorder.plot(suggestion=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>👉🏻LMAE👈🏻</th>\n",
       "      <th>lmae0</th>\n",
       "      <th>lmae1</th>\n",
       "      <th>lmae2</th>\n",
       "      <th>lmae3</th>\n",
       "      <th>lmae4</th>\n",
       "      <th>lmae5</th>\n",
       "      <th>lmae6</th>\n",
       "      <th>lmae7</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.923836</td>\n",
       "      <td>1.892014</td>\n",
       "      <td>1.903274</td>\n",
       "      <td>4.544931</td>\n",
       "      <td>2.481053</td>\n",
       "      <td>3.839742</td>\n",
       "      <td>1.115050</td>\n",
       "      <td>1.114239</td>\n",
       "      <td>1.128871</td>\n",
       "      <td>1.055652</td>\n",
       "      <td>-0.053344</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.877820</td>\n",
       "      <td>1.814517</td>\n",
       "      <td>1.825099</td>\n",
       "      <td>4.551650</td>\n",
       "      <td>2.154546</td>\n",
       "      <td>3.840792</td>\n",
       "      <td>1.101488</td>\n",
       "      <td>0.961620</td>\n",
       "      <td>1.100328</td>\n",
       "      <td>0.966687</td>\n",
       "      <td>-0.076317</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.836131</td>\n",
       "      <td>1.748455</td>\n",
       "      <td>1.757816</td>\n",
       "      <td>4.546707</td>\n",
       "      <td>1.720144</td>\n",
       "      <td>3.838837</td>\n",
       "      <td>1.069207</td>\n",
       "      <td>0.952791</td>\n",
       "      <td>1.089460</td>\n",
       "      <td>0.928286</td>\n",
       "      <td>-0.082907</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.792408</td>\n",
       "      <td>1.704827</td>\n",
       "      <td>1.713639</td>\n",
       "      <td>4.545979</td>\n",
       "      <td>1.411998</td>\n",
       "      <td>3.839187</td>\n",
       "      <td>1.052166</td>\n",
       "      <td>0.935532</td>\n",
       "      <td>1.083355</td>\n",
       "      <td>0.931643</td>\n",
       "      <td>-0.090748</td>\n",
       "      <td>00:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.755377</td>\n",
       "      <td>1.662841</td>\n",
       "      <td>1.670073</td>\n",
       "      <td>4.540301</td>\n",
       "      <td>1.138855</td>\n",
       "      <td>3.834455</td>\n",
       "      <td>1.006818</td>\n",
       "      <td>0.920891</td>\n",
       "      <td>1.084821</td>\n",
       "      <td>0.913618</td>\n",
       "      <td>-0.079175</td>\n",
       "      <td>00:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.725746</td>\n",
       "      <td>1.642192</td>\n",
       "      <td>1.649055</td>\n",
       "      <td>4.536243</td>\n",
       "      <td>1.058694</td>\n",
       "      <td>3.829971</td>\n",
       "      <td>0.984462</td>\n",
       "      <td>0.916381</td>\n",
       "      <td>1.052880</td>\n",
       "      <td>0.902911</td>\n",
       "      <td>-0.089103</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.698207</td>\n",
       "      <td>1.615718</td>\n",
       "      <td>1.621947</td>\n",
       "      <td>4.535979</td>\n",
       "      <td>0.872103</td>\n",
       "      <td>3.832623</td>\n",
       "      <td>0.965210</td>\n",
       "      <td>0.896858</td>\n",
       "      <td>1.049498</td>\n",
       "      <td>0.919115</td>\n",
       "      <td>-0.095807</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.669307</td>\n",
       "      <td>1.594351</td>\n",
       "      <td>1.598292</td>\n",
       "      <td>4.533016</td>\n",
       "      <td>0.790885</td>\n",
       "      <td>3.827724</td>\n",
       "      <td>0.910474</td>\n",
       "      <td>0.881095</td>\n",
       "      <td>1.032679</td>\n",
       "      <td>0.905321</td>\n",
       "      <td>-0.094857</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.641033</td>\n",
       "      <td>1.579051</td>\n",
       "      <td>1.581796</td>\n",
       "      <td>4.528691</td>\n",
       "      <td>0.763807</td>\n",
       "      <td>3.818660</td>\n",
       "      <td>0.855532</td>\n",
       "      <td>0.881279</td>\n",
       "      <td>1.005026</td>\n",
       "      <td>0.912301</td>\n",
       "      <td>-0.110925</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.613362</td>\n",
       "      <td>1.540878</td>\n",
       "      <td>1.542137</td>\n",
       "      <td>4.520724</td>\n",
       "      <td>0.668826</td>\n",
       "      <td>3.790997</td>\n",
       "      <td>0.756880</td>\n",
       "      <td>0.845831</td>\n",
       "      <td>0.982403</td>\n",
       "      <td>0.896324</td>\n",
       "      <td>-0.124892</td>\n",
       "      <td>00:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.589883</td>\n",
       "      <td>1.519280</td>\n",
       "      <td>1.519303</td>\n",
       "      <td>4.510037</td>\n",
       "      <td>0.622508</td>\n",
       "      <td>3.749439</td>\n",
       "      <td>0.692154</td>\n",
       "      <td>0.843737</td>\n",
       "      <td>0.974552</td>\n",
       "      <td>0.885710</td>\n",
       "      <td>-0.123713</td>\n",
       "      <td>00:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.561002</td>\n",
       "      <td>1.489564</td>\n",
       "      <td>1.488681</td>\n",
       "      <td>4.490968</td>\n",
       "      <td>0.577850</td>\n",
       "      <td>3.697011</td>\n",
       "      <td>0.623878</td>\n",
       "      <td>0.817763</td>\n",
       "      <td>0.955849</td>\n",
       "      <td>0.867659</td>\n",
       "      <td>-0.121529</td>\n",
       "      <td>00:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.536811</td>\n",
       "      <td>1.467632</td>\n",
       "      <td>1.466330</td>\n",
       "      <td>4.475904</td>\n",
       "      <td>0.531211</td>\n",
       "      <td>3.660041</td>\n",
       "      <td>0.574022</td>\n",
       "      <td>0.807370</td>\n",
       "      <td>0.950081</td>\n",
       "      <td>0.861580</td>\n",
       "      <td>-0.129574</td>\n",
       "      <td>00:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.517264</td>\n",
       "      <td>1.456582</td>\n",
       "      <td>1.455522</td>\n",
       "      <td>4.460897</td>\n",
       "      <td>0.525507</td>\n",
       "      <td>3.624971</td>\n",
       "      <td>0.584734</td>\n",
       "      <td>0.794764</td>\n",
       "      <td>0.930687</td>\n",
       "      <td>0.860466</td>\n",
       "      <td>-0.137853</td>\n",
       "      <td>00:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.494236</td>\n",
       "      <td>1.428141</td>\n",
       "      <td>1.426566</td>\n",
       "      <td>4.446099</td>\n",
       "      <td>0.475640</td>\n",
       "      <td>3.586227</td>\n",
       "      <td>0.517633</td>\n",
       "      <td>0.784153</td>\n",
       "      <td>0.904597</td>\n",
       "      <td>0.846135</td>\n",
       "      <td>-0.147958</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.469345</td>\n",
       "      <td>1.410141</td>\n",
       "      <td>1.408949</td>\n",
       "      <td>4.430891</td>\n",
       "      <td>0.469025</td>\n",
       "      <td>3.534968</td>\n",
       "      <td>0.476169</td>\n",
       "      <td>0.765884</td>\n",
       "      <td>0.901311</td>\n",
       "      <td>0.853857</td>\n",
       "      <td>-0.160511</td>\n",
       "      <td>00:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.447423</td>\n",
       "      <td>1.388479</td>\n",
       "      <td>1.386788</td>\n",
       "      <td>4.413966</td>\n",
       "      <td>0.449502</td>\n",
       "      <td>3.467374</td>\n",
       "      <td>0.439990</td>\n",
       "      <td>0.740786</td>\n",
       "      <td>0.902595</td>\n",
       "      <td>0.848826</td>\n",
       "      <td>-0.168737</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.424291</td>\n",
       "      <td>1.352320</td>\n",
       "      <td>1.351628</td>\n",
       "      <td>4.393544</td>\n",
       "      <td>0.395242</td>\n",
       "      <td>3.341342</td>\n",
       "      <td>0.400889</td>\n",
       "      <td>0.725934</td>\n",
       "      <td>0.890798</td>\n",
       "      <td>0.841399</td>\n",
       "      <td>-0.176123</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1.399017</td>\n",
       "      <td>1.328408</td>\n",
       "      <td>1.328132</td>\n",
       "      <td>4.364679</td>\n",
       "      <td>0.384392</td>\n",
       "      <td>3.207730</td>\n",
       "      <td>0.402911</td>\n",
       "      <td>0.715249</td>\n",
       "      <td>0.880015</td>\n",
       "      <td>0.841686</td>\n",
       "      <td>-0.171609</td>\n",
       "      <td>00:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>1.375165</td>\n",
       "      <td>1.291644</td>\n",
       "      <td>1.293654</td>\n",
       "      <td>4.331824</td>\n",
       "      <td>0.409631</td>\n",
       "      <td>2.930928</td>\n",
       "      <td>0.396976</td>\n",
       "      <td>0.702520</td>\n",
       "      <td>0.877921</td>\n",
       "      <td>0.834374</td>\n",
       "      <td>-0.134942</td>\n",
       "      <td>00:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.345034</td>\n",
       "      <td>1.250263</td>\n",
       "      <td>1.254080</td>\n",
       "      <td>4.301385</td>\n",
       "      <td>0.395136</td>\n",
       "      <td>2.632747</td>\n",
       "      <td>0.400456</td>\n",
       "      <td>0.719088</td>\n",
       "      <td>0.881278</td>\n",
       "      <td>0.841767</td>\n",
       "      <td>-0.139217</td>\n",
       "      <td>00:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>1.308315</td>\n",
       "      <td>1.215219</td>\n",
       "      <td>1.217924</td>\n",
       "      <td>4.261840</td>\n",
       "      <td>0.394277</td>\n",
       "      <td>2.399891</td>\n",
       "      <td>0.403545</td>\n",
       "      <td>0.703281</td>\n",
       "      <td>0.863887</td>\n",
       "      <td>0.831154</td>\n",
       "      <td>-0.114480</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>1.284607</td>\n",
       "      <td>1.186857</td>\n",
       "      <td>1.189392</td>\n",
       "      <td>4.212810</td>\n",
       "      <td>0.374185</td>\n",
       "      <td>2.230777</td>\n",
       "      <td>0.400012</td>\n",
       "      <td>0.711278</td>\n",
       "      <td>0.861926</td>\n",
       "      <td>0.859567</td>\n",
       "      <td>-0.135422</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>1.261012</td>\n",
       "      <td>1.178134</td>\n",
       "      <td>1.178780</td>\n",
       "      <td>4.177521</td>\n",
       "      <td>0.382206</td>\n",
       "      <td>2.086592</td>\n",
       "      <td>0.464483</td>\n",
       "      <td>0.711483</td>\n",
       "      <td>0.882410</td>\n",
       "      <td>0.870441</td>\n",
       "      <td>-0.144900</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>1.242319</td>\n",
       "      <td>1.137185</td>\n",
       "      <td>1.137640</td>\n",
       "      <td>4.105221</td>\n",
       "      <td>0.358718</td>\n",
       "      <td>1.898090</td>\n",
       "      <td>0.393849</td>\n",
       "      <td>0.728541</td>\n",
       "      <td>0.866972</td>\n",
       "      <td>0.853932</td>\n",
       "      <td>-0.104203</td>\n",
       "      <td>00:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.221243</td>\n",
       "      <td>1.138403</td>\n",
       "      <td>1.136995</td>\n",
       "      <td>4.046175</td>\n",
       "      <td>0.400661</td>\n",
       "      <td>1.820531</td>\n",
       "      <td>0.460289</td>\n",
       "      <td>0.721628</td>\n",
       "      <td>0.884970</td>\n",
       "      <td>0.860725</td>\n",
       "      <td>-0.099023</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>1.191724</td>\n",
       "      <td>1.108611</td>\n",
       "      <td>1.107116</td>\n",
       "      <td>3.988182</td>\n",
       "      <td>0.433007</td>\n",
       "      <td>1.733774</td>\n",
       "      <td>0.400385</td>\n",
       "      <td>0.710866</td>\n",
       "      <td>0.877974</td>\n",
       "      <td>0.851518</td>\n",
       "      <td>-0.138779</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>1.179291</td>\n",
       "      <td>1.092920</td>\n",
       "      <td>1.088558</td>\n",
       "      <td>3.927653</td>\n",
       "      <td>0.384566</td>\n",
       "      <td>1.683049</td>\n",
       "      <td>0.396803</td>\n",
       "      <td>0.730759</td>\n",
       "      <td>0.873813</td>\n",
       "      <td>0.867311</td>\n",
       "      <td>-0.155486</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>1.158669</td>\n",
       "      <td>1.056754</td>\n",
       "      <td>1.053225</td>\n",
       "      <td>3.849535</td>\n",
       "      <td>0.341423</td>\n",
       "      <td>1.570581</td>\n",
       "      <td>0.376412</td>\n",
       "      <td>0.702197</td>\n",
       "      <td>0.874811</td>\n",
       "      <td>0.860835</td>\n",
       "      <td>-0.149994</td>\n",
       "      <td>00:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>1.138293</td>\n",
       "      <td>1.041538</td>\n",
       "      <td>1.038247</td>\n",
       "      <td>3.773652</td>\n",
       "      <td>0.365857</td>\n",
       "      <td>1.579081</td>\n",
       "      <td>0.340936</td>\n",
       "      <td>0.700713</td>\n",
       "      <td>0.872467</td>\n",
       "      <td>0.848862</td>\n",
       "      <td>-0.175591</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.122526</td>\n",
       "      <td>1.030035</td>\n",
       "      <td>1.025777</td>\n",
       "      <td>3.667223</td>\n",
       "      <td>0.372860</td>\n",
       "      <td>1.504975</td>\n",
       "      <td>0.379921</td>\n",
       "      <td>0.702485</td>\n",
       "      <td>0.863527</td>\n",
       "      <td>0.852294</td>\n",
       "      <td>-0.137073</td>\n",
       "      <td>00:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>1.088340</td>\n",
       "      <td>1.009266</td>\n",
       "      <td>1.007081</td>\n",
       "      <td>3.567387</td>\n",
       "      <td>0.357404</td>\n",
       "      <td>1.544812</td>\n",
       "      <td>0.355661</td>\n",
       "      <td>0.692310</td>\n",
       "      <td>0.860421</td>\n",
       "      <td>0.860881</td>\n",
       "      <td>-0.182233</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>1.063123</td>\n",
       "      <td>0.970766</td>\n",
       "      <td>0.967629</td>\n",
       "      <td>3.343905</td>\n",
       "      <td>0.362684</td>\n",
       "      <td>1.475151</td>\n",
       "      <td>0.339467</td>\n",
       "      <td>0.686917</td>\n",
       "      <td>0.869051</td>\n",
       "      <td>0.854812</td>\n",
       "      <td>-0.190960</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>1.030947</td>\n",
       "      <td>0.937928</td>\n",
       "      <td>0.933573</td>\n",
       "      <td>3.091911</td>\n",
       "      <td>0.364635</td>\n",
       "      <td>1.417481</td>\n",
       "      <td>0.329089</td>\n",
       "      <td>0.707405</td>\n",
       "      <td>0.865571</td>\n",
       "      <td>0.852169</td>\n",
       "      <td>-0.159675</td>\n",
       "      <td>00:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.988084</td>\n",
       "      <td>0.879757</td>\n",
       "      <td>0.874077</td>\n",
       "      <td>2.820712</td>\n",
       "      <td>0.336215</td>\n",
       "      <td>1.382177</td>\n",
       "      <td>0.272890</td>\n",
       "      <td>0.672034</td>\n",
       "      <td>0.850804</td>\n",
       "      <td>0.829906</td>\n",
       "      <td>-0.172119</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.948089</td>\n",
       "      <td>0.853719</td>\n",
       "      <td>0.848384</td>\n",
       "      <td>2.612794</td>\n",
       "      <td>0.339465</td>\n",
       "      <td>1.371247</td>\n",
       "      <td>0.270051</td>\n",
       "      <td>0.668963</td>\n",
       "      <td>0.854706</td>\n",
       "      <td>0.837012</td>\n",
       "      <td>-0.167166</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.911878</td>\n",
       "      <td>0.812917</td>\n",
       "      <td>0.810353</td>\n",
       "      <td>2.422790</td>\n",
       "      <td>0.306297</td>\n",
       "      <td>1.305248</td>\n",
       "      <td>0.282196</td>\n",
       "      <td>0.666320</td>\n",
       "      <td>0.857141</td>\n",
       "      <td>0.830411</td>\n",
       "      <td>-0.187578</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.881682</td>\n",
       "      <td>0.776232</td>\n",
       "      <td>0.773372</td>\n",
       "      <td>2.286393</td>\n",
       "      <td>0.312303</td>\n",
       "      <td>1.283530</td>\n",
       "      <td>0.241664</td>\n",
       "      <td>0.633994</td>\n",
       "      <td>0.830950</td>\n",
       "      <td>0.829970</td>\n",
       "      <td>-0.231825</td>\n",
       "      <td>00:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.853535</td>\n",
       "      <td>0.753680</td>\n",
       "      <td>0.749968</td>\n",
       "      <td>2.158809</td>\n",
       "      <td>0.323811</td>\n",
       "      <td>1.183773</td>\n",
       "      <td>0.232670</td>\n",
       "      <td>0.647699</td>\n",
       "      <td>0.832305</td>\n",
       "      <td>0.839643</td>\n",
       "      <td>-0.218967</td>\n",
       "      <td>00:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.825417</td>\n",
       "      <td>0.745766</td>\n",
       "      <td>0.742338</td>\n",
       "      <td>2.118741</td>\n",
       "      <td>0.288449</td>\n",
       "      <td>1.188919</td>\n",
       "      <td>0.244282</td>\n",
       "      <td>0.648807</td>\n",
       "      <td>0.833947</td>\n",
       "      <td>0.823046</td>\n",
       "      <td>-0.207486</td>\n",
       "      <td>00:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.802104</td>\n",
       "      <td>0.722708</td>\n",
       "      <td>0.720510</td>\n",
       "      <td>2.054714</td>\n",
       "      <td>0.240578</td>\n",
       "      <td>1.195355</td>\n",
       "      <td>0.231269</td>\n",
       "      <td>0.626232</td>\n",
       "      <td>0.813684</td>\n",
       "      <td>0.817676</td>\n",
       "      <td>-0.215430</td>\n",
       "      <td>00:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.773969</td>\n",
       "      <td>0.707631</td>\n",
       "      <td>0.702118</td>\n",
       "      <td>2.021908</td>\n",
       "      <td>0.227378</td>\n",
       "      <td>1.132035</td>\n",
       "      <td>0.191450</td>\n",
       "      <td>0.617582</td>\n",
       "      <td>0.819479</td>\n",
       "      <td>0.826353</td>\n",
       "      <td>-0.219239</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.765029</td>\n",
       "      <td>0.698606</td>\n",
       "      <td>0.696980</td>\n",
       "      <td>2.008086</td>\n",
       "      <td>0.240928</td>\n",
       "      <td>1.122008</td>\n",
       "      <td>0.192737</td>\n",
       "      <td>0.605810</td>\n",
       "      <td>0.804484</td>\n",
       "      <td>0.822192</td>\n",
       "      <td>-0.220408</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.738696</td>\n",
       "      <td>0.658858</td>\n",
       "      <td>0.654932</td>\n",
       "      <td>1.928736</td>\n",
       "      <td>0.160455</td>\n",
       "      <td>1.058563</td>\n",
       "      <td>0.145369</td>\n",
       "      <td>0.590711</td>\n",
       "      <td>0.793882</td>\n",
       "      <td>0.811967</td>\n",
       "      <td>-0.250228</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.722087</td>\n",
       "      <td>0.655260</td>\n",
       "      <td>0.652594</td>\n",
       "      <td>1.938915</td>\n",
       "      <td>0.185099</td>\n",
       "      <td>1.043736</td>\n",
       "      <td>0.149113</td>\n",
       "      <td>0.586637</td>\n",
       "      <td>0.799009</td>\n",
       "      <td>0.797424</td>\n",
       "      <td>-0.279178</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.707340</td>\n",
       "      <td>0.667022</td>\n",
       "      <td>0.663311</td>\n",
       "      <td>1.909997</td>\n",
       "      <td>0.183366</td>\n",
       "      <td>1.128880</td>\n",
       "      <td>0.149517</td>\n",
       "      <td>0.581355</td>\n",
       "      <td>0.799011</td>\n",
       "      <td>0.799168</td>\n",
       "      <td>-0.244801</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.684250</td>\n",
       "      <td>0.633633</td>\n",
       "      <td>0.631146</td>\n",
       "      <td>1.825367</td>\n",
       "      <td>0.178207</td>\n",
       "      <td>1.047741</td>\n",
       "      <td>0.112450</td>\n",
       "      <td>0.566847</td>\n",
       "      <td>0.789867</td>\n",
       "      <td>0.790937</td>\n",
       "      <td>-0.262246</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.667008</td>\n",
       "      <td>0.600784</td>\n",
       "      <td>0.598021</td>\n",
       "      <td>1.756916</td>\n",
       "      <td>0.106953</td>\n",
       "      <td>0.999203</td>\n",
       "      <td>0.090114</td>\n",
       "      <td>0.557337</td>\n",
       "      <td>0.766616</td>\n",
       "      <td>0.774552</td>\n",
       "      <td>-0.267520</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.647595</td>\n",
       "      <td>0.608364</td>\n",
       "      <td>0.605881</td>\n",
       "      <td>1.767416</td>\n",
       "      <td>0.156786</td>\n",
       "      <td>0.953133</td>\n",
       "      <td>0.124220</td>\n",
       "      <td>0.570210</td>\n",
       "      <td>0.776492</td>\n",
       "      <td>0.782771</td>\n",
       "      <td>-0.283976</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.628598</td>\n",
       "      <td>0.587941</td>\n",
       "      <td>0.584384</td>\n",
       "      <td>1.765794</td>\n",
       "      <td>0.107037</td>\n",
       "      <td>0.970330</td>\n",
       "      <td>0.072033</td>\n",
       "      <td>0.531358</td>\n",
       "      <td>0.756546</td>\n",
       "      <td>0.776470</td>\n",
       "      <td>-0.304498</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.618050</td>\n",
       "      <td>0.580935</td>\n",
       "      <td>0.576376</td>\n",
       "      <td>1.708804</td>\n",
       "      <td>0.082940</td>\n",
       "      <td>0.976976</td>\n",
       "      <td>0.083811</td>\n",
       "      <td>0.529288</td>\n",
       "      <td>0.756104</td>\n",
       "      <td>0.771389</td>\n",
       "      <td>-0.298300</td>\n",
       "      <td>00:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.603789</td>\n",
       "      <td>0.560784</td>\n",
       "      <td>0.557112</td>\n",
       "      <td>1.727062</td>\n",
       "      <td>0.057660</td>\n",
       "      <td>0.886888</td>\n",
       "      <td>0.046877</td>\n",
       "      <td>0.528047</td>\n",
       "      <td>0.752510</td>\n",
       "      <td>0.765291</td>\n",
       "      <td>-0.307443</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.589746</td>\n",
       "      <td>0.558662</td>\n",
       "      <td>0.555378</td>\n",
       "      <td>1.720502</td>\n",
       "      <td>0.110310</td>\n",
       "      <td>0.885418</td>\n",
       "      <td>0.039288</td>\n",
       "      <td>0.508501</td>\n",
       "      <td>0.754816</td>\n",
       "      <td>0.762739</td>\n",
       "      <td>-0.338552</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.569725</td>\n",
       "      <td>0.546057</td>\n",
       "      <td>0.543375</td>\n",
       "      <td>1.667767</td>\n",
       "      <td>0.054853</td>\n",
       "      <td>0.890882</td>\n",
       "      <td>0.028558</td>\n",
       "      <td>0.519547</td>\n",
       "      <td>0.743991</td>\n",
       "      <td>0.768046</td>\n",
       "      <td>-0.326647</td>\n",
       "      <td>00:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.549088</td>\n",
       "      <td>0.513601</td>\n",
       "      <td>0.509014</td>\n",
       "      <td>1.638978</td>\n",
       "      <td>0.013629</td>\n",
       "      <td>0.843819</td>\n",
       "      <td>-0.009630</td>\n",
       "      <td>0.486969</td>\n",
       "      <td>0.722025</td>\n",
       "      <td>0.753949</td>\n",
       "      <td>-0.377627</td>\n",
       "      <td>00:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.539449</td>\n",
       "      <td>0.511211</td>\n",
       "      <td>0.508493</td>\n",
       "      <td>1.645250</td>\n",
       "      <td>0.015292</td>\n",
       "      <td>0.812835</td>\n",
       "      <td>-0.004344</td>\n",
       "      <td>0.472774</td>\n",
       "      <td>0.712679</td>\n",
       "      <td>0.757672</td>\n",
       "      <td>-0.344214</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.528878</td>\n",
       "      <td>0.505815</td>\n",
       "      <td>0.501539</td>\n",
       "      <td>1.623610</td>\n",
       "      <td>0.020486</td>\n",
       "      <td>0.793692</td>\n",
       "      <td>-0.022176</td>\n",
       "      <td>0.479835</td>\n",
       "      <td>0.719469</td>\n",
       "      <td>0.748931</td>\n",
       "      <td>-0.351538</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.511115</td>\n",
       "      <td>0.489578</td>\n",
       "      <td>0.483804</td>\n",
       "      <td>1.592888</td>\n",
       "      <td>-0.018983</td>\n",
       "      <td>0.779248</td>\n",
       "      <td>-0.025421</td>\n",
       "      <td>0.456451</td>\n",
       "      <td>0.716207</td>\n",
       "      <td>0.742908</td>\n",
       "      <td>-0.372868</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.494335</td>\n",
       "      <td>0.489411</td>\n",
       "      <td>0.485643</td>\n",
       "      <td>1.598357</td>\n",
       "      <td>0.000941</td>\n",
       "      <td>0.764102</td>\n",
       "      <td>-0.032996</td>\n",
       "      <td>0.468687</td>\n",
       "      <td>0.704179</td>\n",
       "      <td>0.740202</td>\n",
       "      <td>-0.358325</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.481194</td>\n",
       "      <td>0.476114</td>\n",
       "      <td>0.473713</td>\n",
       "      <td>1.587115</td>\n",
       "      <td>-0.009114</td>\n",
       "      <td>0.750430</td>\n",
       "      <td>-0.040881</td>\n",
       "      <td>0.442457</td>\n",
       "      <td>0.694639</td>\n",
       "      <td>0.735275</td>\n",
       "      <td>-0.370215</td>\n",
       "      <td>00:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.465736</td>\n",
       "      <td>0.470944</td>\n",
       "      <td>0.466538</td>\n",
       "      <td>1.572433</td>\n",
       "      <td>-0.041230</td>\n",
       "      <td>0.769283</td>\n",
       "      <td>-0.044008</td>\n",
       "      <td>0.444005</td>\n",
       "      <td>0.694550</td>\n",
       "      <td>0.731519</td>\n",
       "      <td>-0.394245</td>\n",
       "      <td>00:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>0.447810</td>\n",
       "      <td>0.447873</td>\n",
       "      <td>0.443458</td>\n",
       "      <td>1.548603</td>\n",
       "      <td>-0.053227</td>\n",
       "      <td>0.708710</td>\n",
       "      <td>-0.074742</td>\n",
       "      <td>0.427311</td>\n",
       "      <td>0.680980</td>\n",
       "      <td>0.722539</td>\n",
       "      <td>-0.412512</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>0.429429</td>\n",
       "      <td>0.429261</td>\n",
       "      <td>0.425588</td>\n",
       "      <td>1.516164</td>\n",
       "      <td>-0.074871</td>\n",
       "      <td>0.672920</td>\n",
       "      <td>-0.109303</td>\n",
       "      <td>0.415940</td>\n",
       "      <td>0.668408</td>\n",
       "      <td>0.722142</td>\n",
       "      <td>-0.406692</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>0.410645</td>\n",
       "      <td>0.412892</td>\n",
       "      <td>0.409110</td>\n",
       "      <td>1.485013</td>\n",
       "      <td>-0.084745</td>\n",
       "      <td>0.643065</td>\n",
       "      <td>-0.124690</td>\n",
       "      <td>0.409055</td>\n",
       "      <td>0.669671</td>\n",
       "      <td>0.713404</td>\n",
       "      <td>-0.437892</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.404449</td>\n",
       "      <td>0.412894</td>\n",
       "      <td>0.409436</td>\n",
       "      <td>1.476325</td>\n",
       "      <td>-0.079059</td>\n",
       "      <td>0.655950</td>\n",
       "      <td>-0.133262</td>\n",
       "      <td>0.417102</td>\n",
       "      <td>0.666478</td>\n",
       "      <td>0.713688</td>\n",
       "      <td>-0.441736</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.389065</td>\n",
       "      <td>0.394628</td>\n",
       "      <td>0.390000</td>\n",
       "      <td>1.462521</td>\n",
       "      <td>-0.109746</td>\n",
       "      <td>0.597826</td>\n",
       "      <td>-0.146258</td>\n",
       "      <td>0.393219</td>\n",
       "      <td>0.653931</td>\n",
       "      <td>0.708326</td>\n",
       "      <td>-0.439823</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>0.368214</td>\n",
       "      <td>0.386160</td>\n",
       "      <td>0.381662</td>\n",
       "      <td>1.452831</td>\n",
       "      <td>-0.121989</td>\n",
       "      <td>0.588956</td>\n",
       "      <td>-0.158161</td>\n",
       "      <td>0.387809</td>\n",
       "      <td>0.649502</td>\n",
       "      <td>0.699512</td>\n",
       "      <td>-0.445165</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>0.349125</td>\n",
       "      <td>0.383791</td>\n",
       "      <td>0.379485</td>\n",
       "      <td>1.449990</td>\n",
       "      <td>-0.127521</td>\n",
       "      <td>0.598858</td>\n",
       "      <td>-0.161175</td>\n",
       "      <td>0.379474</td>\n",
       "      <td>0.645635</td>\n",
       "      <td>0.701557</td>\n",
       "      <td>-0.450942</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>0.344205</td>\n",
       "      <td>0.379494</td>\n",
       "      <td>0.375407</td>\n",
       "      <td>1.433734</td>\n",
       "      <td>-0.104670</td>\n",
       "      <td>0.599808</td>\n",
       "      <td>-0.184458</td>\n",
       "      <td>0.381494</td>\n",
       "      <td>0.641320</td>\n",
       "      <td>0.698571</td>\n",
       "      <td>-0.462541</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>0.326772</td>\n",
       "      <td>0.366981</td>\n",
       "      <td>0.362234</td>\n",
       "      <td>1.441852</td>\n",
       "      <td>-0.124395</td>\n",
       "      <td>0.544368</td>\n",
       "      <td>-0.190324</td>\n",
       "      <td>0.376924</td>\n",
       "      <td>0.642149</td>\n",
       "      <td>0.695693</td>\n",
       "      <td>-0.488397</td>\n",
       "      <td>00:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.312570</td>\n",
       "      <td>0.347947</td>\n",
       "      <td>0.343127</td>\n",
       "      <td>1.408690</td>\n",
       "      <td>-0.168052</td>\n",
       "      <td>0.527579</td>\n",
       "      <td>-0.208268</td>\n",
       "      <td>0.362383</td>\n",
       "      <td>0.628509</td>\n",
       "      <td>0.690789</td>\n",
       "      <td>-0.496611</td>\n",
       "      <td>00:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>0.299730</td>\n",
       "      <td>0.346080</td>\n",
       "      <td>0.341812</td>\n",
       "      <td>1.404641</td>\n",
       "      <td>-0.158018</td>\n",
       "      <td>0.505703</td>\n",
       "      <td>-0.211138</td>\n",
       "      <td>0.368830</td>\n",
       "      <td>0.630378</td>\n",
       "      <td>0.684774</td>\n",
       "      <td>-0.490676</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>0.278936</td>\n",
       "      <td>0.328427</td>\n",
       "      <td>0.323184</td>\n",
       "      <td>1.389887</td>\n",
       "      <td>-0.186112</td>\n",
       "      <td>0.475131</td>\n",
       "      <td>-0.234863</td>\n",
       "      <td>0.349867</td>\n",
       "      <td>0.619424</td>\n",
       "      <td>0.678456</td>\n",
       "      <td>-0.506319</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>0.266708</td>\n",
       "      <td>0.326161</td>\n",
       "      <td>0.320370</td>\n",
       "      <td>1.373820</td>\n",
       "      <td>-0.184507</td>\n",
       "      <td>0.475990</td>\n",
       "      <td>-0.236727</td>\n",
       "      <td>0.348898</td>\n",
       "      <td>0.617483</td>\n",
       "      <td>0.678308</td>\n",
       "      <td>-0.510308</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>0.251395</td>\n",
       "      <td>0.318630</td>\n",
       "      <td>0.313309</td>\n",
       "      <td>1.362268</td>\n",
       "      <td>-0.205638</td>\n",
       "      <td>0.457556</td>\n",
       "      <td>-0.238294</td>\n",
       "      <td>0.337685</td>\n",
       "      <td>0.613173</td>\n",
       "      <td>0.677931</td>\n",
       "      <td>-0.498213</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.231848</td>\n",
       "      <td>0.301533</td>\n",
       "      <td>0.295766</td>\n",
       "      <td>1.352783</td>\n",
       "      <td>-0.216332</td>\n",
       "      <td>0.426007</td>\n",
       "      <td>-0.265255</td>\n",
       "      <td>0.330352</td>\n",
       "      <td>0.598345</td>\n",
       "      <td>0.673051</td>\n",
       "      <td>-0.532822</td>\n",
       "      <td>00:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>0.216864</td>\n",
       "      <td>0.301268</td>\n",
       "      <td>0.295634</td>\n",
       "      <td>1.349958</td>\n",
       "      <td>-0.219465</td>\n",
       "      <td>0.439988</td>\n",
       "      <td>-0.272705</td>\n",
       "      <td>0.326510</td>\n",
       "      <td>0.595352</td>\n",
       "      <td>0.669202</td>\n",
       "      <td>-0.523768</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>0.203260</td>\n",
       "      <td>0.289624</td>\n",
       "      <td>0.283735</td>\n",
       "      <td>1.330982</td>\n",
       "      <td>-0.233808</td>\n",
       "      <td>0.417276</td>\n",
       "      <td>-0.285158</td>\n",
       "      <td>0.321704</td>\n",
       "      <td>0.595618</td>\n",
       "      <td>0.668554</td>\n",
       "      <td>-0.545286</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>0.191922</td>\n",
       "      <td>0.283510</td>\n",
       "      <td>0.278347</td>\n",
       "      <td>1.327657</td>\n",
       "      <td>-0.236438</td>\n",
       "      <td>0.412246</td>\n",
       "      <td>-0.295947</td>\n",
       "      <td>0.314124</td>\n",
       "      <td>0.589033</td>\n",
       "      <td>0.666131</td>\n",
       "      <td>-0.550028</td>\n",
       "      <td>00:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>0.175341</td>\n",
       "      <td>0.276412</td>\n",
       "      <td>0.270557</td>\n",
       "      <td>1.322438</td>\n",
       "      <td>-0.249005</td>\n",
       "      <td>0.404334</td>\n",
       "      <td>-0.312906</td>\n",
       "      <td>0.310435</td>\n",
       "      <td>0.581903</td>\n",
       "      <td>0.660975</td>\n",
       "      <td>-0.553717</td>\n",
       "      <td>00:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.163789</td>\n",
       "      <td>0.268131</td>\n",
       "      <td>0.262168</td>\n",
       "      <td>1.306388</td>\n",
       "      <td>-0.253900</td>\n",
       "      <td>0.372751</td>\n",
       "      <td>-0.322014</td>\n",
       "      <td>0.307929</td>\n",
       "      <td>0.579651</td>\n",
       "      <td>0.658043</td>\n",
       "      <td>-0.551507</td>\n",
       "      <td>00:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>0.145273</td>\n",
       "      <td>0.263101</td>\n",
       "      <td>0.256958</td>\n",
       "      <td>1.308482</td>\n",
       "      <td>-0.261907</td>\n",
       "      <td>0.373626</td>\n",
       "      <td>-0.323901</td>\n",
       "      <td>0.306024</td>\n",
       "      <td>0.572447</td>\n",
       "      <td>0.654328</td>\n",
       "      <td>-0.573440</td>\n",
       "      <td>00:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>0.131835</td>\n",
       "      <td>0.248017</td>\n",
       "      <td>0.241734</td>\n",
       "      <td>1.291164</td>\n",
       "      <td>-0.272984</td>\n",
       "      <td>0.343705</td>\n",
       "      <td>-0.340577</td>\n",
       "      <td>0.292037</td>\n",
       "      <td>0.561483</td>\n",
       "      <td>0.649230</td>\n",
       "      <td>-0.590187</td>\n",
       "      <td>00:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>0.119875</td>\n",
       "      <td>0.246645</td>\n",
       "      <td>0.240520</td>\n",
       "      <td>1.282651</td>\n",
       "      <td>-0.281223</td>\n",
       "      <td>0.347813</td>\n",
       "      <td>-0.345724</td>\n",
       "      <td>0.294152</td>\n",
       "      <td>0.557744</td>\n",
       "      <td>0.646575</td>\n",
       "      <td>-0.577825</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>0.105989</td>\n",
       "      <td>0.242415</td>\n",
       "      <td>0.236442</td>\n",
       "      <td>1.283694</td>\n",
       "      <td>-0.284443</td>\n",
       "      <td>0.331100</td>\n",
       "      <td>-0.356796</td>\n",
       "      <td>0.293228</td>\n",
       "      <td>0.552421</td>\n",
       "      <td>0.645821</td>\n",
       "      <td>-0.573487</td>\n",
       "      <td>00:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>0.092360</td>\n",
       "      <td>0.234495</td>\n",
       "      <td>0.228677</td>\n",
       "      <td>1.272725</td>\n",
       "      <td>-0.295942</td>\n",
       "      <td>0.335082</td>\n",
       "      <td>-0.363440</td>\n",
       "      <td>0.286034</td>\n",
       "      <td>0.547280</td>\n",
       "      <td>0.641447</td>\n",
       "      <td>-0.593772</td>\n",
       "      <td>00:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>0.083258</td>\n",
       "      <td>0.231490</td>\n",
       "      <td>0.225301</td>\n",
       "      <td>1.265883</td>\n",
       "      <td>-0.299378</td>\n",
       "      <td>0.328885</td>\n",
       "      <td>-0.365630</td>\n",
       "      <td>0.284645</td>\n",
       "      <td>0.541166</td>\n",
       "      <td>0.641496</td>\n",
       "      <td>-0.594659</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>0.072029</td>\n",
       "      <td>0.226724</td>\n",
       "      <td>0.220382</td>\n",
       "      <td>1.265616</td>\n",
       "      <td>-0.300181</td>\n",
       "      <td>0.307033</td>\n",
       "      <td>-0.373413</td>\n",
       "      <td>0.285693</td>\n",
       "      <td>0.540041</td>\n",
       "      <td>0.636686</td>\n",
       "      <td>-0.598423</td>\n",
       "      <td>00:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>0.060785</td>\n",
       "      <td>0.223663</td>\n",
       "      <td>0.216966</td>\n",
       "      <td>1.266454</td>\n",
       "      <td>-0.304249</td>\n",
       "      <td>0.298790</td>\n",
       "      <td>-0.376616</td>\n",
       "      <td>0.281674</td>\n",
       "      <td>0.533349</td>\n",
       "      <td>0.636159</td>\n",
       "      <td>-0.599831</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>0.053308</td>\n",
       "      <td>0.221368</td>\n",
       "      <td>0.214555</td>\n",
       "      <td>1.261496</td>\n",
       "      <td>-0.306530</td>\n",
       "      <td>0.298957</td>\n",
       "      <td>-0.380587</td>\n",
       "      <td>0.280558</td>\n",
       "      <td>0.532663</td>\n",
       "      <td>0.635648</td>\n",
       "      <td>-0.605768</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.043992</td>\n",
       "      <td>0.217881</td>\n",
       "      <td>0.210825</td>\n",
       "      <td>1.255895</td>\n",
       "      <td>-0.310783</td>\n",
       "      <td>0.295624</td>\n",
       "      <td>-0.385971</td>\n",
       "      <td>0.279411</td>\n",
       "      <td>0.526982</td>\n",
       "      <td>0.634387</td>\n",
       "      <td>-0.608944</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>0.037121</td>\n",
       "      <td>0.214425</td>\n",
       "      <td>0.207623</td>\n",
       "      <td>1.252612</td>\n",
       "      <td>-0.316141</td>\n",
       "      <td>0.288690</td>\n",
       "      <td>-0.390410</td>\n",
       "      <td>0.277710</td>\n",
       "      <td>0.523935</td>\n",
       "      <td>0.633658</td>\n",
       "      <td>-0.609071</td>\n",
       "      <td>00:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>0.027827</td>\n",
       "      <td>0.212844</td>\n",
       "      <td>0.205739</td>\n",
       "      <td>1.253767</td>\n",
       "      <td>-0.317714</td>\n",
       "      <td>0.285797</td>\n",
       "      <td>-0.390786</td>\n",
       "      <td>0.277618</td>\n",
       "      <td>0.522843</td>\n",
       "      <td>0.632700</td>\n",
       "      <td>-0.618316</td>\n",
       "      <td>00:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>0.023436</td>\n",
       "      <td>0.212755</td>\n",
       "      <td>0.205861</td>\n",
       "      <td>1.252958</td>\n",
       "      <td>-0.318844</td>\n",
       "      <td>0.286281</td>\n",
       "      <td>-0.392951</td>\n",
       "      <td>0.275772</td>\n",
       "      <td>0.521497</td>\n",
       "      <td>0.632794</td>\n",
       "      <td>-0.610621</td>\n",
       "      <td>00:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>0.017402</td>\n",
       "      <td>0.210002</td>\n",
       "      <td>0.203079</td>\n",
       "      <td>1.249531</td>\n",
       "      <td>-0.322527</td>\n",
       "      <td>0.282689</td>\n",
       "      <td>-0.395241</td>\n",
       "      <td>0.275014</td>\n",
       "      <td>0.519019</td>\n",
       "      <td>0.631883</td>\n",
       "      <td>-0.615738</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>0.012591</td>\n",
       "      <td>0.208221</td>\n",
       "      <td>0.201328</td>\n",
       "      <td>1.249026</td>\n",
       "      <td>-0.324140</td>\n",
       "      <td>0.278209</td>\n",
       "      <td>-0.398635</td>\n",
       "      <td>0.274195</td>\n",
       "      <td>0.518554</td>\n",
       "      <td>0.631495</td>\n",
       "      <td>-0.618077</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.013623</td>\n",
       "      <td>0.208700</td>\n",
       "      <td>0.201732</td>\n",
       "      <td>1.248622</td>\n",
       "      <td>-0.323531</td>\n",
       "      <td>0.278167</td>\n",
       "      <td>-0.397845</td>\n",
       "      <td>0.274501</td>\n",
       "      <td>0.518272</td>\n",
       "      <td>0.631479</td>\n",
       "      <td>-0.615810</td>\n",
       "      <td>00:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>0.010723</td>\n",
       "      <td>0.207425</td>\n",
       "      <td>0.200492</td>\n",
       "      <td>1.247827</td>\n",
       "      <td>-0.324854</td>\n",
       "      <td>0.275162</td>\n",
       "      <td>-0.398970</td>\n",
       "      <td>0.273494</td>\n",
       "      <td>0.517258</td>\n",
       "      <td>0.631036</td>\n",
       "      <td>-0.617019</td>\n",
       "      <td>00:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>0.010688</td>\n",
       "      <td>0.207846</td>\n",
       "      <td>0.200908</td>\n",
       "      <td>1.247942</td>\n",
       "      <td>-0.324318</td>\n",
       "      <td>0.276437</td>\n",
       "      <td>-0.398864</td>\n",
       "      <td>0.274143</td>\n",
       "      <td>0.517590</td>\n",
       "      <td>0.631105</td>\n",
       "      <td>-0.616774</td>\n",
       "      <td>00:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>0.008440</td>\n",
       "      <td>0.207835</td>\n",
       "      <td>0.200898</td>\n",
       "      <td>1.247971</td>\n",
       "      <td>-0.324324</td>\n",
       "      <td>0.276356</td>\n",
       "      <td>-0.398911</td>\n",
       "      <td>0.274180</td>\n",
       "      <td>0.517571</td>\n",
       "      <td>0.631114</td>\n",
       "      <td>-0.616774</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 0 with 👉🏻LMAE👈🏻 value: 1.903274416923523.\n",
      "Better model found at epoch 1 with 👉🏻LMAE👈🏻 value: 1.8250993490219116.\n",
      "Better model found at epoch 2 with 👉🏻LMAE👈🏻 value: 1.7578157186508179.\n",
      "Better model found at epoch 3 with 👉🏻LMAE👈🏻 value: 1.7136390209197998.\n",
      "Better model found at epoch 4 with 👉🏻LMAE👈🏻 value: 1.6700730323791504.\n",
      "Better model found at epoch 5 with 👉🏻LMAE👈🏻 value: 1.649054765701294.\n",
      "Better model found at epoch 6 with 👉🏻LMAE👈🏻 value: 1.6219472885131836.\n",
      "Better model found at epoch 7 with 👉🏻LMAE👈🏻 value: 1.5982922315597534.\n",
      "Better model found at epoch 8 with 👉🏻LMAE👈🏻 value: 1.581796407699585.\n",
      "Better model found at epoch 9 with 👉🏻LMAE👈🏻 value: 1.5421366691589355.\n",
      "Better model found at epoch 10 with 👉🏻LMAE👈🏻 value: 1.5193030834197998.\n",
      "Better model found at epoch 11 with 👉🏻LMAE👈🏻 value: 1.4886810779571533.\n",
      "Better model found at epoch 12 with 👉🏻LMAE👈🏻 value: 1.466329574584961.\n",
      "Better model found at epoch 13 with 👉🏻LMAE👈🏻 value: 1.455521583557129.\n",
      "Better model found at epoch 14 with 👉🏻LMAE👈🏻 value: 1.4265658855438232.\n",
      "Better model found at epoch 15 with 👉🏻LMAE👈🏻 value: 1.4089491367340088.\n",
      "Better model found at epoch 16 with 👉🏻LMAE👈🏻 value: 1.38678777217865.\n",
      "Better model found at epoch 17 with 👉🏻LMAE👈🏻 value: 1.351628303527832.\n",
      "Better model found at epoch 18 with 👉🏻LMAE👈🏻 value: 1.3281315565109253.\n",
      "Better model found at epoch 19 with 👉🏻LMAE👈🏻 value: 1.2936540842056274.\n",
      "Better model found at epoch 20 with 👉🏻LMAE👈🏻 value: 1.254080057144165.\n",
      "Better model found at epoch 21 with 👉🏻LMAE👈🏻 value: 1.2179242372512817.\n",
      "Better model found at epoch 22 with 👉🏻LMAE👈🏻 value: 1.1893916130065918.\n",
      "Better model found at epoch 23 with 👉🏻LMAE👈🏻 value: 1.1787796020507812.\n",
      "Better model found at epoch 24 with 👉🏻LMAE👈🏻 value: 1.1376399993896484.\n",
      "Better model found at epoch 25 with 👉🏻LMAE👈🏻 value: 1.1369946002960205.\n",
      "Better model found at epoch 26 with 👉🏻LMAE👈🏻 value: 1.1071158647537231.\n",
      "Better model found at epoch 27 with 👉🏻LMAE👈🏻 value: 1.0885584354400635.\n",
      "Better model found at epoch 28 with 👉🏻LMAE👈🏻 value: 1.053225040435791.\n",
      "Better model found at epoch 29 with 👉🏻LMAE👈🏻 value: 1.0382471084594727.\n",
      "Better model found at epoch 30 with 👉🏻LMAE👈🏻 value: 1.0257765054702759.\n",
      "Better model found at epoch 31 with 👉🏻LMAE👈🏻 value: 1.0070805549621582.\n",
      "Better model found at epoch 32 with 👉🏻LMAE👈🏻 value: 0.9676285982131958.\n",
      "Better model found at epoch 33 with 👉🏻LMAE👈🏻 value: 0.9335732460021973.\n",
      "Better model found at epoch 34 with 👉🏻LMAE👈🏻 value: 0.874077320098877.\n",
      "Better model found at epoch 35 with 👉🏻LMAE👈🏻 value: 0.8483839631080627.\n",
      "Better model found at epoch 36 with 👉🏻LMAE👈🏻 value: 0.8103531002998352.\n",
      "Better model found at epoch 37 with 👉🏻LMAE👈🏻 value: 0.7733722925186157.\n",
      "Better model found at epoch 38 with 👉🏻LMAE👈🏻 value: 0.7499680519104004.\n",
      "Better model found at epoch 39 with 👉🏻LMAE👈🏻 value: 0.7423382997512817.\n",
      "Better model found at epoch 40 with 👉🏻LMAE👈🏻 value: 0.7205098867416382.\n",
      "Better model found at epoch 41 with 👉🏻LMAE👈🏻 value: 0.7021183967590332.\n",
      "Better model found at epoch 42 with 👉🏻LMAE👈🏻 value: 0.6969796419143677.\n",
      "Better model found at epoch 43 with 👉🏻LMAE👈🏻 value: 0.6549316644668579.\n",
      "Better model found at epoch 44 with 👉🏻LMAE👈🏻 value: 0.652594268321991.\n",
      "Better model found at epoch 46 with 👉🏻LMAE👈🏻 value: 0.6311463713645935.\n",
      "Better model found at epoch 47 with 👉🏻LMAE👈🏻 value: 0.598021388053894.\n",
      "Better model found at epoch 49 with 👉🏻LMAE👈🏻 value: 0.5843838453292847.\n",
      "Better model found at epoch 50 with 👉🏻LMAE👈🏻 value: 0.5763764977455139.\n",
      "Better model found at epoch 51 with 👉🏻LMAE👈🏻 value: 0.5571115612983704.\n",
      "Better model found at epoch 52 with 👉🏻LMAE👈🏻 value: 0.5553778409957886.\n",
      "Better model found at epoch 53 with 👉🏻LMAE👈🏻 value: 0.5433747172355652.\n",
      "Better model found at epoch 54 with 👉🏻LMAE👈🏻 value: 0.5090140104293823.\n",
      "Better model found at epoch 55 with 👉🏻LMAE👈🏻 value: 0.5084932446479797.\n",
      "Better model found at epoch 56 with 👉🏻LMAE👈🏻 value: 0.5015386343002319.\n",
      "Better model found at epoch 57 with 👉🏻LMAE👈🏻 value: 0.4838036894798279.\n",
      "Better model found at epoch 59 with 👉🏻LMAE👈🏻 value: 0.47371333837509155.\n",
      "Better model found at epoch 60 with 👉🏻LMAE👈🏻 value: 0.4665382504463196.\n",
      "Better model found at epoch 61 with 👉🏻LMAE👈🏻 value: 0.44345754384994507.\n",
      "Better model found at epoch 62 with 👉🏻LMAE👈🏻 value: 0.4255883991718292.\n",
      "Better model found at epoch 63 with 👉🏻LMAE👈🏻 value: 0.40911027789115906.\n",
      "Better model found at epoch 65 with 👉🏻LMAE👈🏻 value: 0.3899995684623718.\n",
      "Better model found at epoch 66 with 👉🏻LMAE👈🏻 value: 0.3816617429256439.\n",
      "Better model found at epoch 67 with 👉🏻LMAE👈🏻 value: 0.37948453426361084.\n",
      "Better model found at epoch 68 with 👉🏻LMAE👈🏻 value: 0.3754074275493622.\n",
      "Better model found at epoch 69 with 👉🏻LMAE👈🏻 value: 0.3622336685657501.\n",
      "Better model found at epoch 70 with 👉🏻LMAE👈🏻 value: 0.3431273102760315.\n",
      "Better model found at epoch 71 with 👉🏻LMAE👈🏻 value: 0.3418118357658386.\n",
      "Better model found at epoch 72 with 👉🏻LMAE👈🏻 value: 0.3231838345527649.\n",
      "Better model found at epoch 73 with 👉🏻LMAE👈🏻 value: 0.3203697204589844.\n",
      "Better model found at epoch 74 with 👉🏻LMAE👈🏻 value: 0.31330856680870056.\n",
      "Better model found at epoch 75 with 👉🏻LMAE👈🏻 value: 0.29576602578163147.\n",
      "Better model found at epoch 76 with 👉🏻LMAE👈🏻 value: 0.2956339716911316.\n",
      "Better model found at epoch 77 with 👉🏻LMAE👈🏻 value: 0.2837351858615875.\n",
      "Better model found at epoch 78 with 👉🏻LMAE👈🏻 value: 0.2783472537994385.\n",
      "Better model found at epoch 79 with 👉🏻LMAE👈🏻 value: 0.270557165145874.\n",
      "Better model found at epoch 80 with 👉🏻LMAE👈🏻 value: 0.2621678113937378.\n",
      "Better model found at epoch 81 with 👉🏻LMAE👈🏻 value: 0.2569575309753418.\n",
      "Better model found at epoch 82 with 👉🏻LMAE👈🏻 value: 0.24173396825790405.\n",
      "Better model found at epoch 83 with 👉🏻LMAE👈🏻 value: 0.24052023887634277.\n",
      "Better model found at epoch 84 with 👉🏻LMAE👈🏻 value: 0.23644235730171204.\n",
      "Better model found at epoch 85 with 👉🏻LMAE👈🏻 value: 0.22867685556411743.\n",
      "Better model found at epoch 86 with 👉🏻LMAE👈🏻 value: 0.2253008782863617.\n",
      "Better model found at epoch 87 with 👉🏻LMAE👈🏻 value: 0.22038155794143677.\n",
      "Better model found at epoch 88 with 👉🏻LMAE👈🏻 value: 0.21696636080741882.\n",
      "Better model found at epoch 89 with 👉🏻LMAE👈🏻 value: 0.21455460786819458.\n",
      "Better model found at epoch 90 with 👉🏻LMAE👈🏻 value: 0.21082504093647003.\n",
      "Better model found at epoch 91 with 👉🏻LMAE👈🏻 value: 0.20762264728546143.\n",
      "Better model found at epoch 92 with 👉🏻LMAE👈🏻 value: 0.20573869347572327.\n",
      "Better model found at epoch 94 with 👉🏻LMAE👈🏻 value: 0.2030787169933319.\n",
      "Better model found at epoch 95 with 👉🏻LMAE👈🏻 value: 0.20132839679718018.\n",
      "Better model found at epoch 97 with 👉🏻LMAE👈🏻 value: 0.20049187541007996.\n"
     ]
    }
   ],
   "source": [
    "learner.fit_one_cycle(100,3e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEGCAYAAAB/+QKOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAgAElEQVR4nO3deVhV1frA8e/LJA4IAmoOIGrmGCqiYg45T3XVHHIsTctf2nDvtcnmbt1u3m7ZYIOZqQ2mjZplpjaY5oymOA85Iqg4giIosH5/7C0e5YCoHM4B38/znIez11577/dI8Z61115riTEGpZRS6lJe7g5AKaWUZ9IEoZRSyilNEEoppZzSBKGUUsopTRBKKaWc8nF3AAUpNDTUREREuDsMpZQqMtasWXPEGFPe2b5ilSAiIiKIjY11dxhKKVVkiMje3PbpLSallFJOaYJQSinllCYIpZRSThWrPgilVPFw7tw54uPjSUtLc3coxYa/vz9Vq1bF19c338doglBKeZz4+HgCAgKIiIhARNwdTpFnjOHo0aPEx8dTvXr1fB+nt5iUUh4nLS2NkJAQTQ4FREQICQm54haZJgillEfS5FCwrubfUxME8PYvO/h9e5K7w1BKKY/isgQhImEi8puIbBGRTSLydyd1RETeFpGdIhInIlEO+4aKyA77NdRVcQJM/P0vlmiCUErZjh49SqNGjWjUqBE33HADVapUyd4+e/Zsvs5xzz33sG3bNhdH6lqu7KTOAB4xxqwVkQBgjYgsNMZsdqjTDahlv5oD7wPNRSQYeB6IBox97BxjzHFXBOrn48XZzCxXnFopVQSFhISwbt06AF544QXKlCnDo48+elEdYwzGGLy8nH/Pnjp1qsvjdDWXtSCMMYnGmLX2+xRgC1Dlkmo9gU+MZQUQJCKVgC7AQmPMMTspLAS6uipWP28vzmZoglBK5W3nzp00aNCA+++/n6ioKBITExk5ciTR0dHUr1+fF198Mbtuq1atWLduHRkZGQQFBTF27FgaNmxIixYtOHz4sBs/Rf4VymOuIhIBNAZWXrKrCrDfYTveLsut3Nm5RwIjAcLDw68qPj8fTRBKeap/fb+JzQnJBXrOepXL8vzf6l/VsZs3b2bq1KlMnDgRgHHjxhEcHExGRgbt2rWjb9++1KtX76JjTp48ya233sq4ceMYM2YMU6ZMYezYsdf8OVzN5Z3UIlIG+Ab4hzHm0t+ys251k0d5zkJjJhljoo0x0eXLO52Q8LL8fLxI11tMSql8qFmzJk2bNs3enjFjBlFRUURFRbFlyxY2b96c45iSJUvSrVs3AJo0acKePXsKK9xr4tIWhIj4YiWH6caYb51UiQfCHLarAgl2edtLyhe5Jkq9xaSUJ7vab/quUrp06ez3O3bs4K233mLVqlUEBQUxZMgQp2MN/Pz8st97e3uTkZFRKLFeK1c+xSTAR8AWY8z4XKrNAe62n2aKAU4aYxKB+UBnESknIuWAznaZS5T19+Vk6jlXnV4pVUwlJycTEBBA2bJlSUxMZP58l/2ZcgtXtiBaAncBG0RknV32FBAOYIyZCPwIdAd2AqnAPfa+YyLyErDaPu5FY8wxVwVao3xp5m5IJO1cJv6+3q66jFKqmImKiqJevXo0aNCAGjVq0LJlS3eHVKDEGKe39ouk6OhoczULBi376wiDPlzJI51u4qEOtVwQmVLqSmzZsoW6deu6O4xix9m/q4isMcZEO6uvI6mBFjVC6Fi3Iq8v3M7jX6/nVHrRuD+olFKupAkCa46SD+5qwsg2NfhqTTwDJi0n7Vymu8NSSim30gRh8/YSnupel/cHR7HxQDJv/7LD3SEppZRbaYK4RNfqfoyI9OeDxbvYerBgB+copVRRognC0bk0mNCYx0t+R1l/H56dvZGsrOLTia+UUldCE4QjX3+ofRsltnzD812qsXrPcaYu2+PuqJRSyi00QVwq+h44e4qe3svpUKcCr/60lV1Jp9wdlVKqELVt2zbHoLc333yT0aNH53pMmTJlAEhISKBv3765nvdyj+K/+eabpKamZm93796dEydO5Df0AqUJ4lJVm0KFesiaqbzS+2b8fb0ZPX0tKWk60lqp68XAgQOZOXPmRWUzZ85k4MCBlz22cuXKfP3111d97UsTxI8//khQUNBVn+9aaIK4lAg0uQcS11Hh1BbeGdSYnYdP8ciX6ylOgwqVUrnr27cvP/zwA+np6QDs2bOHhIQEGjVqRIcOHYiKiuLmm2/mu+++y3Hsnj17aNCgAQBnzpxhwIABREZG0r9/f86cOZNdb9SoUdnThD///PMAvP322yQkJNCuXTvatWsHQEREBEeOHAFg/PjxNGjQgAYNGvDmm29mX69u3brcd9991K9fn86dO190nWtRKNN9FzmRd8LC5yB2Kq17vM3jXWvznx+38s3aA/RtUtXd0Sl1fZk3Fg5uKNhz3nAzdBuX6+6QkBCaNWvGTz/9RM+ePZk5cyb9+/enZMmSzJo1i7Jly3LkyBFiYmLo0aNHrus9v//++5QqVYq4uDji4uKIispeNJOXX36Z4OBgMjMz6dChA3FxcTz88MOMHz+e3377jdDQ0IvOtWbNGqZOncrKlSsxxtC8eXNuvfVWypUrx44dO5gxYwYffvghd955J9988w1Dhgy55n8mbUE4UzIIGvSBDV9DegojWtWgWUQwL8zZxI5DKe6OTilVCBxvM52/vWSM4amnniIyMpKOHTty4MABDh06lOs5Fi9enP2HOjIyksjIyOx9X375JVFRUTRu3JhNmzY5nSbc0R9//MEdd9xB6dKlKVOmDL1792bJkiUAVK9enUaNGgEFO524tiByE30PrPsMNnyFd/Rw3hjQiB4T/uCJb+L4+v5b8PJy/o1BKVXA8vim70q9evVizJgxrF27ljNnzhAVFcW0adNISkpizZo1+Pr6EhER4XR6b0fOWhe7d+/mtddeY/Xq1ZQrV45hw4Zd9jx53eIuUaJE9ntvb+8Cu8WkLYjcVGkCN0TCivchK4sqQSV5qntd1u47wczV+y9/vFKqSCtTpgxt27Zl+PDh2Z3TJ0+epEKFCvj6+vLbb7+xd+/ePM/Rpk0bpk+fDsDGjRuJi4sDrGnCS5cuTWBgIIcOHWLevHnZxwQEBJCSkvNORZs2bZg9ezapqamcPn2aWbNm0bp164L6uE5pgsiNCLT8OxzZDtvmAtA7qgotaoTw1KwNrNrtstnHlVIeYuDAgaxfv54BAwYAMHjwYGJjY4mOjmb69OnUqVMnz+NHjRrFqVOniIyM5NVXX6VZs2YANGzYkMaNG1O/fn2GDx9+0TThI0eOpFu3btmd1OdFRUUxbNgwmjVrRvPmzbn33ntp3LhxAX/ii+l033nJzIB3oqFkObjvVxAhKSWdpi//TKVAf+b9vTVBpfwufx6l1BXR6b5dw2Om+xaRKSJyWEQ25rL/MRFZZ782ikimiATb+/aIyAZ7XwH+xb9C3j5WKyJhLez+HYDyASX4/sFWHDmVzgOfryVD17JWShVTrrzFNA3omttOY8z/jDGNjDGNgCeB3y9ZNa6dvd9pZis0DQdCmYqw5MKqqTdXDeTZ2+uxdOdRXvxhs46PUEoVSy5LEMaYxUB+b9QPBGa4KpZr4usPLR6wWhAH1mQX3xVTjXtaRvDJ8r2aJJRyAf1/qmBdzb+n2zupRaQUVkvjG4diAywQkTUiMtI9kTmIHg7+gfDzvyDLuqUkIjx3ez3uaRnB1KV7eONnXT9CqYLi7+/P0aNHNUkUEGMMR48exd/f/4qO84RxEH8Dll5ye6mlMSZBRCoAC0Vkq90iycFOICMBwsPDXRNhiQDo+AL88E9Y+b7VouBCkkhNz+TtX3aQlWV4pPNNuY6qVErlT9WqVYmPjycpKcndoRQb/v7+VK16ZTNBeEKCGMAlt5eMMQn2z8MiMgtoBjhNEMaYScAksJ5iclmUTe6BHT/Dzy9ARGuoZI2IFBH+0/tmMrIM7/y2k/jjqbzWryE+3m5vnClVZPn6+lK9enV3h3Hdc+tfMREJBG4FvnMoKy0iAeffA50Bp09CFSoR6DHBeuT1m3vh7IXZFr29hNf6RfJIp5uYvS6BMV+u16eblFJFnisfc50BLAdqi0i8iIwQkftF5H6HancAC4wxpx3KKgJ/iMh6YBUw1xjzk6vivCKlQ+COiXBkGyx45qJdIsJDHWrxeNfazFmfwK3/W6TzNimlijQdKHc15j8Ny9+Bfh9D/V45dv+0MZFnZm8kPSOLD++OJqZGiOtjUkqpq+CWgXLFWofnrbma5jwEx/fk2N21QSW+vv8Wbijrz7Cpq1iyQzvalFJFjyaIq+HjB32nAAJfD4eMszmqRISWZubIGCJCSnPvx7Es23mk8ONUSqlroAniapWLgJ4TrMFzP/wDzuWcXjekTAlm3GcliUGTVzI3LrHw41RKqaukCeJa1OsJrR+FddNhYivYtzJHlXKl/fj03mYElvTlgc/XMnPVPjcEqpRSV04TxLXq8CzcNdu6zTSlCyx8HrIyL6pSIcCfXx+5lVtqhjD22w28vmAbmVnF5+EApVTxpAmiINRsB6OXQdTdsPRNmDkY0k9dVCWkTAmm3dOMfk2qMuHXnYyevoZzOlZCKeXBNEEUlBIB0ONt6P4a7JgPU7vCyQMXVfHz8eLVvpE83b0u8zcdYsTHsZw5m5nLCZVSyr00QRS0ZvfBoK/g2B74oA1snXvRbhHhvjY1+G+fm/ljRxJDPlrJsdM5n4JSSil30wThCrU6wr0/Q9nKMHMQzB4NaScvqtK/aTjvDopi44GT9H5vKVsSk90UrFJKOacJwlUq1IF7f4E2j8H6mfBOU1j+3kVzOHW7uRKf3xdD6tlMer27lLd+tmaEVUopT6AJwpV8/KD9MzBiIYTeBPOfhLciYfXk7CpNqpVj1gMt6VC3Am/8vJ2X5uriQ0opz6AJojBUbQLDfoB7foLydWDuI/DnZ9m7qwSV5N1BUQxvWZ2pS/fQf9IK9h9LzeOESinlepogClO1FtaYieq3WosP7V+VvUtEePb2uoztVodVu49x/2drOJGqnddKKffRBFHYvH2g3zSrA/uLIZCckL1LRLj/1pq81KsBmxKS6TtxOQknck7hoZRShUEThDuUCoaBM+Hsafj8zhwzwt4VU40Z98WQeOIMt0/4g89W7NV+CaVUodME4S4V6lotieN74f2WVp+EQxJoUTOEb0e35MbyZXhm9kb6vL+MpTojrFKqELlyRbkpInJYRJwuFyoibUXkpIiss1/POezrKiLbRGSniIx1VYxuV6sTjFoKlRvDdw9Yt5zOXlhcr/YNAcwcGcOLPeuz92gqgyevZPyCbdqaUEoVCle2IKYBXS9TZ4kxppH9ehFARLyBd4FuQD1goIjUc2Gc7hUUDnfPgU4vwbYf4bO+kHZh0JyXl3B3iwiWPNGOfk2q8vavO3l69kZNEkopl3NZgjDGLAaOXcWhzYCdxphdxpizwEygZ4EG52m8vKDlw9BnMsSvgk96QurF/3Sl/Hz4b59I/q9NDT5fuY/Bk1ey/K+jbgpYKXU9cHcfRAsRWS8i80Skvl1WBdjvUCfeLnNKREaKSKyIxCYlFfGlPRv0gf6fwaGNMO02OLLjot1eXsLYbnV4qnsd/tx3goEfrmDcvK06+lop5RLuTBBrgWrGmIbABGC2XS5O6ub6F9AYM8kYE22MiS5fvrwLwixktbvB4K8gJREmtobVH13UeS0ijGxTk4Vj2hAWXJKJv//Fv77fpFOHK6UKnNsShDEm2Rhzyn7/I+ArIqFYLYYwh6pVgQQnpyi+arSFUcshPAbmjoHpfWHnz5CZkV2larlSLH6sHcNbVufj5Xvp/MZivl4Tr30TSqkC47YEISI3iIjY75vZsRwFVgO1RKS6iPgBA4A57orTbcpWgiHfQtdxsH81fNYHxteF+U9nP+l0fvT15Luj8fUWHv1qPc/M3kiGtiaUUgXAx1UnFpEZQFsgVETigecBXwBjzESgLzBKRDKAM8AAY339zRCRB4H5gDcwxRizyVVxejQvL4gZBU3ugR0LYMNXsOI92L8SBn0JpYIRETrWq0j7OhX47/ytfPD7LrYeTGHCwMZUDirp7k+glCrCpDjdkoiOjjaxsbHuDsO1tnwPX4+A4OpWCyPw4v77L1fv5/Fv4mhYNZDp98VQpoTLvgMopYoBEVljjIl2ts/dTzGpK1X3bzDkG2s50486w46fL9p9Z9MwJg5pwsaEZNq/tojHvlpP2jld1lQpdeU0QRRF1VvDPXPBpwRM7wPT74QjO7N3d21wA5PvjqZSoD9frYmn38Tl2nmtlLpimiCKqkoNYfQKawT23mXwXgwsfg2yrNZCuzoV+O7BVoxoVZ0NB07Sb+JyDienuTlopVRRogmiKPPxs0ZgP7wW6twGv74EU7vBsd3ZVZ7uXpeH299I7N7jNPvPL7y+YBvJaefcGLRSqqjQBFEclKlgzQzb+0M4vBUmtoI9SwFr9PWYzrX5ZHgz6lcuy4Rfd9LznaUcO62LESml8qYJorgQgcg7rdlhy1aGmYMumqqjzU3lmTW6JQ+2u5HdR07ztwl/6LKmSqk8aYIoboLCrDESXj4wvR+cvrCGhJ+PF492qc37g6M4ejqdfhOXc0j7JZRSudAEURwFV7dWrEtJtFoSZy9uKXS7uRIz7ovh5JlzdH9rCTsPn3JToEopT6YJorgKawp3fAD7V1njJY7+ddHuxuHl+GhYNEdPn6X3e0tZs/dqZmZXShVnmiCKs/q9rNtNJ/fDpLaw5YeLdt9SM5QZ98WQnJbBwEkr+XXrIffEqZTySJogirubOsP9SyCkJnwx2Jo+3EGLmiH8+Wwnat8QwN9nrGPPkdO5nEgpdb3RBHE9CAqH4fOhVhf48TFr6nAH5Ur78f6QKLy8hNHT15KeoVNzKKU0QVw/fEpA34+gQl346h5rvISDquVK8Xq/hmxOTGbQhyv16SallCaI60qJAOvpJh9/+PzOix6BBehYryLP3FaXNXuP0+KVX5jyx+5cTqSUuh5ogrjeBIVZSeLUIfhmRPbcTefd27oG8//RhubVQ3j5xy2s23/CTYEqpdzNZQlCRKaIyGER2ZjL/sEiEme/lolIQ4d9e0Rkg4isE5FivsCDG1RtAt3+C7sWwZLxOXbXviGAiXc1oWJACXq9u5TVe47pKnVKXYdc2YKYBnTNY/9u4FZjTCTwEjDpkv3tjDGNclvIQl2jqKFw852w6D+we3GO3YElfXm1r5Wz+01cTu/3l7H9UEphR6mUciOXJQhjzGIg19FXxphlxpjj9uYKoKqrYlFOiMDtb0BwTfjmXjh1OEeVVrVC+Xb0LTzetTZ7j6bS+Y3F1H32J+ZtSHRDwEqpwuYpfRAjgHkO2wZYICJrRGRkXgeKyEgRiRWR2KSkJJcGWeyUKAN3fgxpyTBjAJzNOQYiKrwco9veyG+PtqVj3YqcOZfJqOlrWbJD/62VKu7cniBEpB1WgnjCobilMSYK6AY8ICJtcjveGDPJGBNtjIkuX768i6MthirWtx5/TfgTvh4OmRlOqwWX9mPy0GjWPdeJOjcEMHzaauZtSNSV6pQqxtyaIEQkEpgM9DTGHD1fboxJsH8eBmYBzdwT4XWizm3Q/TXY/hPMHQN5/NEPKuXHzJExhAeXYtT0tbR45Vc2JZwsxGCVUoXFbQlCRMKBb4G7jDHbHcpLi0jA+fdAZ8Dpk1CqADUdAa0fgbUfw4JnLpskvh3VkvtaV+dQShq3vf0HX8XuL8RglVKFwcdVJxaRGUBbIFRE4oHnAV8AY8xE4DkgBHhPRAAy7CeWKgKz7DIf4HNjzE+uilM5aP+s1R+x/B1IPQY9JoC38/9EAkv58vRt9RjWsjotx/3KY1/HkXgyjYc71CrkoJVSriLF6R5ydHS0iY3VYRPXxBj4/VXr8debukKfydYI7DykZ2Ry10erWLX7GHfFVOOxrrUp6+9bSAErpa6FiKzJbTiBJgjl3OqPYO4j4O0L1W6Bmh2g0SAoHeq0+tmMLB7/ej2z1yUA8EC7mnSqdwONwoIKM2ql1BXSBKGuTnwsbJoFO3+BpC1ww80wYiH4lnRa3RjDD3GJPDTjz+yykW1q8FT3uoUVsVLqCmmCUNdu2zxrrETUUOjxdp5VjTEcSk4n5pVfAGhYNZAnutahRc0Q7L4lpZSHyCtBuH0chCoianeDVv+0nnJaPzPPqiLCDYH+rH++MyGl/dh7LJVBk1fS4Pn5xMXr5H9KFRXaglD5l5kBn/SwBtV1HWdN13HuDES0hor1cj3szNlMPlj8F2/+vAOAf/dqwJCYaoUVtVIqD3qLSRWc5ESYdKs1Xfh5vqVg4Ayo0TbPQ/ccOU3b1xZlbz93ez3ualENX29tyCrlLpogVMFKT4GUg1ZndUY6fDEEjv4F/T+Fm7rkeejhlDTunLicPUdTAWhdK5RPhjfTvgml3ET7IFTBKhEAobUgsCqE1IRhc62lTGcOgjXTcixC5KhCgD8/j7mVsd3q0LX+DSzZcYTPVuwtvNiVUvmmCUJdu1LBMHQOhDWH7/8O78XAhq9zTRQ+3l7cf2tN3hscxc1VAnn2u00knjxTyEErpS4nXwlCRGqKSAn7fVsReVhEdASUusA/EIb+AP0+BvG2ljP9pKd1CyoXXl7Cv3s1AKDPe8t4atYGzunKdUp5jPy2IL4BMkXkRuAjoDrwucuiUkWTlxfU7wWjlsFtr8OeJfDdA3lO/NcwLIj3BkeRkpbB5yv3UevpeRw4oa0JpTxBfhNEljEmA7gDeNMY80+gkuvCUkWalxc0vdea/G/DV/Dbf/Ks3v3mSsS90JnejasA8LcJf3DwZFphRKqUykN+E8Q5ERkIDAV+sMt0NjaVt9aPQKMhsPhVWPkBZOV++0hEGN+/Ef/qUZ9jp89y+4Qluga2Um6W3wRxD9ACeNkYs1tEqgOfuS4sVSycX/e6RjuY97jVeR33VZ5POQ29JYIfHmqFiHDvx7EcOZV7H4ZSyrWueByEiJQDwowxca4J6erpOAgPlZUJm2fD7/+zJv0LDIeGA6xXSE2nh6zdd5xBH66gVoUAZo6MoXQJly1dotR17ZrHQYjIIhEpKyLBwHpgqoiML8ggVTHm5Q0N+lid13d+aiWFxf+DCVHwSS9rkN0losLL8e6gKDYcOEn95+ezZu8xNwSu1PUtv7eYAo0xyUBvYKoxpgnQ8XIHicgUETksIk6XDBXL2yKyU0TiRCTKYd9QEdlhv4bmM07lyby8oF4PuHs2/HMTdHgODqyF91vCiok5+ig61K3IhIGN8fUWhkxexZ4jp90UuFLXp/wmCB8RqQTcyYVO6vyYBnTNY383oJb9Ggm8D2C3VJ4HmgPNgOftW1uquAisYnVij14OEa3gpyfgsztyjJv4W8PKLH68HT5ewtOzN1CcpoZRytPlN0G8CMwH/jLGrBaRGsCOyx1kjFkM5HVvoCfwibGsAILsRNQFWGiMOWaMOQ4sJO9Eo4qqwCow+Cu4bTzsWgRzx+QYN1EpsCSPdqnN0p1HWbQtyT1xKnUdyleCMMZ8ZYyJNMaMsrd3GWP6FMD1qwD7Hbbj7bLcynMQkZEiEisisUlJ+sejSBKBpiOgzWPw52ew6sMcVQY1DycipBTj5m0lM0tbEUoVhvx2UlcVkVl2f8IhEflGRKoWwPWdTeFp8ijPWWjMJGNMtDEmunz58gUQknKbtk/BTd3gp7Gwe8lFu3y9vfhnp5vYdiiFBZsOuilApa4v+b3FNBWYA1TG+ib/vV12reKBMIftqkBCHuWqOPPygt6TrKecvrwbDm+9aHe3Btbg/VHT15KhczYp5XL5TRDljTFTjTEZ9msaUBBf1+cAd9tPM8UAJ40xiVj9HZ1FpJzdOd3ZLlPFnX9ZGPQFePvCp73g2O7sXX4+XtzbqjoAAz9cobealHKx/CaIIyIyRES87dcQ4OjlDhKRGcByoLaIxIvICBG5X0Tut6v8COwCdgIfAqMBjDHHgJeA1fbrRbtMXQ+Ca8BdsyEjzZoRNvlC4/GZ2+sxqHk4q/ccp9GLCzh55pwbA1WqeMvXSGoRCQfewZpuwwDLgIeNMftcG96V0ZHUxcyBtfBxDyhbGf5vMfj6Z+8av3A7b/+yg9oVA5j9QEtK+nm7MVCliq5rHkltjNlnjOlhjClvjKlgjOmFNWhOKdepEgX9psKRbbB68kW7xnS6idf7NWTboRS6vbVYbzcp5QLXsqLcmAKLQqnc1OpkTfa35DVIO3nRrj5NqjLslgj2HE1lzJfrdBCdUgXsWhKErjKvCkfHF+DMcVj6do5dz/+tHs0igvluXQIdxv/O2Qx9ukmpgnItCUK/rqnCUbkR1O8NK96DlIvHQIgIM0fG0LdJVXYlneaJb+L0dpNSBSTPBCEiKSKS7OSVgjUmQqnC0f4ZyDwLv7+aY5eXl/C/vpH0alSZWX8e4J5pq/V2k1IFIM8EYYwJMMaUdfIKMMboBP2q8ITUhCbDYM00iM05RlNEeKN/I26pGcLi7Un8tu1woYeoVHFzLbeYlCpcHV+Amu3gh3/AvLE5VqYTET4e3ozQMn4MnxbL8r8uO1RHKZUHTRCq6CgRAAO/gJjRsPJ9mDEgx/Tgvt5e/LtXA8AabX3nxOU6mE6pq6QJQhUt3j7Q9RVrevAdC2D+UzmqdG1QiVVPdeDh9jeyas8xmry0kKQUXdtaqSulCUIVTU1HQIsHrQF0G77OsbtCWX/GdK7Nq30iyTSGvhOXsSH+pJMTKaVyowlCFV0dX4CwGJjzMCRtc1rlzqZhvDWgMfuOpfK3d/5gzd7jhRqiUkWZJghVdHn7WlNx+Ja0pgdPP+W0Wo+Glfn+wVb4eAl93l9GxNi5HE5JK+RglSp6NEGooq1sZegzGY5sh2/uzfFk03kNqgTy+p0Ns7fveHcZOw+nFFaUShVJmiBU0VezHXR7FbbPg4XP5VqtZ6Mq7Bl3GzNHxpB0Kp2O4xfz5LdxOqhOqVxoglDFQ7P7oNn/wfJ3IHZKnlVjaoTw5f+1AGDGqv1MX+lRs9Yr5TFcmiBEpKuIbBORnSIy1sn+N0RknUxYL54AAB5FSURBVP3aLiInHPZlOuyb48o4VTHR5T9wYyeY+ygsmwBZuU/c1ygsiL/+052YGsE8M3sjEWPn8suWQ4UYrFKeL18LBl3ViUW8ge1AJ6w1plcDA40xm3Op/xDQ2Bgz3N4+ZYwpcyXX1AWDFOkpMOt+2PoD1GwPvSZCQMVcqx85lU671xaRkpaRXfZav4b0bVK1MKJVyu3yWjDIlQmiBfCCMaaLvf0kgDHmlVzqLwOeN8YstLc1QairYwysmQo/PQl+paHO7VC9DUS0zjVZnE7PoOP430k8aT3dFFqmBIsea0uZEjrlmCrernlFuatUBdjvsB1vl+UgItWA6sCvDsX+IhIrIitEpFduFxGRkXa92KSkpIKIWxV1IhA9HEYussZJbJoF34yA12vDzy9AZs6pN0qX8GHZ2PaseaYjN1Yow5FT6XR9czHHT58t7OiV8hiuTBDOFhTKrbkyAPjaGOP4jGK4ndUGAW+KSE1nBxpjJhljoo0x0eXLl7+2iFXxUqEuDPwcHt8N9/0KjQfDH2/AtNvgxP4c1UWEkDIl+HnMrQxsFkb88TOM+Hg1J1I1SajrkysTRDwQ5rBdFUjIpe4AYIZjgTEmwf65C1gENC74ENV1wdsHqjSBnu9Cn4/g0GaY2AqWvwdnU50e8krvSCYMbMzafSdo9OJCIsbOZfKSXYUcuFLu5co+CB+sTuoOwAGsTupBxphNl9SrDcwHqhs7GBEpB6QaY9JFJBRYDvTMrYP7PO2DUPly9C/4/u+wZwmUCoUWoyFyAATmvAO6avcxFm4+yIdLdgMwuHk4AE92r6v9E6pYcEsntX3h7sCbgDcwxRjzsoi8CMQaY+bYdV4A/I0xYx2OuwX4AMjCauW8aYz56HLX0wShrsi+FbD4Ndi50Nqu2ABqdYZbHoJSwRdVPXIqnaFTVrEpIRmAphHl+PL/WiCiS7Oros1tCaKwaYJQVyVpG2z/CbYvgH3LIaw5DJ1jzfXk4FR6Bkt3HmHW2gP8tOkgA5qG8XjXOpTw8aK0tiZUEaUJQqn8ivsSvr0PYh6Arv9xWiUry/Do1+v5du2B7LJZo2+hcXi5wopSqQLjrsdclSp6Iu+0puxY8a7TdSbIzMBrz++M716FL0bGZBff8d4yxnyxjoMndZZYVXxoC0KpS2WchY9vh4MboPUYCKoGpUPhr98g7gs4dQhCasHwn6B0KHHxJ+jxztLswx/pdBP3tamBv6+3Gz+EUvmjt5iUulLJifBpL0jaeqHMywdqdbFGZf/8ApS/CYZ+D/6BAMTFn+DZ2RtZH3+S0DIleHtgI26pGeqe+JXKJ00QSl2ts6mQfACSE6BCPShjD8bcvgBmDrQ6tId8Yy1aZJv95wH+8cU6AAY0DeOFHvW1NaE8liYIpVxhw9fWIkW1OkH/6eDjl70rOe0c/T9YwZZE67HY2yIr8Uinm6hR/oqmF1PK5bSTWilXuLkv3P4G7FhgPfmUeWFG2LL+vky/tzntalstjrlxibR//Xcixs7lnV93kJGZ+1TkSnkKbUEoda2WTYAFz0CjwdDlZdi3EvavgNDa0HAAmQamLt3NsdNneW/RX9mHfTEyhjo3lCWwlG8eJ1fKtfQWk1Ku9tsr8Ps4hwIBDFS/FXq8DeUiADDG8PTsjXzusIrdN6Na0KTaxSO3lSosmiCUcjVjYO0nkJII1VpakwPGzYQFz4HJhKYjrPKqzaB0CPuPpTJg0goOnDiTfQoRiHu+MwH+2qJQhUcThFLucjIefnwcdsyHLLuPIqI13DYeyt/E/mOpPPfdRk6eOcfafScI8Pdh4pAmtLxRH49VhUMThFLudjYVEtfBnqWw/B04lwq3PgEt/54959MHv//FK/OscReta4XyzqAoAktqa0K5liYIpTxJyiGY9zhsng2+paFUCJQMgvK1+bN8D+77vSRHTp8ltEwJHu9Sm75NquLlpbPGKtfQBKGUJ9r2E+z6Dc6cgDPHYf9KSDsBITfyZ9hQ7lhhLaJYI7Q0Hw1rSkgZPwJK+OgU46pAaYJQqig4dwY2fwerJsGBNZxt/wIvHevEpyv2ZlcJDy7FkJhwIqsGEVMjxI3BquJCE4RSRUlWpjVCe9O30P011lTsy/uLdrJoWxIZWRf+f21dK5RX+0ZSKbBkHidTKm9uG0ktIl1FZJuI7BSRsU72DxORJBFZZ7/uddg3VER22K+hroxTKY/i5Q29J0Ht2+DHR2mybwqTW6ey8+FwFj9wM//seBPNqwcTu+c4ncYvZuLvf7Er6ZS7o1bFkCvXpPbGWpO6ExCPtSb1QMd1pUVkGBBtjHnwkmODgVggGjDAGqCJMeZ4XtfUFoQqVjLSYeYg2PnzxeWlQqFCXQ4H3szdm6LYeqoUABMGNub2yEraR6GuSF4tCFeuk9gM2GmM2WUHMRPoCWzO8yhLF2ChMeaYfexCoCsww0WxKuV5fErAoK/gyDZIPWq9Th6Aw5vh8BYqbJjEPC9fttQbwKjdrXloxloemiH0aFiZf/WoT7nSfpe/hlJ5cGWCqALsd9iOB5o7qddHRNpgtTb+aYzZn8uxVZxdRERGAiMBwsPDCyBspTyIlxdUqOt837FdyKJx1Iubxu9MBX+rOHlrKX7b3Z6kenfT9pZbuLFCQOHFq4oVVyYIZ+3cS+9nfQ/MMMaki8j9wMdA+3weaxUaMwmYBNYtpqsPV6kiJriG1VfR8h+wdS5kZZCZlUnKni103zcP33U/sHRNfZLCm9I4Kgb/sMZQoY67o1ZFiCsTRDwQ5rBdFUhwrGCMOeqw+SHwX4dj215y7KICj1Cp4qBiPesFeGM1tRMP7OPX6f+l6anfqB7/Kb4HpgFwJvJuSv7t1YsWOFIqN658imk1UEtEqouIHzAAmONYQUQqOWz2ALbY7+cDnUWknIiUAzrbZUqpfKhUJZzBj79LmTFruL/a9wzweYMPMm6jZNwnnH73Vkja7u4QVRHgshaEMSZDRB7E+sPuDUwxxmwSkReBWGPMHOBhEekBZADHgGH2scdE5CWsJAPw4vkOa6VU/lUOKslHw1sCLdl4oA+PfjKJJ4+/hbzTipXVRtLmrufw9tXObOWcDpRT6jqy+8hpnpjyE6NS36Uda9jvHUZ8zL9o0amPu0NTbqIjqZVSOSz98TPCV71IGIfYEXgLtQa+Cjfc7O6wVCHTBKGUcupEcjLT3xrLkIxZBEoq8ZU6U/nGhniJgJePte52SE13h6lcSBOEUipXp9MzeGbGUmrsnMIw7/mUJg0EvDBkefni1eIBaPMYlCjj7lCVC2iCUEpd1pFT6Qydsorth1KIrhbMzt1/8bj3TPr5LOakTyhlmg3Bu1YHCGtujfJWxYImCKXUFYs/nspr87exd/3vPOLzJTHeW/EhE3xKQlAYlAy2FjsKrArB1a2Be2HNrcWPVJGhCUIpddWMMYz8dA3LNu8hxmszT9Q+TK2SyUjqMWt+qBP74WyKVdmnJDToDU3ugarRoBMHejx3TdanlCoGRIQPhjThvk8Mv2wtyS9bYNgtEbxwZ32rgjFWokjaChu+hg1fwbrpUK0V3Pa6Tu9RhGkLQimVb2nnMnnw8z/5ecshvL2EAU3DeLxrHQJL+l6olJ4C62bAby/D2VNwy0PQZBj4B0KJstZ6F8pj6C0mpVSBycwyDPxwBRsPnCT1bOZF+yoH+vP5fTFEhJaG00dg4XNWa8JR+C3Q5d9QpUkhRq1yowlCKVXgMrMM7/22k9cX5pzXafLd0XSoW8FavChhHRzaCGnJkHoE1n4Kpw9DZH+IHgGlQ6FkOfAPsqY3V4VKE4RSqlCs2HWUYVNXkXYuC4DX+jWkd+MqeHk5dFanp8CS8bD8XchMv1DuHwThLaDaLVC7G4TWKuTor0+aIJRShSb+eCpjvljPqj3W/JoBJXwYFBPOwKbh1q2n85IT4OBGOHMMUo9B0hbYuwyO7gTxgkaDod1TULaymz7J9UEThFKq0J1Oz+C1BduYtmwP5//MlPLz5n99G3JbZKXcD0xOhOXvwKpJIN5Q93bw8beSRkAliLxTp/8oQJoglFJutSUxmYWbDzF5yS6S0zL4bERzWtUKzfug43vg13/D3uVgsqzX6cPWz7Dm0KCP1dFdsb4ugHQNNEEopTxCcto5er+3jJ2HTxFUypfP742hXuWyV3CCRIj7AtZ9Dke2WWXiDeXrQFgzCI+xXuUiXBJ/ceS2BCEiXYG3sBYMmmyMGXfJ/jHAvVgLBiUBw40xe+19mcAGu+o+Y0yPy11PE4RSnm//sVTu+ySWrQdTKOnrzZPd69AoLIhaFQIo6ZfPMRLGwMn9kBgHievhQCzEx0J6srU/qBrUaAsRra3lWENu1PmjcuGWBCEi3sB2oBPWGtOrgYHGmM0OddoBK40xqSIyCmhrjOlv7ztljLmi6SM1QShVdKzdd5znvtvIxgPWH/Xw4FI0rx7MkJhqNAy7ivmcsjIhaRvsXQq7FsHuxRcShnhBaG1oPtLq/NZkkc1dCaIF8IIxpou9/SSAMeaVXOo3Bt4xxrS0tzVBKFXMGWPYeCCZr9bsZ9XuY2w9aM3pNKJVdZ7oWgc/n2sYF5GZYT0ZlbQNjmyHnT/DgTUQUNlKFIFh4FcaSgRAUDiUrXJdjvJ2V4LoC3Q1xtxrb98FNDfGPJhL/XeAg8aYf9vbGcA6rNtP44wxs3M5biQwEiA8PLzJ3r17C/yzKKUKx45DKUxespsvYvcDVqJ4qntdvL0KYNI/Y6yWxeLXYO8fOfd7+1l9F2HNIKINRLS0kkYxn3DQXQmiH9DlkgTRzBjzkJO6Q4AHgVuNMel2WWVjTIKI1AB+BToYY/7K65raglCqeJi/6SD/9+kaAG6PrMT/+jbMf/9EfiQnWrefzp6GtJNwYi8c2221NPYug7QTVj0vXyhTEQIqWtOZh9SC0BuhbFWrrMwN4OtfcHG5gbtmc40Hwhy2qwIJl1YSkY7A0zgkBwBjTIL9c5eILAIaA3kmCKVU8dCl/g3sGXcbE3//i3HztvJDXCKd61VkSEw12txU/tovULYSkMtYjKxMa2qQfSshJQFSDlk/9620Zqvlki/VvqWhVLA1XUhAJWtgX2AVK4mUrWytl1GyHPiVAR+/a4+9ELmyBeGD1UndATiA1Uk9yBizyaFOY+BrrFtROxzKywGpxph0EQkFlgM9HTu4ndEWhFLFz/frE3hoxp8Xlb3Ysz79moQVbKsiP86dgWO7rBZISiKcOmiNAj9z3JryPCURTh6wRoc74+1nvQAQaxnX0uWhTAVrAaYSAeBf1mq5mEzIyrA62H1LW2M9vH2tsqxMazzIeX6lodl9V/WR3PmYa3fgTazHXKcYY14WkReBWGPMHBH5GbgZSLQP2WeM6SEitwAfAFmAF/CmMeajy11PE4RSxVfiyTP8bcJSjpy6MH9T61qhTB3WFB9vD5vk72yqlSySD1gJI+0EpJ+yFlbKyrT6QzDWBIank6wBgGeOW/NUpSVD1jkrSXh5W/WzzuV9vdIV4LEdedfJhQ6UU0oVG2nnMnnq2w3sTDpFXPzJ7PLHutRm1K01L54YsCgyJmfHeOY5OJdqPZnl5Q1ePlbLwpFfqau6nCYIpVSx9NEfu/n33M3Zcz1VCSrJu4OjaHQ14yiuU5oglFLFWkraOTq/sZjEk2kA3NMygvZ1KlC7YgAVyhbtp4xcTROEUuq6sP9YKs9+t5FF25Kyy0r5ebPiqQ6U9ffN48jrlyYIpdR1IyMzi+/WJZCcdo7F25P4zU4WD7W/kfZ1KtA4vJybI/QsmiCUUtetWX/G888v1mdv925chZfvuLnwH5H1UHklCA97NkwppQrWHY2r8td/uvPs7fUA+PbPA9R97ife+XUHh1PS3BydZ9MWhFLqurJg00Ge+24TB5Ot5BAWXJL+0WGMbntj0X9E9iroLSallHJgjGH+pkO8vmAbOw6fAqyhB13r38CbAxpRwuf6uf2kCUIppXKRknaOl37YzJex8QA0qx7MwGZh1KoQQP3KZRGdzbV40AShlLpayWnn+OD3v/h85T6Op1pTW1QPLU3PRpXZdyyVB9vdSI3yV7RETZGgCUIppfIpIzOLdftPMHP1fn7ckEjq2UwAfLyEhzvUonP9itxUIaDY9FdoglBKqauQlWU4nJLOws0HeX3hdk7YLYvWtULp1agKzaoHExZ8dXMgeQpNEEopdY3SzmUy4dcdJJ5MY9afB3D809k/OozwkFL0jqpCpcCS7gvyKmiCUEqpAhQXf4K9R1OJP36G//609aJ9DaqUJSq8HNERwXSuVxF/X89+IkoThFJKuVB6RiZ/7jvBI1+u58CJMzn2x9QI5pXekVQK9Pe4hOHOBYO6Am9hLRg02Rgz7pL9JYBPgCbAUaC/MWaPve9JYASQCTxsjJl/uetpglBKuZMxhtSzmRw7fZY56xOYG5fI5sTkHPW61K9IRqahf9MwOtat6NYOb7ckCBHxxlpytBPW+tSrgYGOy4aKyGgg0hhzv4gMAO4wxvQXkXrADKAZUBn4GbjJGJOZ1zU1QSilPNHsPw+QlJLOvmOpfLpiL+VK+WY/SntevUplOZF6lta1ypNpDH/uO06FAH/a16lAlXIl8fYSKpb1x8dL8PYSfL29qBFamn3HUgkLLoX3VSYZdyWIFsALxpgu9vaTAMaYVxzqzLfrLLfXsD4IlAfGOtZ1rJfXNTVBKKWKinOZWcxae4BJS3ax0x7NfbVCy/jxxxPtr+r2VV4JwueaospbFWC/w3Y80Dy3OsaYDBE5CYTY5SsuObaK60JVSqnC5evtxZ1Nw7izaRgAZzOyOHo6nZK+3iScSCMjK4typfzYezSV+ZsO0jg8iNPpGfy5/wQlfLwo5eeDl8CSHUeIqlbOJX0brkwQzto7lzZXcquTn2OtE4iMBEYChIeHX0l8SinlMfx8vLIfkQ0q5ZddHhZcila1QrO372pReDG5crrveCDMYbsqkJBbHfsWUyBwLJ/HAmCMmWSMiTbGRJcvX76AQldKKeXKBLEaqCUi1UXEDxgAzLmkzhxgqP2+L/CrsTpF5gADRKSEiFQHagGrXBirUkqpS7jsFpPdp/AgMB/rMdcpxphNIvIiEGuMmQN8BHwqIjuxWg4D7GM3iciXwGYgA3jgck8wKaWUKlg6UE4ppa5juuSoUkqpK6YJQimllFOaIJRSSjmlCUIppZRTxaqTWkSSgL1XeXgocKQAw3EFjbFgaIwFoyjECEUjTnfGWM0Y43QQWbFKENdCRGJz68n3FBpjwdAYC0ZRiBGKRpyeGqPeYlJKKeWUJgillFJOaYK4YJK7A8gHjbFgaIwFoyjECEUjTo+MUfsglFJKOaUtCKWUUk5pglBKKeXUdZ8gRKSriGwTkZ0iMraQrz1FRA6LyEaHsmARWSgiO+yf5exyEZG37TjjRCTK4Zihdv0dIjLU2bWuIcYwEflNRLaIyCYR+bunxSki/iKySkTW2zH+yy6vLiIr7et9YU87jz2N/Bd2jCtFJMLhXE/a5dtEpEtBxehwfm8R+VNEfvDgGPeIyAYRWScisXaZx/y+7XMHicjXIrLV/m+zhSfFKCK17X+/869kEfmHJ8WYL8aY6/aFNQ35X0ANwA9YD9QrxOu3AaKAjQ5lrwJj7fdjgf/a77sD87BW24sBVtrlwcAu+2c5+325AoyxEhBlvw8AtgP1PClO+1pl7Pe+wEr72l8CA+zyicAo+/1oYKL9fgDwhf2+nv3fQAmguv3fhncB/87HAJ8DP9jbnhjjHiD0kjKP+X3b5/8YuNd+7wcEeVqMDrF6AweBap4aY66xF9aFPPEFtADmO2w/CTxZyDFEcHGC2AZUst9XArbZ7z8ABl5aDxgIfOBQflE9F8T7HdDJU+MESgFrsdY/PwL4XPq7xlqjpIX93seuJ5f+/h3rFVBsVYFfgPbAD/Y1PSpG+5x7yJkgPOb3DZQFdmM/ZOOJMV4SV2dgqSfHmNvrer/FVAXY77Adb5e5U0VjTCKA/bOCXZ5brIX2GezbHI2xvqF7VJz2rZt1wGFgIdY36xPGmAwn18uOxd5/EghxdYzAm8DjQJa9HeKBMYK1/vsCEVkj1prv4Fm/7xpAEjDVvl03WURKe1iMjgYAM+z3nhqjU9d7ghAnZZ763G9usRbKZxCRMsA3wD+MMcl5Vc0lHpfGaYzJNMY0wvqW3gyom8f1Cj1GEbkdOGyMWeNYnMf13Pn7bmmMiQK6AQ+ISJs86rojTh+sW7PvG2MaA6exbtfkxm3/lnafUg/gq8tVzSUWt/6Nut4TRDwQ5rBdFUhwUyznHRKRSgD2z8N2eW6xuvwziIgvVnKYboz51lPjBDDGnAAWYd3HDRKR88vqOl4vOxZ7fyDWkreujLEl0ENE9gAzsW4zvelhMQJgjEmwfx4GZmElXE/6fccD8caYlfb211gJw5NiPK8bsNYYc8je9sQYc3W9J4jVQC37SRI/rKbgHDfHNAc4/6TCUKx7/ufL77afdogBTtpN1PlAZxEpZz8R0dkuKxAiIlhrh28xxoz3xDhFpLyIBNnvSwIdgS3Ab0DfXGI8H3tf4Fdj3eCdAwywnyCqDtQCVhVEjMaYJ40xVY0xEVj/nf1qjBnsSTECiEhpEQk4/x7r97QRD/p9G2MOAvtFpLZd1AFr/XqPidHBQC7cXjofi6fFmLvC6uzw1BfW0wPbse5ZP13I154BJALnsL4pjMC6z/wLsMP+GWzXFeBdO84NQLTDeYYDO+3XPQUcYyusJm0csM5+dfekOIFI4E87xo3Ac3Z5Daw/njuxmvgl7HJ/e3unvb+Gw7metmPfBnRz0e+9LReeYvKoGO141tuvTef/n/Ck37d97kZArP07n431hI+nxVgKOAoEOpR5VIyXe+lUG0oppZy63m8xKaWUyoUmCKWUUk5pglBKKeWUJgillFJOaYJQSinllCYIVaSISKY9O+Z6EVkrIrdcpn6QiIzOx3kXiYjHLRrvTiIyTUT6Xr6mKq40Qaii5owxppExpiHWxHWvXKZ+ENbMqB7JYRS1Uh5HE4QqysoCx8GaK0pEfrFbFRtEpKddZxxQ0251/M+u+7hdZ72IjHM4Xz+x1pXYLiKt7breIvI/EVltz9P/f3Z5JRFZbJ934/n6jsRaV+G/9jlXiciNdvk0ERkvIr8B/xVrjYDZ9vlXiEikw2eaascaJyJ97PLOIrLc/qxfiTVPFiIyTkQ223Vfs8v62fGtF5HFl/lMIiLv2OeYy4WJ5NR1Sr+9qKKmpFiztvpjTYfc3i5PA+4wxiSLSCiwQkTmYE3i1sBYE/khIt2AXkBzY0yqiAQ7nNvHGNNMRLoDz2NN2TECa9qDpiJSAlgqIguA3lhTc78sIt5Yo2adSbbPeTfW3Eu32+U3AR2NMZkiMgH40xjTS0TaA59gjRR+1r72zXbs5ezP9ox97GkReQIYIyLvAHcAdYwx5vzUI8BzQBdjzAGHstw+U2OgNnAzUBFr+oop+fqtqGJJE4Qqas44/LFvAXwiIg2wpir4j1gzj2ZhTYlc0cnxHYGpxphUAGPMMYd95yciXIO1TgdYc99EOtyLD8Sa/2g1MEWsiQxnG2PW5RLvDIefbziUf2WMybTftwL62PH8KiIhIhJoxzrg/AHGmONizQpbD+uPOliL5SwHkrGS5GT72/8P9mFLgWki8qXD58vtM7UBZthxJYjIr7l8JnWd0AShiixjzHL7G3V5rPmhygNNjDHnxJo11d/JYULu0yWn2z8zufD/hgAPGWNyTJBmJ6PbgE9F5H/GmE+chZnL+9OXxOTsOGexCrDQGDPQSTzNsCauGwA8CLQ3xtwvIs3tONeJSKPcPpPdctK5d1Q27YNQRZaI1MFazvEo1rfgw3ZyaIe1vCNACtZSqectAIaLSCn7HI63mJyZD4yyWwqIyE1izXhazb7eh1iz3Ublcnx/h5/Lc6mzGBhsn78tcMRYa24swPpDf/7zlgNWAC0d+jNK2TGVwZoU7kfgH1i3qBCRmsaYlcaY57BWpQvL7TPZcQyw+ygqAe0u82+jijltQaii5nwfBFjfhIfa9/GnA9+LSCzWjLNbAYwxR0VkqYhsBOYZYx6zv0XHishZ4EfgqTyuNxnrdtNase7pJGH1YbQFHhORc8Ap4O5cji8hIiuxvozl+NZvewFrdbQ4IJUL00H/G3jXjj0T+Jcx5lsRGQbMsPsPwOqTSAG+ExF/+9/ln/a+/4lILbvsF6xZWuNy+UyzsPp0NmDNcPx7Hv8u6jqgs7kq5SL2ba5oY8wRd8ei1NXQW0xKKaWc0haEUkopp7QFoZRSyilNEEoppZzSBKGUUsopTRBKKaWc0gShlFLKqf8HvuPacVhV7IsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learner.recorder.plot_losses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learner.recorder.plot_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional Fine tune regular fit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learner.callbacks.append(\n",
    "    ReduceLROnPlateauCallback(learner, monitor='train_loss', mode='min', factor=0.2, patience=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learner.fit(100, 5e-5)# , moms=(0.75,0.70))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learner.fit_one_cycle(300, 5e-4)#, moms=(0.75,0.70))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learner.recorder.plot_lr(show_moms=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learner.recorder.plot_losses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learner.recorder.plot_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learner.fit(10,1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learner.to_fp32()\n",
    "val = learner.validate()[1]\n",
    "print(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    sub_fname = f'loss{learner.recorder.losses[-1]:.04f}val{val:.04f}'\n",
    "except:\n",
    "    sub_fname = f'val{val:.04f}'\n",
    "learner.save(sub_fname)\n",
    "print(sub_fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference\n",
    "\n",
    "Make sure `tranforms` are activated to test set otherwise TTA > 1 will be as TTA =1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    xt_coulombmat = load_fn(f'xt_coulombmat32{ext}.npy')\n",
    "except:\n",
    "    xt_coulombmat = np.load(f'xt_coulombmat{ext}.npy', allow_pickle=True)\n",
    "    xt_coulombmat = np.array(xt_coulombmat.tolist()).astype(np.float32)\n",
    "    np.save(f'xt_coulombmat32{ext}.npy', xt_coulombmat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_fname = Path('test.npz')\n",
    "try:\n",
    "    npzfile  = np.load(fname_ext(test_fname, ext))\n",
    "    xt_xyz   = npzfile['x_xyz']\n",
    "    xt_type  = npzfile['x_type']\n",
    "    xt_ext   = npzfile['x_ext']\n",
    "    xt_atom  = npzfile['x_atom']\n",
    "    mt = npzfile['m']\n",
    "    xt_ids = npzfile['x_ids']\n",
    "except:\n",
    "    xt_xyz,xt_type,xt_ext,xt_atom,mt,xt_ids = \\\n",
    "        preprocess(test_fname.with_suffix('.csv'), type_index=types,ext=ext)\n",
    "    np.savez(fname_ext('_'+test_fname, ext), \n",
    "             x_xyz  = xt_xyz,\n",
    "             x_type = xt_type,\n",
    "             x_ext  = xt_ext,\n",
    "             x_atom = xt_atom,\n",
    "             m=mt,\n",
    "             x_ids=xt_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xt_qm9_mulliken = load_fn(f'xt_qm9_mulliken{ext}.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "[v.shape for v in [xt_xyz,xt_type,xt_ext,xt_atom, xt_qm9_mulliken,xt_ids, mt]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learner.data.add_test(ItemList(items=(MoleculeItem(i,*v) for i,v in \n",
    "                              enumerate(zip(xt_xyz,xt_type,xt_ext,xt_atom,xt_qm9_mulliken,xt_coulombmat)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TTA_N = 1\n",
    "learner.data.test_ds.tfms = tta_tfms if TTA_N > 1 else tfms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#learner.model = learner.model.module\n",
    "#data.batch_size = 4096*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#learner.model = nn.DataParallel(learner.model)\n",
    "old_bs = data.batch_size\n",
    "data.batch_size *= 2\n",
    "\n",
    "sub = defaultdict(int)\n",
    "xt_ids_not_extended = (xt_ids!=0) & (xt_ids<=7163688) # TODO\n",
    "ids = xt_ids[xt_ids_not_extended]\n",
    "\n",
    "mb = master_bar(range(TTA_N))\n",
    "for tta in mb:\n",
    "    test_preds = np.zeros((0, 29), dtype=np.float32)\n",
    "\n",
    "    for batch_idx, batch in progress_bar(\n",
    "        enumerate(learner.dl(DatasetType.Test)), total=len(learner.dl(DatasetType.Test)), parent=mb):\n",
    "        _, _, preds_,_,_,_ = learner.pred_batch(ds_type=DatasetType.Test, batch=batch)\n",
    "        preds_ = preds_.sum(dim=1)\n",
    "        test_preds = np.concatenate([test_preds, preds_.data.cpu().numpy()], axis = 0)\n",
    "\n",
    "    preds = test_preds[xt_ids_not_extended]\n",
    "    for k in range(len(ids)):\n",
    "        sub[int(ids[k])] += preds[k]\n",
    "    \n",
    "for k in range(len(ids)):\n",
    "    sub[int(ids[k])] = sub[int(ids[k])]/TTA_N\n",
    "\n",
    "learner.model = learner.model.module\n",
    "data.batch_size = old_bs\n",
    "\n",
    "sub_df = pd.DataFrame(sub.items(), columns=['id', 'scalar_coupling_constant'])\n",
    "sub_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submit to Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "csv_fname = f'{sub_fname}_tta{TTA_N}.csv'\n",
    "sub_df.to_csv(csv_fname, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "comp = 'champs-scalar-coupling'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!kaggle competitions submit -c {comp} -f {csv_fname} -m 'QM9 tta {TTA_N} {ext}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "time.sleep(60)\n",
    "!kaggle competitions submissions -c {comp} -v > submissions-{comp}.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submissions = pd.read_csv(f'submissions-{comp}.csv')\n",
    "submissions.iloc[0].publicScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (root)",
   "language": "python",
   "name": "root"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
