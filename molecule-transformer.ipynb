{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%load_ext autoreload\n",
    "#%autoreload 2\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.utils.data\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from torch.utils.data.sampler import RandomSampler\n",
    "#RandomSampler_num_samples = RandomSampler.num_samples\n",
    "#@property\n",
    "#def RandomSampler_num_samples_x10(self): return  self.RandomSampler_num_samples*10\n",
    "#RandomSampler.num_samples= RandomSampler_num_samples_x10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.nn import Parameter\n",
    "\n",
    "def _load_from_state_dict(self, state_dict, prefix, local_metadata, strict,\n",
    "                          missing_keys, unexpected_keys, error_msgs):\n",
    "    r\"\"\"Copies parameters and buffers from :attr:`state_dict` into only\n",
    "    this module, but not its descendants. This is called on every submodule\n",
    "    in :meth:`~torch.nn.Module.load_state_dict`. Metadata saved for this\n",
    "    module in input :attr:`state_dict` is provided as :attr:`local_metadata`.\n",
    "    For state dicts without metadata, :attr:`local_metadata` is empty.\n",
    "    Subclasses can achieve class-specific backward compatible loading using\n",
    "    the version number at `local_metadata.get(\"version\", None)`.\n",
    "\n",
    "    .. note::\n",
    "        :attr:`state_dict` is not the same object as the input\n",
    "        :attr:`state_dict` to :meth:`~torch.nn.Module.load_state_dict`. So\n",
    "        it can be modified.\n",
    "\n",
    "    Arguments:\n",
    "        state_dict (dict): a dict containing parameters and\n",
    "            persistent buffers.\n",
    "        prefix (str): the prefix for parameters and buffers used in this\n",
    "            module\n",
    "        local_metadata (dict): a dict containing the metadata for this module.\n",
    "            See\n",
    "        strict (bool): whether to strictly enforce that the keys in\n",
    "            :attr:`state_dict` with :attr:`prefix` match the names of\n",
    "            parameters and buffers in this module\n",
    "        missing_keys (list of str): if ``strict=True``, add missing keys to\n",
    "            this list\n",
    "        unexpected_keys (list of str): if ``strict=True``, add unexpected\n",
    "            keys to this list\n",
    "        error_msgs (list of str): error messages should be added to this\n",
    "            list, and will be reported together in\n",
    "            :meth:`~torch.nn.Module.load_state_dict`\n",
    "    \"\"\"\n",
    "    for hook in self._load_state_dict_pre_hooks.values():\n",
    "        hook(state_dict, prefix, local_metadata, strict, missing_keys,\n",
    "             unexpected_keys, error_msgs)\n",
    "\n",
    "    local_name_params = itertools.chain(self._parameters.items(),\n",
    "                                        self._buffers.items())\n",
    "    local_state = {k: v.data for k, v in local_name_params if v is not None}\n",
    "\n",
    "    for name, param in local_state.items():\n",
    "        key = prefix + name\n",
    "        if key in state_dict:\n",
    "            input_param = state_dict[key]\n",
    "\n",
    "            # Backward compatibility: loading 1-dim tensor from 0.3.* to version 0.4+\n",
    "            if len(param.shape) == 0 and len(input_param.shape) == 1:\n",
    "                input_param = input_param[0]\n",
    "\n",
    "            if input_param.shape != param.shape:\n",
    "                # local shape should match the one in checkpoint\n",
    "                error_msgs.append(\n",
    "                    'size mismatch for {}: copying a param with shape {} from checkpoint, '\n",
    "                    'the shape in current model is {}.'.format(\n",
    "                        key, input_param.shape, param.shape))\n",
    "                #if not strict:\n",
    "                #    continue\n",
    "\n",
    "            if isinstance(input_param, Parameter):\n",
    "                # backwards compatibility for serialized parameters\n",
    "                input_param = input_param.data\n",
    "\n",
    "            try:\n",
    "                param.copy_(input_param)\n",
    "            except Exception:\n",
    "                error_msgs.append(\n",
    "                    'While copying the parameter named \"{}\", '\n",
    "                    'whose dimensions in the model are {} and '\n",
    "                    'whose dimensions in the checkpoint are {}.'.format(\n",
    "                        key, param.size(), input_param.size()))\n",
    "                # PG load partially\n",
    "\n",
    "                if len(input_param.size()) == 3:\n",
    "                    error_msgs.append(\n",
    "                        'Partially copying the parameter named \"{}\", '\n",
    "                        'whose dimensions in the model are {} and '\n",
    "                        'whose dimensions in the checkpoint are {}. - trying {}'\n",
    "                        .format(\n",
    "                            key, param.size(), input_param.size(),\n",
    "                            param[:input_param.size()[0], :input_param.size(\n",
    "                            )[1], :input_param.size()[2]].shape))\n",
    "                else:\n",
    "                    error_msgs.append(\n",
    "                        'Partially copying the parameter named \"{}\", '\n",
    "                        'whose dimensions in the model are {} and '\n",
    "                        'whose dimensions in the checkpoint are {}. - trying {}'\n",
    "                        .format(key, param.size(), input_param.size(),\n",
    "                                param[:input_param.size()[0]].shape))\n",
    "\n",
    "                try:\n",
    "                    new_input_param = torch.empty_like(param)\n",
    "                    new_input_param = torch.nn.init.normal_(new_input_param,\n",
    "                                                            mean=input_param.mean(),\n",
    "                                                            std=input_param.std())\n",
    "\n",
    "                    if len(input_param.size()) == 3:\n",
    "                        new_input_param[:input_param.size()[0], :input_param.\n",
    "                                        size()[1], :input_param.size(\n",
    "                                        )[2]] = input_param\n",
    "                    else:\n",
    "                        new_input_param[:input_param.size()[0]] = input_param\n",
    "                    param.copy_(new_input_param)\n",
    "                except Exception as e:\n",
    "                    assert e\n",
    "                    error_msgs.append(\n",
    "                        'Failed to load weights partially {}'.format(e))\n",
    "        elif strict:\n",
    "            missing_keys.append(key)\n",
    "\n",
    "    if strict:\n",
    "        for key in state_dict.keys():\n",
    "            if key.startswith(prefix):\n",
    "                input_name = key[len(prefix):]\n",
    "                input_name = input_name.split(\n",
    "                    '.', 1)[0]  # get the name of param/buffer/child\n",
    "                if input_name not in self._modules and input_name not in local_state:\n",
    "                    unexpected_keys.append(key)\n",
    "\n",
    "def load_state_dict(self, state_dict, strict=True):\n",
    "    r\"\"\"Copies parameters and buffers from :attr:`state_dict` into\n",
    "    this module and its descendants. If :attr:`strict` is ``True``, then\n",
    "    the keys of :attr:`state_dict` must exactly match the keys returned\n",
    "    by this module's :meth:`~torch.nn.Module.state_dict` function.\n",
    "\n",
    "    Arguments:\n",
    "        state_dict (dict): a dict containing parameters and\n",
    "            persistent buffers.\n",
    "        strict (bool, optional): whether to strictly enforce that the keys\n",
    "            in :attr:`state_dict` match the keys returned by this module's\n",
    "            :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\n",
    "\n",
    "    Returns:\n",
    "        ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n",
    "            * **missing_keys** is a list of str containing the missing keys\n",
    "            * **unexpected_keys** is a list of str containing the unexpected keys\n",
    "    \"\"\"\n",
    "    missing_keys = []\n",
    "    unexpected_keys = []\n",
    "    error_msgs = []\n",
    "\n",
    "    # copy state_dict so _load_from_state_dict can modify it\n",
    "    metadata = getattr(state_dict, '_metadata', None)\n",
    "    state_dict = state_dict.copy()\n",
    "    if metadata is not None:\n",
    "        state_dict._metadata = metadata\n",
    "\n",
    "    def load(module, prefix=''):\n",
    "        local_metadata = {} if metadata is None else metadata.get(\n",
    "            prefix[:-1], {})\n",
    "        module._load_from_state_dict(state_dict, prefix, local_metadata, True,\n",
    "                                     missing_keys, unexpected_keys, error_msgs)\n",
    "        for name, child in module._modules.items():\n",
    "            if child is not None:\n",
    "                load(child, prefix + name + '.')\n",
    "                \n",
    "    load(self)\n",
    "\n",
    "    if strict:\n",
    "        if len(unexpected_keys) > 0:\n",
    "            error_msgs.insert(\n",
    "                0, 'Unexpected key(s) in state_dict: {}. '.format(', '.join(\n",
    "                    '\"{}\"'.format(k) for k in unexpected_keys)))\n",
    "        if len(missing_keys) > 0:\n",
    "            error_msgs.insert(\n",
    "                0, 'Missing key(s) in state_dict: {}. '.format(', '.join(\n",
    "                    '\"{}\"'.format(k) for k in missing_keys)))\n",
    "\n",
    "    if strict and len(error_msgs) > 0:\n",
    "        raise RuntimeError(\n",
    "            'Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n",
    "                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nn.Module._load_from_state_dict = _load_from_state_dict\n",
    "nn.Module.load_state_dict = load_state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from fastai import *\n",
    "from fastai.basic_train import *\n",
    "from fastai.basic_data import *\n",
    "from fastai.data_block import *\n",
    "from fastai.torch_core import *\n",
    "from fastai.train import *\n",
    "from fastai.callback import *\n",
    "from fastai.callbacks import *\n",
    "from fastai.distributed import *\n",
    "from fastai.layers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ext = '_ext' # or ''\n",
    "multi = '_molecule'\n",
    "train_fname = Path('train.npz')\n",
    "type_index=types = ['1JHC', '2JHH', '1JHN', '2JHN', '2JHC', '3JHH', '3JHC', '3JHN'] \n",
    "atoms = 'CFHNO'\n",
    "xyz = ['x', 'y', 'z']\n",
    "max_atoms = 29 #int(s.atom_index.max() + 1)\n",
    "\n",
    "fname_ext = lambda fname,ext,multi: f'{str(fname)[:-4]}{ext}{multi}{str(fname)[-4:]}'\n",
    "s  = pd.read_csv('structures.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_group(process_args,i):\n",
    "    m_group,has_y,x0_xyz,x0_type,x0_ext,x0_atom,x0_id,m0,y0_scalar = process_args\n",
    "    x0_atom[:] = -1\n",
    "    x0_type[:] = -1\n",
    "    x0_id[:] = -1\n",
    "    y0_scalar[:] = np.nan\n",
    "    (m_name, m_index) ,m_group = m_group\n",
    "    ss = s[s.molecule_name==m_name]\n",
    "    n_atoms = len(ss)\n",
    "    ii = 0\n",
    "    x0_xyz[ii] = 0.\n",
    "    x0_type[ii] = -1\n",
    "    x0_ext[ii] =  True\n",
    "    x0_xyz[ii,:,:n_atoms] = ss[xyz].values.T  # xyz \n",
    "    x0_type[ii,0,m_group['atom_index_1'], m_group['atom_index_0']] = m_group['type_idx'].T\n",
    "    x0_ext[ii,0, m_group['atom_index_0']] = m_group['ext'].T\n",
    "    x0_atom[ii,:,:n_atoms] = ss['atom_idx'].T\n",
    "    x0_id[ii,0,m_group['atom_index_1'], m_group['atom_index_0']] = m_group['id'].T\n",
    "    m0[ii] = m_index\n",
    "    res = [x0_xyz,x0_type,x0_ext,x0_atom,x0_id,m0]\n",
    "    if has_y:\n",
    "        y0_scalar[0,0,m_group['atom_index_1'], m_group['atom_index_0']] =  m_group['scalar_coupling_constant']\n",
    "        res.append(y0_scalar)\n",
    "    return i,res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(fname, type_index=None, ext=''):\n",
    "    t  = pd.read_csv(fname_ext(fname,ext,''))\n",
    "    has_y = 'scalar_coupling_constant' in t.columns\n",
    "    t['molecule_index'] = pd.factorize(t['molecule_name'])[0] + t['id'].min()\n",
    "    # make sure we use the same indexes in train/test (test needs to provide type_index)\n",
    "    if type_index is not None:\n",
    "        t['type_idx'] = t['type'].apply(lambda x: type_index.index(x) ) # pd.factorize(pd.concat([pd.Series(type_index),t['type']]))[0][len(type_index):]\n",
    "    else:\n",
    "        t['type_idx'] = pd.factorize(t['type'])[0]\n",
    "\n",
    "    s['atom_idx'] = s['atom'].apply(lambda x: atoms.index(x) )\n",
    "\n",
    "    max_items = t['molecule_name'].nunique()\n",
    "    x_xyz   = np.zeros((max_items,len(xyz),  max_atoms), dtype=np.float32)\n",
    "    x_type  = np.empty((max_items,1,         max_atoms,max_atoms), dtype=np.int8)\n",
    "    x_ext   = np.zeros((max_items,1,         max_atoms), dtype=np.bool_)\n",
    "    x_atom  = np.empty((max_items,1,         max_atoms), dtype=np.int8)\n",
    "    x_ids   = np.zeros((max_items,           max_atoms,max_atoms), dtype=np.int32)\n",
    "    x_atom[:] = -1\n",
    "    x_type[:] = -1\n",
    "    x_ids[:] = -1\n",
    "    m = np.zeros((max_items,), dtype=np.int32)\n",
    "    if has_y:\n",
    "        y_scalar   = np.empty((max_items,1 ,max_atoms,max_atoms), dtype=np.float32)\n",
    "        y_scalar[:] = np.nan\n",
    "        \n",
    "    x0_xyz   = np.zeros((1,len(xyz),  max_atoms), dtype=np.float32)        \n",
    "    x0_type  = np.empty((1,1,         max_atoms,max_atoms), dtype=np.int8)\n",
    "    x0_ext   = np.zeros((1,1,         max_atoms), dtype=np.bool_)\n",
    "    x0_atom  = np.empty((1,1,         max_atoms), dtype=np.int8)\n",
    "    x0_id    = np.zeros((1,1,         max_atoms,max_atoms), dtype=np.int32)\n",
    "    y0_scalar   = np.empty((1,1 ,max_atoms,max_atoms), dtype=np.float32)\n",
    "    m0 = np.zeros((1,), dtype=np.int32)\n",
    "    \n",
    "    process_args = [(m_group,has_y,x0_xyz,x0_type,x0_ext,x0_atom,x0_id,m0,y0_scalar) \n",
    "                    for m_group in t.groupby(['molecule_name', 'molecule_index'])]\n",
    "    res = parallel(process_group,process_args,max_workers=multiprocessing.cpu_count())\n",
    "    for i,r in progress_bar(res):\n",
    "        x_xyz[i],x_type[i],x_ext[i],x_atom[i],x_ids[i],m[i] = r[:6]\n",
    "        if has_y: y_scalar[i]=r[6]\n",
    "        \n",
    "    res = [x_xyz,x_type,x_ext,x_atom,x_ids,m]\n",
    "    if has_y: res.append(y_scalar)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define where you want to use original training set '' or extended ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load preprocessed or preprocess and save for later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='85003' class='' max='85003', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [85003/85003 00:00<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_fname = Path('train.npz')\n",
    "types = ['1JHC', '2JHH', '1JHN', '2JHN', '2JHC', '3JHH', '3JHC', '3JHN'] \n",
    "atoms = 'CFHNO'\n",
    "\n",
    "try:\n",
    "    npzfile = np.load(fname_ext(train_fname,ext,multi))\n",
    "    x_xyz   = npzfile['x_xyz']\n",
    "    x_type  = npzfile['x_type']\n",
    "    x_ext   = npzfile['x_ext']\n",
    "    x_atom  = npzfile['x_atom']\n",
    "\n",
    "    y_scalar    = npzfile['y_scalar']\n",
    "    m = npzfile['m']\n",
    "    max_items, max_atoms = x_xyz.shape[0], x_xyz.shape[-1]\n",
    "except:\n",
    "    x_xyz,x_type,x_ext,x_atom,x_ids,m,y_scalar=preprocess(train_fname.with_suffix('.csv'),type_index=types,ext=ext)\n",
    "    np.savez(fname_ext(train_fname,ext,multi), \n",
    "             x_xyz=x_xyz,\n",
    "             x_type=x_type,\n",
    "             x_ext=x_ext,\n",
    "             x_atom=x_atom,\n",
    "             x_ids=x_ids,\n",
    "             y_scalar=y_scalar,\n",
    "             m=m)\n",
    "n_types = int(x_type[~np.isnan(x_type)].max() + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(85003, 3, 29),\n",
       " (85003, 1, 29, 29),\n",
       " (85003, 1, 29),\n",
       " (85003, 1, 29),\n",
       " (85003, 1, 29, 29),\n",
       " (85003,)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[v.shape for v in [x_xyz,x_type,x_ext,x_atom,\n",
    "                   y_scalar, m]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_xyz_mean, x_xyz_std = Tensor(x_xyz.mean(axis=(0,2),keepdims=True)), Tensor(x_xyz.std(axis=(0,2),keepdims=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 1])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_xyz_std.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fastai classes (this should should be done into its own `application` but who has time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MoleculeItem(ItemBase):\n",
    "    def __init__(self,i,xyz,type,ext,atom):#,qm9_mulliken,coulomb): \n",
    "        self.i,self.xyz,self.type,self.ext, self.atom = \\\n",
    "            i,xyz,type,ext,atom\n",
    "        self.data = [Tensor(xyz), LongTensor((type)), \n",
    "                     Tensor(ext), LongTensor((atom))]\n",
    "    def __str__(self):\n",
    "        # TODO: count n_atoms correctly. \n",
    "        n_atoms = np.count_nonzero(np.sum(np.absolute(self.xyz), axis=0))+1\n",
    "        n_couplings = np.sum((self.type!=-1))\n",
    "        return f'{self.i} {n_atoms} atoms {n_couplings} couplings'\n",
    "    \n",
    "    def apply_tfms(self, tfms:Collection, **kwargs):\n",
    "        x = self.clone()\n",
    "        for t in tfms:\n",
    "            if t: x.data = t(x.data)\n",
    "        return x\n",
    "    \n",
    "    def clone(self):\n",
    "        return self.__class__(self.i,self.xyz,self.type,self.ext,self.atom)#,self.qm9_mulliken,self.coulomb)\n",
    "    \n",
    "class ScalarCouplingItem(ItemBase):\n",
    "    def __init__(self,scalar,**kwargs): \n",
    "        self.scalar = scalar#,magnetic,mulliken,dipole,potential\n",
    "        self.data = Tensor(scalar) #, Tensor(magnetic), Tensor(dipole), Tensor(potential))\n",
    "    def __str__(self):\n",
    "        #res, spacer, n_couplings = '', '', 0\n",
    "        n_couplings = (~torch.isnan(self.data)).sum()\n",
    "        return f'{n_couplings}'\n",
    "    def apply_tfms(self, tfms:Collection, **kwargs):\n",
    "        y = self.clone()\n",
    "        for t in tfms:\n",
    "            if 'label_smoothing' == t.__name__:\n",
    "                if t: y.data = t(y.data)\n",
    "        return y\n",
    "    def clone(self): return self.__class__(self.scalar)#,self.magnetic,self.mulliken,self.dipole,self.potential)\n",
    "    def __hash__(self): return hash(str(self))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LMAEMaskedLoss(Module):\n",
    "    def __init__(self,types_w = [1]*n_types, return_all=False, proxy_log=torch.log, exclude_ext=False):\n",
    "        #self.contrib_w,self.magnetic_w,self.dipole_w,self.potential_w = contrib_w,magnetic_w,dipole_w,potential_w\n",
    "        self.types_w = types_w\n",
    "        self.return_all = return_all\n",
    "        self.proxy_log = proxy_log\n",
    "        self.exclude_ext = exclude_ext\n",
    "    \n",
    "    def forward(self, input_outputs, t_scalar):#, t_magnetic, t_dipole, t_potential):    \n",
    "        type, ext, p_scalar = input_outputs\n",
    "        loss = 0.\n",
    "        n = 0\n",
    "        j_loss = [0] * n_types\n",
    "        t_scalar = t_scalar.view(-1,1)\n",
    "        t_scalar = t_scalar[~torch.isnan(t_scalar)].unsqueeze(-1)\n",
    "\n",
    "        for t in range(n_types):\n",
    "            #if self.exclude_ext:\n",
    "            #    print(type.shape)\n",
    "            mask = (type == t) #if not self.exclude_ext else ((type == t) & (ext == 0)).squeeze(1)\n",
    "            if mask.sum() > 0:\n",
    "                _output,_target = p_scalar[mask], t_scalar[mask]\n",
    "                # LMAE scalar\n",
    "                s_loss = self.proxy_log((_output - _target).abs().mean()+1e-9)\n",
    "                loss += self.types_w[t] * s_loss\n",
    "                j_loss[t] += s_loss\n",
    "                n+=1\n",
    "        loss /= n\n",
    "        \n",
    "        return loss if not self.return_all else (loss, *j_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ScalarCouplingList(ItemList):\n",
    "    def __init__(self, items:Iterator, **kwargs):\n",
    "        super().__init__(items, **kwargs)\n",
    "        self.loss_func = LMAEMaskedLoss\n",
    "\n",
    "    def get(self, i):\n",
    "        o = super().get(i)\n",
    "        #print(len(o),o[0].shape,o[0].dtype)\n",
    "        return ScalarCouplingItem(np.array(o, dtype=np.float32))\n",
    "\n",
    "    def reconstruct(self,t): return 0; # TODO for viz !!!! ScalarCouplingItem(t.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quaterions allow us to rotate 3d points randoming with a nice uniform distribution of 3 numbers hece we use them, however it's still to be seen if are useful here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#https://github.com/facebookresearch/QuaterNet/blob/master/common/quaternion.py\n",
    "def qrot(q, v):\n",
    "    \"\"\"\n",
    "    Rotate vector(s) v about the rotation described by quaternion(s) q.\n",
    "    Expects a tensor of shape (*, 4) for q and a tensor of shape (*, 3) for v,\n",
    "    where * denotes any number of dimensions.\n",
    "    Returns a tensor of shape (*, 3).\n",
    "    \"\"\"\n",
    "    assert q.shape[-1] == 4\n",
    "    assert v.shape[-1] == 3\n",
    "    assert q.shape[:-1] == v.shape[:-1]\n",
    "    \n",
    "    original_shape = list(v.shape)\n",
    "    q = q.view(-1, 4)\n",
    "    v = v.view(-1, 3)\n",
    "    \n",
    "    qvec = q[:, 1:]\n",
    "    uv = torch.cross(qvec, v, dim=1)\n",
    "    uuv = torch.cross(qvec, uv, dim=1)\n",
    "    return (v + 2 * (q[:, :1] * uv + uuv)).view(original_shape)\n",
    "\n",
    "def random_rotation(data):\n",
    "    x_xyz = data[0].transpose(0,1)\n",
    "    r = torch.rand(3)\n",
    "    sq1_v1,sqv1,v2_2pi,v3_2pi = torch.sqrt(1-r[:1]),torch.sqrt(r[:1]),2*math.pi*r[1:2],2*math.pi*r[2:3]\n",
    "    q = torch.cat([sq1_v1*torch.sin(v2_2pi), sq1_v1*torch.cos(v2_2pi), \n",
    "                   sqv1  *torch.sin(v3_2pi), sqv1  *torch.cos(v3_2pi)], dim=0).unsqueeze(0)\n",
    "    x_xyz = qrot(q.expand(x_xyz.shape[0],-1), x_xyz).squeeze(0).transpose(0,1)\n",
    "    return (x_xyz, *data[1:])\n",
    "\n",
    "def normalize(data):\n",
    "    sq = False\n",
    "    if data[0].ndim < 3:\n",
    "        data[0].unsqueeze_(0)\n",
    "        #data[4].unsqueeze_(0)\n",
    "        sq = True\n",
    "    x_xyz      = (data[0] - x_xyz_mean)          / x_xyz_std\n",
    "    #x_mulliken = (data[4] - x_qm9_mulliken_mean) / x_qm9_mulliken_std\n",
    "    if sq:\n",
    "        x_xyz.squeeze_(0)\n",
    "        #x_mulliken.squeeze_(0)\n",
    "    return (x_xyz, *data[1:])\n",
    "\n",
    "def canonize(data):\n",
    "    xyz,type,ext,atom = data\n",
    "    mask = (atom == -1).squeeze(0)\n",
    "    i_max_atom=torch.nonzero(type != -1).max()\n",
    "    mask_atoms = torch.ones((max_atoms, max_atoms),dtype=torch.uint8)\n",
    "    zeros = torch.zeros((i_max_atom, i_max_atom), dtype=torch.uint8)\n",
    "    mask_atoms[:zeros.size(0),:zeros.size(1)] = zeros\n",
    "    n_atoms = mask.sum()\n",
    "    #xyz[:,mask], type[:,mask],ext[:,mask],atom[:,mask] = 0,-1,1,-1\n",
    "    return (xyz,type,ext,atom,mask_atoms.unsqueeze(0),n_atoms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build `data` bunch etc. for fastai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = ItemList(items=(MoleculeItem(i,*v) for i,v in \n",
    "                       enumerate(zip(x_xyz,x_type,x_ext,x_atom))),\n",
    "                label_cls=ScalarCouplingItem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "_, idx_valid_split = train_test_split(range(m.max()+1), test_size=0.1, random_state=13)\n",
    "idx_valid_split = np.argwhere(np.isin(m, idx_valid_split)).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [],
   "source": [
    "data = data.split_by_idx(idx_valid_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def l(o): return np.array(y_scalar[o.i], dtype=np.float32)\n",
    "data = data.label_from_func(func=l,label_cls=ScalarCouplingList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfms = [normalize, canonize]\n",
    "tta_tfms = list(tfms)\n",
    "tta_tfms.insert(0,random_rotation)\n",
    "data = data.transform((tta_tfms, tfms))#.transform_y(([label_smoothing], None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.databunch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data.train_ds.filter_by_func(lambda item, _: len(item.data[1].cpu().numpy()[item.data[1].cpu().numpy()==0]) == 0)\n",
    "#data.valid_ds.filter_by_func(lambda item, _: len(item.data[1].cpu().numpy()[item.data[1].cpu().numpy()==0]) == 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "Whole model here, self-contained (needs some cleanup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LMAEMetric(LearnerCallback):\n",
    "    _order=-20 # Needs to run before the recorder\n",
    "    def __init__(self, learn, val_only=True):\n",
    "        super().__init__(learn)\n",
    "        self.val_only=val_only\n",
    "        self.metric = LMAEMaskedLoss(return_all=True, exclude_ext=True)\n",
    "\n",
    "    def on_train_begin(self, **kwargs):\n",
    "        if not self.val_only: self.learn.recorder.add_metric_names(['tLMAE'])\n",
    "        self.learn.recorder.add_metric_names(['👉🏻LMAE👈🏻'] + [f'lmae{i}' for i in range(n_types)])\n",
    "            \n",
    "    def on_batch_end(self, train, last_output, last_target, **kwargs):\n",
    "        if self.val_only and train: return \n",
    "        preds,targs = self.preds[int(train)], self.targs[int(train)] # 0 val 1 train\n",
    "        \n",
    "        if preds is None:\n",
    "            targs, preds = last_target, listify(last_output)\n",
    "            targs,preds = targs.detach(),[t.detach() for t in preds]\n",
    "        else:\n",
    "            targs = torch.cat([targs,last_target.detach()], dim=0)\n",
    "            for i,(o,t) in enumerate(zip(last_output, last_target)):\n",
    "                preds[i] = torch.cat([preds[i], o.detach()], dim=0)\n",
    "        self.preds[int(train)], self.targs[int(train)] = preds,targs\n",
    "        \n",
    "    def on_epoch_begin(self, **kwargs):\n",
    "        self.targs, self.preds = [None, None], [None, None]\n",
    "\n",
    "    def on_epoch_end(self, last_metrics, **kwargs):\n",
    "        if True: #(kwargs['epoch'] % max_atoms) == 0:\n",
    "            mets = []\n",
    "            if self.preds[1]: mets.append(self.metric.forward(self.preds[1], self.targs[1])[0]) # just tLMAE\n",
    "            if self.preds[0]: \n",
    "                mets.extend(self.metric.forward(self.preds[0], self.targs[0]))\n",
    "            return add_metrics(last_metrics, mets) if mets else None\n",
    "        else: return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Activation = Enum('Activation', 'ReLU Swish GeLU')\n",
    "\n",
    "class PositionalEncoding(Module):\n",
    "    \"Encode the position with a sinusoid.\"\n",
    "    def __init__(self, d:int): self.register_buffer('freq', 1 / (10000 ** (torch.arange(0., d, 2.)/d)))\n",
    "\n",
    "    def forward(self, pos:Tensor):\n",
    "        inp = torch.ger(pos, self.freq)\n",
    "        enc = torch.cat([inp.sin(), inp.cos()], dim=-1)\n",
    "        return enc\n",
    "    \n",
    "class GeLU(Module):\n",
    "    def forward(self, x): return 0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n",
    "\n",
    "class Swish(Module):\n",
    "    def forward(self, x): return x * torch.sigmoid(x)\n",
    "\n",
    "_activ_func = {Activation.ReLU:nn.ReLU(inplace=True), Activation.GeLU:GeLU(), Activation.Swish: Swish()}\n",
    "\n",
    "def feed_forward(d_model:int,d_model_out:int, d_ff:int, ff_p:float=0.,\n",
    "                 act:Activation=Activation.ReLU, double_drop:bool=True):\n",
    "    if d_model==d_model_out:\n",
    "        layers = [nn.Linear(d_model, d_ff), _activ_func[act]]\n",
    "        if double_drop: layers.append(nn.Dropout(ff_p))\n",
    "        return SequentialEx(*layers,nn.Linear(d_ff,d_model_out),nn.Dropout(ff_p),MergeLayer(),nn.LayerNorm(d_model_out))\n",
    "    else:\n",
    "        return SequentialEx(nn.Linear(d_model,d_model_out))#,nn.Dropout(ff_p)),nn.LayerNorm(d_model_out))\n",
    "\n",
    "class MultiHeadAttention(Module):\n",
    "    \"MutiHeadAttention.\"\n",
    "    def __init__(self, n_heads:int, d_model:int, d_head:int=None, resid_p:float=0., attn_p:float=0., bias:bool=True,\n",
    "                 scale:bool=True):\n",
    "        d_head = ifnone(d_head, d_model//n_heads)\n",
    "        self.n_heads,self.d_head,self.scale = n_heads,d_head,scale\n",
    "        self.attention = nn.Linear(d_model, 3 * n_heads * d_head, bias=bias)\n",
    "        self.out = nn.Linear(n_heads * d_head, d_model, bias=bias)\n",
    "        self.drop_att,self.drop_res = nn.Dropout(attn_p),nn.Dropout(resid_p)\n",
    "        self.ln = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x:Tensor, mask:Tensor=None, **kwargs):\n",
    "        return self.ln(x + self.drop_res(self.out(self._apply_attention(x, mask=mask, **kwargs))))\n",
    "\n",
    "    def _apply_attention(self, x:Tensor, mask:Tensor=None):\n",
    "        bs,x_len = x.size(0),x.size(1)\n",
    "        wq,wk,wv = torch.chunk(self.attention(x), 3, dim=-1)\n",
    "        wq,wk,wv = map(lambda x:x.view(bs, x.size(1), self.n_heads, self.d_head), (wq,wk,wv))\n",
    "        wq,wk,wv = wq.permute(0, 2, 1, 3),wk.permute(0, 2, 3, 1),wv.permute(0, 2, 1, 3)\n",
    "        attn_score = torch.matmul(wq, wk)\n",
    "        if self.scale: attn_score.div_(self.d_head ** 0.5)\n",
    "        if mask is not None:\n",
    "            minus_inf = -65504 if attn_score.dtype == torch.float16 else -1e9\n",
    "            attn_score = attn_score.masked_fill(mask, minus_inf).type_as(attn_score)\n",
    "        attn_prob = self.drop_att(F.softmax(attn_score, dim=-1))\n",
    "        attn_vec = torch.matmul(attn_prob, wv)\n",
    "        return attn_vec.permute(0, 2, 1, 3).contiguous().contiguous().view(bs, x_len, -1)\n",
    "\n",
    "def _line_shift(x:Tensor, mask:bool=False):\n",
    "    \"Shift the line i of `x` by p-i elements to the left, is `mask` puts 0s on the diagonal.\"\n",
    "    bs,nh,n,p = x.size()\n",
    "    x_pad = torch.cat([x.new_zeros(bs,nh,n,1), x], dim=3)\n",
    "    x_shift = x_pad.view(bs,nh,p + 1,n)[:,:,1:].view_as(x)\n",
    "    if mask: x_shift.mul_(torch.tril(x.new_ones(n,p), p-n)[None,None,])\n",
    "    return x_shift\n",
    "\n",
    "class DecoderLayer(Module):\n",
    "    \"Basic block of a Transformer model.\"\n",
    "    #Can't use Sequential directly cause more than one input...\n",
    "    def __init__(self, n_heads:int, d_model:int, d_model_out:int,d_head:int, d_inner:int, \n",
    "                 resid_p:float=0., attn_p:float=0.,ff_p:float=0.,bias:bool=True, scale:bool=True, \n",
    "                 act:Activation=Activation.ReLU, double_drop:bool=True, attn_cls:Callable=MultiHeadAttention):\n",
    "        self.mhra = attn_cls(n_heads, d_model, d_head, resid_p=resid_p, attn_p=attn_p, bias=bias, scale=scale)\n",
    "        self.ff   = feed_forward(d_model, d_model_out, d_inner, ff_p=ff_p, act=act, double_drop=double_drop)\n",
    "\n",
    "    def forward(self, x:Tensor, mask:Tensor=None, **kwargs): return self.ff(self.mhra(x, mask=mask, **kwargs))\n",
    "\n",
    "class Transformer(Module):\n",
    "    \"Transformer model: https://arxiv.org/abs/1706.03762.\"\n",
    "    def __init__(self, n_layers:int, n_heads:int, d_model:int, d_model_out:int,d_head:int, d_inner:int,\n",
    "                 resid_p:float=0., attn_p:float=0., ff_p:float=0., embed_p:float=0., bias:bool=True, scale:bool=True,\n",
    "                 act:Activation=Activation.ReLU, double_drop:bool=True, attn_cls:Callable=MultiHeadAttention,\n",
    "                 learned_pos_enc:bool=True, mask:bool=True, dense_out:bool=False,final_p:float=0.):\n",
    "        self.mask = mask\n",
    "        self.drop_final = nn.Dropout(final_p)\n",
    "        self.dense_out = dense_out\n",
    "        self.layers = nn.ModuleList([DecoderLayer(\n",
    "            n_heads, d_model, d_model_out if k == n_layers-1 else d_model, d_head, d_inner, \n",
    "            resid_p=resid_p, attn_p=attn_p, ff_p=ff_p, bias=bias, scale=scale, act=act, double_drop=double_drop, \n",
    "            attn_cls=attn_cls) for k in range(n_layers)])\n",
    "\n",
    "    def reset(self): pass\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        inp = x\n",
    "        if self.dense_out: out = x.new()\n",
    "        for i,layer in enumerate(self.layers):\n",
    "            inp = layer(inp, mask=mask)\n",
    "            inp_d = self.drop_final(inp)\n",
    "            if self.dense_out: out = torch.cat([out, inp_d], dim=-1)\n",
    "        return out if self.dense_out else inp_d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoleculeTransformer(Module):\n",
    "    def __init__(self,n_layers,n_heads,d_model,embed_p:float=0,final_p:float=0,d_head=None,deep_decoder=False,\n",
    "                 dense_out=False, **kwargs):\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        d_head = ifnone(d_head, d_model//n_heads)\n",
    "        self.transformer = Transformer(n_layers=n_layers,n_heads=n_heads,d_model=d_model,d_model_out=d_model,\n",
    "                                       d_head=d_head,final_p=final_p,dense_out=dense_out,**kwargs)\n",
    "        \n",
    "        channels_out = d_model*n_layers if dense_out else d_model\n",
    "        channels_out_scalar = channels_out# + n_types + 1\n",
    "        if deep_decoder:\n",
    "            sl = [int(channels_out_scalar/(2**d)) for d in range(int(math.ceil(np.log2(channels_out_scalar/4)-1)))]\n",
    "            self.scalar = nn.Sequential(*(list(itertools.chain.from_iterable(\n",
    "                [[nn.Conv2d(sl[i],sl[i+1],1),nn.ReLU(),nn.BatchNorm2d(sl[i+1])] for i in range(len(sl)-1)])) + \n",
    "                [nn.Conv2d(sl[-1], 1, 1)]))\n",
    "        else:\n",
    "            self.scalar = nn.Conv1d(channels_out_scalar,max_atoms,1)\n",
    "        \n",
    "        n_atom_embedding = d_model  - 3 - max_atoms - ((n_types+1) * max_atoms) #//2\n",
    "        self.atom_embedding = nn.Embedding(len(atoms)+1,n_atom_embedding)\n",
    "        self.drop_atom = nn.Dropout(embed_p)\n",
    "        self.pos_enc = PositionalEncoding(d_model)\n",
    "\n",
    "\n",
    "    def forward(self,xyz,type,ext,atom,mask_atoms,n_atoms):\n",
    "        bs, _, n_pts = xyz.shape        \n",
    "        a = self.drop_atom(self.atom_embedding((atom+1).squeeze(1)))\n",
    "        x = xyz\n",
    "        t_one_hot=torch.zeros(bs,n_types+1,n_pts,n_pts,device=type.device,dtype=x.dtype).scatter_(1,type+1, 1.)\n",
    "        # bn,n_types+1,max_atoms(target),max_atoms(reference)\n",
    "        # print(type[0,:,9,1],t_one_hot[0,:,9,1])\n",
    "        t_one_hot=t_one_hot.view(bs,-1,n_pts)\n",
    "        pos = torch.arange(0, n_pts, device=x.device).expand(bs,1,n_pts)#, dtype=x.dtype)\n",
    "        pos_one_hot = torch.zeros(bs,max_atoms,n_pts,device=x.device,dtype=x.dtype)\n",
    "        pos_one_hot.scatter_(1,pos, 1)\n",
    "\n",
    "        x = torch.cat([x.transpose(1,2),a,pos_one_hot.transpose(1,2),t_one_hot.transpose(1,2)], dim=-1) \n",
    "        \n",
    "        x *= math.sqrt(self.d_model) # B,N(29),d_model\n",
    "        x = self.transformer(x,(mask_atoms == 1)).transpose(1,2)\n",
    "        \n",
    "        scalar    = self.scalar(x).view(bs,-1)\n",
    "        type = type.view(bs,1,-1)\n",
    "        ext  = ext.view(bs,1,-1)\n",
    "        mask= (type != -1).squeeze(1)\n",
    "        return type.transpose(1,2)[mask],ext,scalar[mask].unsqueeze(-1)\n",
    "    \n",
    "    def reset(self): pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This callback allows to insert multiple stateful (not averaged) metrics in one pass. Addditionally we could add metrics for train if we want to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model instantiation: where's all your TPUs/GPUs when you need a decent hyperparam sweep?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "net, learner = None,None\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "net = MoleculeTransformer(n_layers=6, n_heads=8*max_atoms,d_model=512,d_inner=2048*2,\n",
    "                      resid_p=0., attn_p=0., ff_p=0., embed_p=0, final_p=0.,\n",
    "                      deep_decoder=False, dense_out=False)\n",
    "learner = Learner(data,net, loss_func=LMAEMaskedLoss(),)\n",
    "learner.callbacks.extend([\n",
    "    SaveModelCallback(learner, monitor='👉🏻LMAE👈🏻', mode='min'),\n",
    "    LMAEMetric(learner)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MoleculeTransformer\n",
       "======================================================================\n",
       "Layer (type)         Output Shape         Param #    Trainable \n",
       "======================================================================\n",
       "Dropout              [29, 512]            0          False     \n",
       "______________________________________________________________________\n",
       "Linear               [29, 1392]           714,096    True      \n",
       "______________________________________________________________________\n",
       "Linear               [29, 512]            238,080    True      \n",
       "______________________________________________________________________\n",
       "Dropout              [232, 29, 29]        0          False     \n",
       "______________________________________________________________________\n",
       "Dropout              [29, 512]            0          False     \n",
       "______________________________________________________________________\n",
       "LayerNorm            [29, 512]            1,024      True      \n",
       "______________________________________________________________________\n",
       "Linear               [29, 4096]           2,101,248  True      \n",
       "______________________________________________________________________\n",
       "ReLU                 [29, 4096]           0          False     \n",
       "______________________________________________________________________\n",
       "Dropout              [29, 4096]           0          False     \n",
       "______________________________________________________________________\n",
       "Linear               [29, 512]            2,097,664  True      \n",
       "______________________________________________________________________\n",
       "Dropout              [29, 512]            0          False     \n",
       "______________________________________________________________________\n",
       "MergeLayer           [29, 512]            0          False     \n",
       "______________________________________________________________________\n",
       "LayerNorm            [29, 512]            1,024      True      \n",
       "______________________________________________________________________\n",
       "Linear               [29, 1392]           714,096    True      \n",
       "______________________________________________________________________\n",
       "Linear               [29, 512]            238,080    True      \n",
       "______________________________________________________________________\n",
       "Dropout              [232, 29, 29]        0          False     \n",
       "______________________________________________________________________\n",
       "Dropout              [29, 512]            0          False     \n",
       "______________________________________________________________________\n",
       "LayerNorm            [29, 512]            1,024      True      \n",
       "______________________________________________________________________\n",
       "Linear               [29, 4096]           2,101,248  True      \n",
       "______________________________________________________________________\n",
       "ReLU                 [29, 4096]           0          False     \n",
       "______________________________________________________________________\n",
       "Dropout              [29, 4096]           0          False     \n",
       "______________________________________________________________________\n",
       "Linear               [29, 512]            2,097,664  True      \n",
       "______________________________________________________________________\n",
       "Dropout              [29, 512]            0          False     \n",
       "______________________________________________________________________\n",
       "MergeLayer           [29, 512]            0          False     \n",
       "______________________________________________________________________\n",
       "LayerNorm            [29, 512]            1,024      True      \n",
       "______________________________________________________________________\n",
       "Linear               [29, 1392]           714,096    True      \n",
       "______________________________________________________________________\n",
       "Linear               [29, 512]            238,080    True      \n",
       "______________________________________________________________________\n",
       "Dropout              [232, 29, 29]        0          False     \n",
       "______________________________________________________________________\n",
       "Dropout              [29, 512]            0          False     \n",
       "______________________________________________________________________\n",
       "LayerNorm            [29, 512]            1,024      True      \n",
       "______________________________________________________________________\n",
       "Linear               [29, 4096]           2,101,248  True      \n",
       "______________________________________________________________________\n",
       "ReLU                 [29, 4096]           0          False     \n",
       "______________________________________________________________________\n",
       "Dropout              [29, 4096]           0          False     \n",
       "______________________________________________________________________\n",
       "Linear               [29, 512]            2,097,664  True      \n",
       "______________________________________________________________________\n",
       "Dropout              [29, 512]            0          False     \n",
       "______________________________________________________________________\n",
       "MergeLayer           [29, 512]            0          False     \n",
       "______________________________________________________________________\n",
       "LayerNorm            [29, 512]            1,024      True      \n",
       "______________________________________________________________________\n",
       "Linear               [29, 1392]           714,096    True      \n",
       "______________________________________________________________________\n",
       "Linear               [29, 512]            238,080    True      \n",
       "______________________________________________________________________\n",
       "Dropout              [232, 29, 29]        0          False     \n",
       "______________________________________________________________________\n",
       "Dropout              [29, 512]            0          False     \n",
       "______________________________________________________________________\n",
       "LayerNorm            [29, 512]            1,024      True      \n",
       "______________________________________________________________________\n",
       "Linear               [29, 4096]           2,101,248  True      \n",
       "______________________________________________________________________\n",
       "ReLU                 [29, 4096]           0          False     \n",
       "______________________________________________________________________\n",
       "Dropout              [29, 4096]           0          False     \n",
       "______________________________________________________________________\n",
       "Linear               [29, 512]            2,097,664  True      \n",
       "______________________________________________________________________\n",
       "Dropout              [29, 512]            0          False     \n",
       "______________________________________________________________________\n",
       "MergeLayer           [29, 512]            0          False     \n",
       "______________________________________________________________________\n",
       "LayerNorm            [29, 512]            1,024      True      \n",
       "______________________________________________________________________\n",
       "Linear               [29, 1392]           714,096    True      \n",
       "______________________________________________________________________\n",
       "Linear               [29, 512]            238,080    True      \n",
       "______________________________________________________________________\n",
       "Dropout              [232, 29, 29]        0          False     \n",
       "______________________________________________________________________\n",
       "Dropout              [29, 512]            0          False     \n",
       "______________________________________________________________________\n",
       "LayerNorm            [29, 512]            1,024      True      \n",
       "______________________________________________________________________\n",
       "Linear               [29, 4096]           2,101,248  True      \n",
       "______________________________________________________________________\n",
       "ReLU                 [29, 4096]           0          False     \n",
       "______________________________________________________________________\n",
       "Dropout              [29, 4096]           0          False     \n",
       "______________________________________________________________________\n",
       "Linear               [29, 512]            2,097,664  True      \n",
       "______________________________________________________________________\n",
       "Dropout              [29, 512]            0          False     \n",
       "______________________________________________________________________\n",
       "MergeLayer           [29, 512]            0          False     \n",
       "______________________________________________________________________\n",
       "LayerNorm            [29, 512]            1,024      True      \n",
       "______________________________________________________________________\n",
       "Linear               [29, 1392]           714,096    True      \n",
       "______________________________________________________________________\n",
       "Linear               [29, 512]            238,080    True      \n",
       "______________________________________________________________________\n",
       "Dropout              [232, 29, 29]        0          False     \n",
       "______________________________________________________________________\n",
       "Dropout              [29, 512]            0          False     \n",
       "______________________________________________________________________\n",
       "LayerNorm            [29, 512]            1,024      True      \n",
       "______________________________________________________________________\n",
       "Linear               [29, 4096]           2,101,248  True      \n",
       "______________________________________________________________________\n",
       "ReLU                 [29, 4096]           0          False     \n",
       "______________________________________________________________________\n",
       "Dropout              [29, 4096]           0          False     \n",
       "______________________________________________________________________\n",
       "Linear               [29, 512]            2,097,664  True      \n",
       "______________________________________________________________________\n",
       "Dropout              [29, 512]            0          False     \n",
       "______________________________________________________________________\n",
       "MergeLayer           [29, 512]            0          False     \n",
       "______________________________________________________________________\n",
       "LayerNorm            [29, 512]            1,024      True      \n",
       "______________________________________________________________________\n",
       "Conv1d               [29, 29]             14,877     True      \n",
       "______________________________________________________________________\n",
       "Embedding            [29, 219]            1,314      True      \n",
       "______________________________________________________________________\n",
       "Dropout              [29, 219]            0          False     \n",
       "______________________________________________________________________\n",
       "\n",
       "Total params: 30,935,007\n",
       "Total trainable params: 30,935,007\n",
       "Total non-trainable params: 0\n",
       "Optimized with 'torch.optim.adam.Adam', betas=(0.9, 0.99)\n",
       "Using true weight decay as discussed in https://www.fast.ai/2018/07/02/adam-weight-decay/ \n",
       "Loss function : LMAEMaskedLoss\n",
       "======================================================================\n",
       "Callbacks functions applied \n",
       "    SaveModelCallback\n",
       "    LMAEMetric"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MoleculeTransformer(\n",
       "  (transformer): Transformer(\n",
       "    (drop_final): Dropout(p=0.0)\n",
       "    (layers): ModuleList(\n",
       "      (0): DecoderLayer(\n",
       "        (mhra): MultiHeadAttention(\n",
       "          (attention): Linear(in_features=512, out_features=1392, bias=True)\n",
       "          (out): Linear(in_features=464, out_features=512, bias=True)\n",
       "          (drop_att): Dropout(p=0.0)\n",
       "          (drop_res): Dropout(p=0.0)\n",
       "          (ln): LayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=4096, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.0)\n",
       "            (3): Linear(in_features=4096, out_features=512, bias=True)\n",
       "            (4): Dropout(p=0.0)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): DecoderLayer(\n",
       "        (mhra): MultiHeadAttention(\n",
       "          (attention): Linear(in_features=512, out_features=1392, bias=True)\n",
       "          (out): Linear(in_features=464, out_features=512, bias=True)\n",
       "          (drop_att): Dropout(p=0.0)\n",
       "          (drop_res): Dropout(p=0.0)\n",
       "          (ln): LayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=4096, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.0)\n",
       "            (3): Linear(in_features=4096, out_features=512, bias=True)\n",
       "            (4): Dropout(p=0.0)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): DecoderLayer(\n",
       "        (mhra): MultiHeadAttention(\n",
       "          (attention): Linear(in_features=512, out_features=1392, bias=True)\n",
       "          (out): Linear(in_features=464, out_features=512, bias=True)\n",
       "          (drop_att): Dropout(p=0.0)\n",
       "          (drop_res): Dropout(p=0.0)\n",
       "          (ln): LayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=4096, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.0)\n",
       "            (3): Linear(in_features=4096, out_features=512, bias=True)\n",
       "            (4): Dropout(p=0.0)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): DecoderLayer(\n",
       "        (mhra): MultiHeadAttention(\n",
       "          (attention): Linear(in_features=512, out_features=1392, bias=True)\n",
       "          (out): Linear(in_features=464, out_features=512, bias=True)\n",
       "          (drop_att): Dropout(p=0.0)\n",
       "          (drop_res): Dropout(p=0.0)\n",
       "          (ln): LayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=4096, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.0)\n",
       "            (3): Linear(in_features=4096, out_features=512, bias=True)\n",
       "            (4): Dropout(p=0.0)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): DecoderLayer(\n",
       "        (mhra): MultiHeadAttention(\n",
       "          (attention): Linear(in_features=512, out_features=1392, bias=True)\n",
       "          (out): Linear(in_features=464, out_features=512, bias=True)\n",
       "          (drop_att): Dropout(p=0.0)\n",
       "          (drop_res): Dropout(p=0.0)\n",
       "          (ln): LayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=4096, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.0)\n",
       "            (3): Linear(in_features=4096, out_features=512, bias=True)\n",
       "            (4): Dropout(p=0.0)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): DecoderLayer(\n",
       "        (mhra): MultiHeadAttention(\n",
       "          (attention): Linear(in_features=512, out_features=1392, bias=True)\n",
       "          (out): Linear(in_features=464, out_features=512, bias=True)\n",
       "          (drop_att): Dropout(p=0.0)\n",
       "          (drop_res): Dropout(p=0.0)\n",
       "          (ln): LayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=4096, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.0)\n",
       "            (3): Linear(in_features=4096, out_features=512, bias=True)\n",
       "            (4): Dropout(p=0.0)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (scalar): Conv1d(512, 29, kernel_size=(1,), stride=(1,))\n",
       "  (atom_embedding): Embedding(6, 219)\n",
       "  (drop_atom): Dropout(p=0)\n",
       "  (pos_enc): PositionalEncoding()\n",
       ")"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sub_fname = \"loss-1.4032val-1.4586\" # uncomment or set None to skip loading trained net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOT loaded!  name 'sub_fname' is not defined\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print(f\"Attempting to load: {sub_fname}... \", end=\"\")\n",
    "    learner.load(sub_fname, strict=False,with_opt=False)\n",
    "    print(\"Loaded\")\n",
    "except Exception as e:\n",
    "    print(\"NOT loaded! \", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# neural weight manual transplant \n",
    "if False:\n",
    "    for name,child in learner.model.named_children():\n",
    "        print(\"CHILD: \",name)\n",
    "        if not (name in ['scalar', 'magnetic', 'dipole', 'potential']):\n",
    "            print(\"FREEZING\")\n",
    "            for param in child.parameters(): param.requires_grad = False\n",
    "        else:\n",
    "            for name,param in child.named_parameters(): \n",
    "                param.requires_grad = True\n",
    "    learner.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learner = learner.to_parallel()#.to_fp16()#loss_scale=128, dynamic=False)\n",
    "data.batch_size = int(2048//2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Real loss func. Need to test different auxiliary tasks weights: `magnetic_w`, `dipole_w`, `potential`, weights of indivial `lmae`s: `types_w`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learner.loss_func = LMAEMaskedLoss(types_w = [1] + [1] * (n_types-1))#, proxy_log=lambda x: x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n",
      "Min numerical gradient: 6.92E-06\n",
      "Min loss divided by 10: 7.59E-04\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEGCAYAAAB/+QKOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAgAElEQVR4nO3deXxcd3nv8c+jXdZuS5Ztyba8Z/cSZ8MkZIFsBJJQ4EULKQnQtEBIAgncFu6lLVxaWi7Q0lJCGkJLIGxZIIGE4JINZ/Vux3bi2LFia7Etydosa5/n/jFHzlgZ2XKsM4v0fb9e89LM7/zOzKPxeB79zm8zd0dERGS4jGQHICIiqUkJQkRE4lKCEBGRuJQgREQkLiUIERGJKyvZAYyl8vJyr6mpSXYYIiJpY+3atc3uXhHv2LhKEDU1NaxZsybZYYiIpA0ze32kY7rEJCIicSlBiIhIXEoQIiISlxKEiIjEpQQhIiJxKUGIiEhcShAiIhKXEoSISBpbuXUf339qZyjPrQQhIpLGHt3cyI+eG3Gu2wlRghARSWP1bd3MKM0L5bmVIERE0lhDezdVpfmhPLcShIhImhqMOI1tPcxQghARkVhNnb0MRFwJQkREjlTf1g1AVZkShIiIxDicINSCEBGRWA1BgtAlJhEROUJDWzcl+dkU5oaz91toCcLMZprZE2a2zcy2mNktcep82Mw2BbdnzWxxzLHLzewVM9thZn8dVpwiIumqvrU7tNYDhLvl6ABwm7uvM7MiYK2ZrXT3rTF1dgHvcPdWM7sCuBM4x8wyge8C7wLqgNVm9tCwc0VEJrT6tm6qQ+qghhBbEO7e6O7rgvudwDagalidZ929NXj4PFAd3D8b2OHur7l7H/Az4OqwYhURSUf1beFNkoME9UGYWQ2wFHjhKNU+Djwa3K8C9sQcq2NYchERmcg6evrp7BlI20tMAJhZIXA/cKu7d4xQ5yKiCeLtQ0VxqvkI594I3Agwa9asE45XRCQdNLb1AOGNYIKQWxBmlk00OfzE3R8Yoc4ZwF3A1e7eEhTXATNjqlUDDfHOd/c73X25uy+vqKgYu+BFRFJYfdshILxJchDuKCYDfgBsc/dvjVBnFvAAcJ27b485tBpYYGZzzCwH+BDwUFixioikm/qgBRFmH0SYl5hWANcBm81sQ1D2RWAWgLvfAXwZmAL8RzSfMBC0BgbM7CbgMSATuNvdt4QYq4hIWmlo6yY706gozA3tNUJLEO6+ivh9CbF1PgF8YoRjjwCPhBCaiEjaq2/tZnpJPhkZR/2aPSGaSS0ikoYaQtwoaIgShIhIGoomiPD6H0AJQkQk7fQPRtjb0UO1EoSIiMTa19FDxMOdAwFKECIiaachAZPkQAlCRCTtJGKSHChBiIikncMtiBIlCBERiVHf1s3kghzyczJDfR0lCBGRNFPfGu4y30OUIERE0kwiJsmBEoSISFpx94RMkgMlCBGRtNLe3U9X36AuMYmIyJHq27qB8OdAgBKEiEha2d/ZC0BlsfogREQkRlOQIKYWhbcPxBAlCBGRNDKUIMpD3ChoiBKEiEgaaerspSg3K/RJcqAEISKSVpoO9lKRgMtLoAQhIpJWmjp7KVeCEBGR4Zo71YIQEZE4mjp7qUhABzUoQYiIpI3uvkE6ewfUghARkSM1H4wOcVWCEBGRIwzNolaCEBGRIxxuQagPQkREYjWpBSEiIvE0dfZiBpMLchLyekoQIiJpoulgL5Mn5ZCdmZivbiUIEZE00ZTASXIQYoIws5lm9oSZbTOzLWZ2S5w6J5nZc2bWa2a3DztWa2abzWyDma0JK04RkXSR6ASRFeJzDwC3ufs6MysC1prZSnffGlPnAHAzcM0Iz3GRuzeHGKOISNpo6uxlbnlBwl4vtBaEuze6+7rgfiewDagaVme/u68G+sOKQ0RkPHD3hK7kCgnqgzCzGmAp8MJxnObA781srZndeJTnvtHM1pjZmqamphMLVEQkRXX0DNA3EBlfCcLMCoH7gVvdveM4Tl3h7suAK4BPm9kF8Sq5+53uvtzdl1dUVIxBxCIiqSfRcyAg5ARhZtlEk8NP3P2B4znX3RuCn/uBB4Gzxz5CEZH0cDhBJGgWNYQ7ismAHwDb3P1bx3luQdCxjZkVAJcCL419lCIi6aEpwQv1QbijmFYA1wGbzWxDUPZFYBaAu99hZtOANUAxEDGzW4FTgHLgwWiOIQu4191/F2KsIiIpLRmXmEJLEO6+CrBj1NkLVMc51AEsDiMuEZF01NTZS3amUZKfnbDX1ExqEZE00NTZS3lhLsGVlYRQghARSQPNCZ4DAUoQIiJpIZF7UQ9RghARSQOJnkUNShAiIilvMOK0KEGIiMhwB7r6iHhih7iCEoSISMpLxixqUIIQEUl5yZhFDUoQIiIpLxmzqEEJQkQk5Q0liHJdYhIRkVhNnb0U5GRSkBvm8nlvpgQhIpLikjEHApQgRERSXlNnT8IvL4EShIhIytvfoRaEiIgMMxhx6lq7mTV5UsJfWwlCRCSFNbR10zcYoaa8IOGvrQQhIpLCXm85BEDNFCUIERGJsaulC4Cacl1iEhGRGLXNXeRlZ1BZlJfw11aCEBFJYa+3dDF7cgEZGYnbanSIEoSISArb1dyVlMtLoAQhIpKyBiPOngPdSemgBiUIEZGUlcwhrqAEISKSsoaGuM6eoktMIiISY2iI6xy1IEREJNbrzV3kZiVniCsoQYiIpKzali5qpiRniCsoQYiIpKzalkNJ63+AEBOEmc00syfMbJuZbTGzW+LUOcnMnjOzXjO7fdixy83sFTPbYWZ/HVacIiKpaDDi7G45lLT+B4Aw968bAG5z93VmVgSsNbOV7r41ps4B4GbgmtgTzSwT+C7wLqAOWG1mDw07V0Rk3Gpsjw5xnZ2kORAQYgvC3RvdfV1wvxPYBlQNq7Pf3VcD/cNOPxvY4e6vuXsf8DPg6rBiFRFJNbXNwSquSZpFDQnqgzCzGmAp8MIoT6kC9sQ8rmNYcol57hvNbI2ZrWlqajqRMEVEUkZtkoe4QgIShJkVAvcDt7p7x2hPi1Pm8Sq6+53uvtzdl1dUVLzVMEVEUkptkoe4QsgJwsyyiSaHn7j7A8dxah0wM+ZxNdAwlrGJiKSy2pZDSR3iCuGOYjLgB8A2d//WcZ6+GlhgZnPMLAf4EPDQWMcoIpKqalu6kjrEFUY5isnM5gF17t5rZhcCZwA/cve2o5y2ArgO2GxmG4KyLwKzANz9DjObBqwBioGImd0KnOLuHWZ2E/AYkAnc7e5bjv/XExFJP0NDXC85aWpS4xjtMNf7geVmNp9oq+Ah4F7gypFOcPdVxO9LiK2zl+jlo3jHHgEeGWV8IiLjRioMcYXRX2KKuPsAcC3wL+7+WWB6eGGJiExcQ6u4JnOIK4w+QfSb2Z8CHwV+E5RlhxOSiMjEtqs5OsQ1WRsFDRltgrgBOA/4mrvvMrM5wI/DC0tEZOLa03qInMwMphUnb4grjLIPIlji4mYAMysDitz962EGJiIyUdW3djO9NC+pQ1xhlC0IM3vSzIrNbDKwEfihmR3v0FURERmF+rZuqkrzkx3GqC8xlQSzoN8H/NDdzwTeGV5YIiITV31reiWILDObDnyQNzqpRURkjPUODLK/s5eqsvRJEF8hOmltp7uvNrO5wKvhhSUiMjE1tvUApEQLYrSd1L8Efhnz+DXgT8IKSkRkoqpv6wZInxaEmVWb2YNmtt/M9pnZ/WYWdwa0iIi8dfWt0QRRXZrcSXIw+ktMPyS6vMYMovsyPByUiYjIGKpv68YMppUkdw4EjD5BVLj7D919ILj9F6DNF0RExlh9WzeVRXnkZCVkP7ejGm0EzWb2ETPLDG4fAVrCDExEZCKqb+1mRmnyWw8w+gTxMaJDXPcCjcD7iS6/ISIiY6i+rZuqsuT3P8AoE4S773b397p7hbtPdfdriE6aExGRMRKJOI3tqTFJDk5sR7nPjVkUIiLC/s5e+gc9JYa4wokliOSuIiUiMs7Ut0X3gageBy0IH7MoRESEutbUmSQHx5hJbWadxE8EBqTGbyAiMk4cnkWdIi2IoyYIdy9KVCAiIhNdfWs3pZOyKcgd1SpIoUv+TAwREQFSZx+IIUoQIiIpIlX2gRiiBCEikgLcnYa27pTpoAYlCBGRlNDe3U9X36BaECIicqShIa7VakGIiEisN4a4psY6TKAEISKSEupTbJIcKEGIiKSE+rZu8rIzKJuUnexQDgstQZjZTDN7wsy2mdkWM7slTh0zs++Y2Q4z22Rmy2KODZrZhuD2UFhxioikgqEhrmaps8xdmNP1BoDb3H2dmRUBa81spbtvjalzBbAguJ0DfC/4CdDt7ktCjE9EJGWk0j4QQ0JrQbh7o7uvC+53AtuI7mcd62rgRx71PFBqZtPDiklEJFWl2ixqSFAfhJnVAEuBF4YdqgL2xDyu440kkmdma8zseTO75ijPfWNQb01TU9MYRi0ikhjdfYMc6OpLqSGukIAEYWaFwP3Are7eMfxwnFOGVo+d5e7LgT8D/sXM5sV7fne/092Xu/vyioqKMYtbRCRRtu2NfjXOqyhMciRHCjVBmFk20eTwE3d/IE6VOmBmzONqoAHA3Yd+vgY8SbQFIiIy7qzf3QbA0lmlSY7kSGGOYjLgB8A2d//WCNUeAv48GM10LtDu7o1mVmZmucHzlAMrgK0jPIeISFpbv7uVqtJ8Kovzkh3KEcIcxbQCuA7YbGYbgrIvArMA3P0O4BHgSmAHcAi4Iah3MvB9M4sQTWJfHzb6SURk3Fi/u40lKdZ6gBAThLuv4hj7Vru7A5+OU/4scHpIoYmIpIz9HT3Ut3Vzw4qaZIfyJppJLSKSROv3DPU/lCU5kjdTghARSaL1u9vIzjROnVGc7FDeRAlCRCSJ1u9u5ZQZJeRlZyY7lDdRghARSZKBwQib6tpZOjP1OqhBCUJEJGle2ddJd/9gys1/GKIEISKSJIcnyM1MvQ5qUIIQEUmaDXvamFKQw8zJqbUG0xAlCBGRJFm/u5Wls0pTag+IWEoQIiJJ0H6on51NXSk5/2GIEoSISBJsqBvqf0jNDmpQghARSYr1u1sxgzOUIEREJNaqV5tZVFlEYW6Ya6aeGCUIEZEEe6m+nTWvt/L+M6uTHcpRKUGIiCTY3c/soiAnkw+eNfPYlZModds2CfSL1XsoysuisiSPyuI8phblkp2p3CkiY29/Rw8Pb2zgw+fMpjgvO9nhHNWETxCRiPOlX22mf9APl2UYTC/Jp7osn1mTJzFvaiGnzSjh1BnFlBXkJDFaEUl3P37+dQYizvVvq0l2KMc04ROEGTz/N5ewt6OHfR097G3vZW97N3tau9lz4BBPbW/il2vrDtevKs3npGlFLJxWxKLKIk6rKmZeRWHKTnQRkdTR0z/Ij1/YzSUnVVJTXpDscI5JCcKMKYW5TCnM5dQZJXHrtB3qY0tDBy/Vt/NSQwfb93by1PYmBiLRVseCqYW8d/EM3rN4Rlr8o4tIcjy0oYEDXX187O01yQ5lVCy66+f4sHz5cl+zZk1CXqtvIEJtSxcvvNbCwxsbebH2AABzywtYPLOUxdUlLJlVxmkzislSf4bIhOfuXPGvf8TMeOTmt6fMVQczW+vuy+Mdm/AtiLcqJyuDhZVFLKws4rrzamho6+aRzY28sOsAq3Y08+D6egCK87K4YGEFFy2ayoWLKphSmJvkyEUkGV7YdYCX93byjfefkTLJ4ViUIMbIjNJ8PnH+XD5x/lzcnb0dPax9vZWnXmniye1N/GZTIzmZGVy7tIq/uGAu86cWJjtkEUmgx1/eT05mBu8+Y3qyQxk1JYgQmBnTS/K56ox8rjpjBpGIs6Whg5+v2c0v19Tx8zV7eOfJU7nuvBrePr+czIz0+GtCRN66Va82s2x2KZNy0udrN30iTWMZGcbp1SWcXn06n33nQv77ude557la/mfbfiqLc7lmSRXvP7OaBZVFyQ5VRELQcrCXrY0dfP6yRckO5bgoQSTYlMJcPveuhXzqwnk8/vJ+7l9bx12rdvH9p1/jHQsr+OSF8zhnzuS0uUYpIsf27M4WAFbML09yJMdHCSJJ8rIzufL06Vx5+nSaD/by89V7uHvVLj505/Msm1XKZy5ewIWLKpQoRMaBZ3Y0U5SXxelV8YfSpyqNv0wB5YW5fPqi+az6XxfzlatPZV9HLzf812o++P3nWB0MnxWR9OTu/PHVZt42b0ra9TcqQaSQ/JxM/vy8Gp64/UK+es1p1LYc4gN3PMfH/2s12xo7jqy8cyd86lNQXAwZGdGfn/pUtFxEUsbuA4eob+vm7Wl2eQmUIFJSTlYG1507m6c+fyFfuHwRL9Ye4Mrv/JGbf7qeXc1d8OijcMYZcNdd0NkJ7tGfd90VLX/00WT/CiISWLWjGUi//gcIMUGY2Uwze8LMtpnZFjO7JU4dM7PvmNkOM9tkZstijn3UzF4Nbh8NK85UNikni09dOJ9VX7iYT75jHiu37uNj/+dn9F37Pjh0CPr7jzyhvz9a/v73qyUhkiKe2dHMjJI85qThMjxhtiAGgNvc/WTgXODTZnbKsDpXAAuC243A9wDMbDLwt8A5wNnA35pZ6u7sHbKSSdl84fKTeOoLF/JPtSvfnBiG6++Hb387McGJyIgGI86zO1tYMb88LQechJYg3L3R3dcF9zuBbUDVsGpXAz/yqOeBUjObDlwGrHT3A+7eCqwELg8r1nQxtSiPs595hJzI4NEr9vfDPfckJigRGdHWhg7aDvXz9gXpd3kJEtQHYWY1wFLghWGHqoA9MY/rgrKRyuM9941mtsbM1jQ1NY1VyKnr4MGxrScioRnqf3jbPCWIuMysELgfuNXdO4YfjnOKH6X8zYXud7r7cndfXlFRcWLBpoPCUa7hNNp6IhKaZ3Y0c9K0IiqK0nORzlAThJllE00OP3H3B+JUqQNiN2WtBhqOUi4f+QhkH32bwr6MTB4/61I27mlLUFAiMlxHTz8v1h5Iy9FLQ8IcxWTAD4Bt7v6tEao9BPx5MJrpXKDd3RuBx4BLzaws6Jy+NCiT2247ZoKw7By+eeq7ufq7z/CRu17gN5sa6B04Rr+FiIyp3720l76BCFel0eqtw4W51MYK4Dpgs5ltCMq+CMwCcPc7gEeAK4EdwCHghuDYATP7KrA6OO8r7q4pxQDz5sF990WHsvb3HzmiKTsbsrPJvu8+fn7xu7gnWBTwpnvXUzYpm2uWVvHB5TM5eXpx0sIXmSh+vaGe2VMmsWRmabJDecu0o1y62rkzOpT1nnuiHdKFhXDddfDZz0aTSGAw4qza0cwv1uxh5ZZ99A1GOK2qmPcvq+a9S6qYXJCTxF9CZHza297DeV//A5+5eAGfe9fCZIdzVEfbUU4JYgJp7erj1xvquW9dHS/Vd5BhMH9qIadXlbJ4Zglnz5nMosqitByvLZJK/vPp1/jaI9t4/LZ3MLcitQeMaMtRAaCsIIfrV8zh+hVz2NbYwWNb9rKprp2ntu/n/nV1AMyeMonLT53GZadNY3F1adotLiaSCn61oZ7F1SUpnxyORQligjp5evHhvgh3p76tm6e3N/O7LXv5QbA/RUl+NufOncyK+eWcN3cK8yoKyVDCEDmqV/d1sqWhgy9fNXzhiPSjBCGYGdVlk/izc2bxZ+fMor27nydf2c8zO5p5ZkcLj23ZB0BJfjbLZpVy5uwyzpk7hSUzS8nO1HqPIrF+taGezAzjPYtnJDuUE6YEIW9Skp/N1UuquHpJFe7OngPdPP9aC+t2t7JudytPvBKdsV6Ym8V586ZwwYJyLj11GpXFeUmOXCS5IhHn1xsaWDG/PG0nx8VSgpCjMjNmTZnErCmT+OBZ0bmL7Yf6ee61Fv74ahNPv9rEyq37+PJDWzhr9mSuWjydy0+dxlQlC5mA1u5upa61O+VHLo2WRjHJCdux/yC/3dTIbzc3sH1fdA2oU2cUc9GiqVy4qIKls8rU2S3jXk//INf/8EU27mln9f9+J4W56fH3t4a5SsJs39fJyq37ePKV/azb3cZgxKkoyuXK06bx7jNmsHx2mTq6ZdzpG4jwyR+v5Q8v7+dbH1zM+5ZVJzukUVOCkKRoP9TPU6828ejmRh5/eT+9AxGmFedx5enTec/i6SyZWao5F5L2BgYj3PKzDfx2cyP/95rT+Mi5s5Md0nFRgpCkO9g7wB+27ePhjY08vb2JvsEI1WX5XLu0ig+fM5tpJeqzkPQTiTifv28T96+r43+/+2Q+cf7cZId03JQgJKW0d/ezcus+Ht7YwNOvNpFpxmWnTeP6t9WwfHaZWhWSNu58eif/8MjL3PrOBdz6zvTsmFaCkJS1u+UQ9zxfy89X76GjZ4CZk/O5/NRpXHH6dJZUl6q/QlLW5rp23ve9Z7jkpEq+95FlafuHjRKEpLxDfQP8ZlMjj25uZNWOZvoHnRkleVy7rIo/WVad9ksWyPjS1TvAVf+2ip7+QR695XxKJ6XvopdKEJJW2rv7efzlffx6QwNPb28i4rB8dhnXLqviytOmU6YVaCXJbv/lRu5fV8dP/+Jczp07JdnhnBAlCElb+zp6eHB9PfevrePV/QfJyjDOX1DOexbP4B0LK5hSmP6zVSW9PLSxgZt/up6bLprP7ZctSnY4J0wJQtKeu7O1sYOHNjbwm42N1Ld1A3DStCJWzC9nxfwpnD1nStpMTpL0VNvcxVX/tooFlYX84i/PGxdrkSlByLgSiTib69tZtaOZZ3Y0s+b1VvoGImRlGItnlvK2eVM4f0EFS2dpMUEZOz39g7zvP56lvq2b3978dqrLJiU7pDGhBCHjWk//IGtfb+XZndHVZzfVtRFxKMrN4m3zp3DhoqlccvJUphZproW8dV98cDP3vrCbH3x0OZecXJnscMaMNgyScS0vOzO4zFTO5y+LdnI/t7OZp7Y38fT2Zh7bsg8zWDarjMtOreSSkyuZW16QtsMSJfF+vaGee1/YzV++Y+64Sg7HohaEjGvuzvZ9B3lsy14e27KXLQ0dAFSV5nPBwnIuWFDBmTVlal3IiLY0tPOBO57jlOnF/PTGc8fdZUtdYhIJ7DlwiCe3N/HH7U08u7OFg70DQDRhLJ1VyrJZZayYX87CykK1MCY4d+dnq/fwdw9toSQ/m1/ftILpJfnJDmvMKUGIxNE/GGFTXRvrd7exfk8bG3a3HR4dVVGUy4p5UzhvXnR0VM2USUoYE0hHTz9/88BmfrupkfMXlPPNDy4et61M9UGIxJGdmcGZsydz5uzJh8vq27qDrVabWbWjmV9taACiCePsOZM5u2YyZ9VMZtG0Iu1xMc4c6hvg+ddaeOqVJn63ZS/NB/v4wuWL+KsL5k3YJV/UghAZgbuzs6mLF3cd4IVdLby46wCN7T0AFOVlcebsMpbPLmPZ7DIWV5dSoDkYKcndqWvtZnXtAXY1d1HX2s2eA4fY19lDJAJm0du+9l76BiPkZ2dy7tzJ3HTxAs6cXZbs8EOnS0wiYyD2i2Z17QHWvt56eAe9zAzjjOoS3h6Mplo6q5TcrMwkRzyxRCJOQ3s39a3dNLR309DWw/Z9nUck9swMY3pJHtVl+Uwrzou2DBwcmFqUy/kLKlheU0Ze9sT5t1OCEAlJ+6F+1u1pZW1tdB7Gxrp2BiNOXnYGS2eWcVZNGWfNmcySmaUU5WUnO9xxY2h02rM7m3mpvoNX93eyY/9BDvUNHlGvsjiXs2omc/ac6KXBBVMLyRpno5BOlBKESIJ09PTzwmsHeHZnM6trD7C1oYOIRy9hzC0vYHF1KadWlVCSn012ppGTmUFeTiblBbmUF+UwpSCXnCx9gQ1xdw71DbKn9RC1zV3Uthxia0MHz+5soflgLxDtH1pUWcSCykIWTC1i5uR8ZpTmM6Mkn/ycidMSeKuUIESS5GDvAOteb2XDnjY21bWxsa6dps7eo54zoySPBZVFLJpWxMLKIk6dUcz8qYXjZvy9uzMYccyMDAMzo7tvkPV7Wnlx1wFe3HWA2uYuOnsH6OodIDLsK2pqUS7nzZvCinnlnDdvCjMnj48lL5JFo5hEkqQwN4sLFlZwwcKKw2XNB3vp7hukbzBC/2CErt5BWg720nywj6bOXmpbunhlbyfPvdZC30AEgJysDE6aVsRJ04qoKS9gbnkBc8oLqSmflJS+joHBCB09Axzo6mNfRw9723vY29FDV+8ADrjDYCTC3o5e6lsPUd/WTWtXPwORyJu+8INuADxoaZ0yvZjz5pVTlJdFYW4WBblZVJXlM2dKAbPLJ1GsS3UJE1qCMLO7gauA/e5+WpzjZcDdwDygB/iYu78UHKsFOoFBYGCk7CaSjspHuUT5wGCE2pYutjR0sKWhg5fq23n85SaaD9YdrpOVYcyrKGTRtCIWTC2ksjiPiqJcKopyKS/MZXJBzuFLVrXNXTz+8n6eeGU/O/cfpCgvm5L8bIrzs3F3uvoGONQ3SHffIBlmZGQYmRnRL+6+gQh9gxF6+yN09vTTNexaf2w8ZmAYGRkwtSjaIXzBggrKi3LJyjAyzMjMsGgScScScTIzjMUzSzhz9mRK8pUAUkVol5jM7ALgIPCjERLEN4CD7v73ZnYS8F13vyQ4Vgssd/fm43lNXWKSiaCzp5/a5kO81nyQ7fs6ebmxk5f3dh6e5DdccV4W+TmZ7OuIXtqaV1HAGdWldPUO0N7dT3t3P5kZRkFOFgW5meRlZ+IOAxEn4o4BudkZ5GRmkJOV8UZiycuirCCHqUV5TC/Jo7I4T9f801BSLjG5+9NmVnOUKqcA/xjUfdnMasys0t33hRWTyHhQlJfN6dUlnF5dckR5d98gzQd7aTrYy/6OXlq6emk52EfLwV46egY4o7qEi0+ayuwpBUmKXNJNMvsgNgLvA1aZ2dnAbKAa2Ef0kuTvzcyB77v7nSM9iZndCNwIMGvWrNCDFklV+TmZzJw8SZ22MmaSOSzi60CZmW0APgOsBwaCYyvcfRlwBfDp4HJVXO5+p7svd/flFRUVI1UTEZHjlLQWhLt3ADcAWHQVtF3BDXdvCH7uN7MHgbOBp5MUqojIhJS0FoSZlZpZTvDwE8DT7t5hZlP+uGgAAAhRSURBVAVmVhTUKQAuBV5KVpwiIhNVmMNcfwpcCJSbWR3wt0A2gLvfAZwM/MjMBoGtwMeDUyuBB4OllbOAe939d2HFKSIi8YU5iulPj3H8OWBBnPLXgMVhxSUiIqMzPubui4jImFOCEBGRuJQgREQkrnG1mquZtQOvxjlUArSP8vHQ/Xhl5cBxLf8R57VGezxeebyYRrp/IjEfLa7RxpcuMccrT8fPx2hijr2vz8foj4/3z8cCdz9yWv4Qdx83N+DO0ZQf7fHQ/RHK1oxVTMcb80gxHSv+txLzW407HWMeL5+P0cSc7Pdan4/U/3wMv423S0wPj7L8aI8fPkrZWMZ0rOPxykeK6VjxvxVvJe50jDleeTp+PkYTc+x9fT5Gf3wifT6OMK4uMYXNzNZ4mi09rpgTJx3jVsyJk45xj7cWRNhGXDQwhSnmxEnHuBVz4qRd3GpBiIhIXGpBiIhIXEoQIiIS14RNEGZ2t5ntN7PjXinWzM40s81mtsPMvhMsVz507DNm9oqZbTGzf071mM3s78ys3sw2BLcrUz3mmOO3m5mbWfnYRXz4ucN4r79qZpuC9/n3ZjYjDWL+hpm9HMT9oJmVpkHMHwj+/0XMbMw6hU8k1hGe76Nm9mpw+2hM+VE/9wn1VsYTj4cbcAGwDHjpLZz7InAeYMCjwBVB+UXA/wC5weOpaRDz3wG3p9P7HBybCTwGvA6Up0PcQHFMnZuBO9Ig5kuBrOD+PwH/lAYxnwwsAp4kurd9UmMN4qgZVjYZeC34WRbcLzva75WM24RtQbj708CB2DIzm2dmvzOztWb2RzM7afh5Zjad6H/05zz6r/kj4Jrg8CeBr7t7b/Aa+9Mg5lCFGPO3gS8Q3Z42LeL26CZZQwrGOvaQYv69uw/t9Pg80W2BUz3mbe7+yljGeSKxjuAyYKW7H3D3VmAlcHky/6/GM2ETxAjuBD7j7mcCtwP/EadOFVAX87guKANYCJxvZi+Y2VNmdlao0UadaMwANwWXEO42s7LwQj3shGI2s/cC9e6+MexAhznh99rMvmZme4APA18OMdYhY/H5GPIxon/Rhm0sYw7baGKNpwrYE/N4KP5U+b2AJG45mmrMrBB4G/DLmEt+ufGqxikb+kswi2hz8VzgLOAXZjY3+EtgzI1RzN8Dvho8/irwTaJfBKE40ZjNbBLwJaKXPhJmjN5r3P1LwJfM7G+Am4hupBWKsYo5eK4vEd0z/idjGeObAhnDmMN2tFjN7AbglqBsPvCImfUBu9z9WkaOP+m/VywliDdkAG3uviS20MwygbXBw4eIfqHGNrOrgYbgfh3wQJAQXjSzCNEFuppSNWZ33xdz3n8Cvwkp1iEnGvM8YA6wMfhPWQ2sM7Oz3X1vCsc93L3AbwkxQTBGMQcdqFcBl4T1x06MsX6fwxQ3VgB3/yHwQwAzexK43t1rY6rUEd1xc0g10b6KOpL/e70hWZ0fqXADaojpcAKeBT4Q3Ddg8QjnrSbaShjqRLoyKP8r4CvB/YVEm5CW4jFPj6nzWeBnqf4+D6tTSwid1CG91wti6nwGuC8NYr6c6JbAFWG8x2F+PhjjTuq3Gisjd1LvInrFoSy4P3m0n/tE3ZLyoqlwA34KNAL9RLP2x4n+Zfo7YGPwn+LLI5y7HHgJ2An8O2/MSM8BfhwcWwdcnAYx3wNsBjYR/ctseqrHPKxOLeGMYgrjvb4/KN9EdIG0qjSIeQfRP3Q2BLexHnkVRszXBs/VC+wDHktmrMRJEEH5x4L3dwdww/F87hN101IbIiISl0YxiYhIXEoQIiISlxKEiIjEpQQhIiJxKUGIiEhcShAyrpnZwQS/3l1mdsoYPdegRVd+fcnMHj7WSqpmVmpmnxqL1xYB7Sgn45yZHXT3wjF8vix/Y/G6UMXGbmb/DWx3968dpX4N8Bt3Py0R8cn4pxaETDhmVmFm95vZ6uC2Iig/28yeNbP1wc9FQfn1ZvZLM3sY+L2ZXWhmT5rZfRbdK+EnQ2v2B+XLg/sHg8X5NprZ82ZWGZTPCx6vNrOvjLKV8xxvLFZYaGZ/MLN1Ft034OqgzteBeUGr4xtB3c8Hr7PJzP5+DN9GmQCUIGQi+lfg2+5+FvAnwF1B+cvABe6+lOhKq/8Qc855wEfd/eLg8VLgVuAUYC6wIs7rFADPu/ti4GngL2Je/1+D1z/mOjvBOkSXEJ3pDtADXOvuy4juQfLNIEH9NbDT3Ze4++fN7FJgAXA2sAQ408wuONbriQzRYn0yEb0TOCVmBc5iMysCSoD/NrMFRFfQzI45Z6W7x+4F8KK71wGY2Qaia/SsGvY6fbyx+OFa4F3B/fN4Y43/e4H/N0Kc+THPvZbongEQXaPnH4Iv+wjRlkVlnPMvDW7rg8eFRBPG0yO8nsgRlCBkIsoAznP37thCM/s34Al3vza4nv9kzOGuYc/RG3N/kPj/l/r9jU6+keocTbe7LzGzEqKJ5tPAd4juJVEBnOnu/WZWC+TFOd+Af3T37x/n64oAusQkE9Pvie7FAICZDS3XXALUB/evD/H1nyd6aQvgQ8eq7O7tRLcovd3MsonGuT9IDhcBs4OqnUBRzKmPAR8L9i3AzKrMbOoY/Q4yAShByHg3yczqYm6fI/pluzzouN1KdJl2gH8G/tHMngEyQ4zpVuBzZvYiMB1oP9YJ7r6e6IqhHyK6ac9yM1tDtDXxclCnBXgmGBb7DXf/PdFLWM+Z2WbgPo5MICJHpWGuIgkW7IrX7e5uZh8C/tTdrz7WeSKJpj4IkcQ7E/j3YORRGyFu8SpyItSCEBGRuNQHISIicSlBiIhIXEoQIiISlxKEiIjEpQQhIiJx/X87GIlAVDS5KgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "! rm models/bestmodel.pth\n",
    "learner.lr_find()\n",
    "learner.recorder.plot(suggestion=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>👉🏻LMAE👈🏻</th>\n",
       "      <th>lmae0</th>\n",
       "      <th>lmae1</th>\n",
       "      <th>lmae2</th>\n",
       "      <th>lmae3</th>\n",
       "      <th>lmae4</th>\n",
       "      <th>lmae5</th>\n",
       "      <th>lmae6</th>\n",
       "      <th>lmae7</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.871488</td>\n",
       "      <td>1.777467</td>\n",
       "      <td>1.787293</td>\n",
       "      <td>4.541058</td>\n",
       "      <td>1.880430</td>\n",
       "      <td>3.825547</td>\n",
       "      <td>1.015847</td>\n",
       "      <td>0.958708</td>\n",
       "      <td>1.157054</td>\n",
       "      <td>0.926036</td>\n",
       "      <td>-0.006338</td>\n",
       "      <td>00:48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.700884</td>\n",
       "      <td>1.614116</td>\n",
       "      <td>1.618546</td>\n",
       "      <td>4.532626</td>\n",
       "      <td>0.899160</td>\n",
       "      <td>3.782544</td>\n",
       "      <td>0.898254</td>\n",
       "      <td>0.912347</td>\n",
       "      <td>1.057976</td>\n",
       "      <td>0.900807</td>\n",
       "      <td>-0.035343</td>\n",
       "      <td>00:48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.585725</td>\n",
       "      <td>1.501187</td>\n",
       "      <td>1.502357</td>\n",
       "      <td>4.516291</td>\n",
       "      <td>0.558055</td>\n",
       "      <td>3.691849</td>\n",
       "      <td>0.701876</td>\n",
       "      <td>0.811505</td>\n",
       "      <td>0.958622</td>\n",
       "      <td>0.861290</td>\n",
       "      <td>-0.080632</td>\n",
       "      <td>00:48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.451697</td>\n",
       "      <td>1.363583</td>\n",
       "      <td>1.362760</td>\n",
       "      <td>4.491962</td>\n",
       "      <td>0.284218</td>\n",
       "      <td>3.530538</td>\n",
       "      <td>0.405305</td>\n",
       "      <td>0.701384</td>\n",
       "      <td>0.877089</td>\n",
       "      <td>0.818098</td>\n",
       "      <td>-0.206514</td>\n",
       "      <td>00:48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.321470</td>\n",
       "      <td>1.254046</td>\n",
       "      <td>1.252887</td>\n",
       "      <td>4.447087</td>\n",
       "      <td>0.144995</td>\n",
       "      <td>3.342573</td>\n",
       "      <td>0.199850</td>\n",
       "      <td>0.594682</td>\n",
       "      <td>0.809281</td>\n",
       "      <td>0.766077</td>\n",
       "      <td>-0.281448</td>\n",
       "      <td>00:48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.204014</td>\n",
       "      <td>1.151332</td>\n",
       "      <td>1.151180</td>\n",
       "      <td>4.404368</td>\n",
       "      <td>-0.029107</td>\n",
       "      <td>3.208508</td>\n",
       "      <td>0.031266</td>\n",
       "      <td>0.503474</td>\n",
       "      <td>0.746399</td>\n",
       "      <td>0.728503</td>\n",
       "      <td>-0.383967</td>\n",
       "      <td>00:48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.105726</td>\n",
       "      <td>1.086226</td>\n",
       "      <td>1.086976</td>\n",
       "      <td>4.372825</td>\n",
       "      <td>-0.111237</td>\n",
       "      <td>3.101904</td>\n",
       "      <td>-0.064417</td>\n",
       "      <td>0.437614</td>\n",
       "      <td>0.705858</td>\n",
       "      <td>0.698274</td>\n",
       "      <td>-0.445012</td>\n",
       "      <td>00:48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.017542</td>\n",
       "      <td>1.029209</td>\n",
       "      <td>1.030631</td>\n",
       "      <td>4.352117</td>\n",
       "      <td>-0.182596</td>\n",
       "      <td>3.028710</td>\n",
       "      <td>-0.168264</td>\n",
       "      <td>0.384484</td>\n",
       "      <td>0.669828</td>\n",
       "      <td>0.673984</td>\n",
       "      <td>-0.513211</td>\n",
       "      <td>00:48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.944522</td>\n",
       "      <td>0.997295</td>\n",
       "      <td>0.999218</td>\n",
       "      <td>4.342843</td>\n",
       "      <td>-0.223280</td>\n",
       "      <td>2.993183</td>\n",
       "      <td>-0.226469</td>\n",
       "      <td>0.358447</td>\n",
       "      <td>0.645820</td>\n",
       "      <td>0.658854</td>\n",
       "      <td>-0.555657</td>\n",
       "      <td>00:48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.897411</td>\n",
       "      <td>0.988178</td>\n",
       "      <td>0.990026</td>\n",
       "      <td>4.339897</td>\n",
       "      <td>-0.234063</td>\n",
       "      <td>2.987078</td>\n",
       "      <td>-0.248316</td>\n",
       "      <td>0.351829</td>\n",
       "      <td>0.639549</td>\n",
       "      <td>0.655014</td>\n",
       "      <td>-0.570783</td>\n",
       "      <td>00:48</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 0 with 👉🏻LMAE👈🏻 value: 1.787292718887329.\n",
      "Better model found at epoch 1 with 👉🏻LMAE👈🏻 value: 1.6185462474822998.\n",
      "Better model found at epoch 2 with 👉🏻LMAE👈🏻 value: 1.502356767654419.\n",
      "Better model found at epoch 3 with 👉🏻LMAE👈🏻 value: 1.3627599477767944.\n",
      "Better model found at epoch 4 with 👉🏻LMAE👈🏻 value: 1.2528871297836304.\n",
      "Better model found at epoch 5 with 👉🏻LMAE👈🏻 value: 1.151180386543274.\n",
      "Better model found at epoch 6 with 👉🏻LMAE👈🏻 value: 1.086976170539856.\n",
      "Better model found at epoch 7 with 👉🏻LMAE👈🏻 value: 1.030631422996521.\n",
      "Better model found at epoch 8 with 👉🏻LMAE👈🏻 value: 0.9992177486419678.\n",
      "Better model found at epoch 9 with 👉🏻LMAE👈🏻 value: 0.990025520324707.\n"
     ]
    }
   ],
   "source": [
    "learner.fit_one_cycle(10, 8e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.recorder.plot_losses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learner.recorder.plot_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional Fine tune regular fit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learner.callbacks.append(\n",
    "    ReduceLROnPlateauCallback(learner, monitor='train_loss', mode='min', factor=0.2, patience=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learner.fit(100, 5e-5)# , moms=(0.75,0.70))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learner.fit_one_cycle(300, 5e-4)#, moms=(0.75,0.70))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learner.recorder.plot_lr(show_moms=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learner.recorder.plot_losses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learner.recorder.plot_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learner.fit(10,1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learner.to_fp32()\n",
    "val = learner.validate()[1]\n",
    "print(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    sub_fname = f'loss{learner.recorder.losses[-1]:.04f}val{val:.04f}'\n",
    "except:\n",
    "    sub_fname = f'val{val:.04f}'\n",
    "learner.save(sub_fname)\n",
    "print(sub_fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference\n",
    "\n",
    "Make sure `tranforms` are activated to test set otherwise TTA > 1 will be as TTA =1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    xt_coulombmat = load_fn(f'xt_coulombmat32{ext}.npy')\n",
    "except:\n",
    "    xt_coulombmat = np.load(f'xt_coulombmat{ext}.npy', allow_pickle=True)\n",
    "    xt_coulombmat = np.array(xt_coulombmat.tolist()).astype(np.float32)\n",
    "    np.save(f'xt_coulombmat32{ext}.npy', xt_coulombmat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_fname = Path('test.npz')\n",
    "try:\n",
    "    npzfile  = np.load(fname_ext(test_fname, ext))\n",
    "    xt_xyz   = npzfile['x_xyz']\n",
    "    xt_type  = npzfile['x_type']\n",
    "    xt_ext   = npzfile['x_ext']\n",
    "    xt_atom  = npzfile['x_atom']\n",
    "    mt = npzfile['m']\n",
    "    xt_ids = npzfile['x_ids']\n",
    "except:\n",
    "    xt_xyz,xt_type,xt_ext,xt_atom,mt,xt_ids = \\\n",
    "        preprocess(test_fname.with_suffix('.csv'), type_index=types,ext=ext)\n",
    "    np.savez(fname_ext('_'+test_fname, ext), \n",
    "             x_xyz  = xt_xyz,\n",
    "             x_type = xt_type,\n",
    "             x_ext  = xt_ext,\n",
    "             x_atom = xt_atom,\n",
    "             m=mt,\n",
    "             x_ids=xt_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xt_qm9_mulliken = load_fn(f'xt_qm9_mulliken{ext}.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "[v.shape for v in [xt_xyz,xt_type,xt_ext,xt_atom, xt_qm9_mulliken,xt_ids, mt]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learner.data.add_test(ItemList(items=(MoleculeItem(i,*v) for i,v in \n",
    "                              enumerate(zip(xt_xyz,xt_type,xt_ext,xt_atom,xt_qm9_mulliken,xt_coulombmat)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TTA_N = 1\n",
    "learner.data.test_ds.tfms = tta_tfms if TTA_N > 1 else tfms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#learner.model = learner.model.module\n",
    "#data.batch_size = 4096*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#learner.model = nn.DataParallel(learner.model)\n",
    "old_bs = data.batch_size\n",
    "data.batch_size *= 2\n",
    "\n",
    "sub = defaultdict(int)\n",
    "xt_ids_not_extended = (xt_ids!=0) & (xt_ids<=7163688) # TODO\n",
    "ids = xt_ids[xt_ids_not_extended]\n",
    "\n",
    "mb = master_bar(range(TTA_N))\n",
    "for tta in mb:\n",
    "    test_preds = np.zeros((0, 29), dtype=np.float32)\n",
    "\n",
    "    for batch_idx, batch in progress_bar(\n",
    "        enumerate(learner.dl(DatasetType.Test)), total=len(learner.dl(DatasetType.Test)), parent=mb):\n",
    "        _, _, preds_,_,_,_ = learner.pred_batch(ds_type=DatasetType.Test, batch=batch)\n",
    "        preds_ = preds_.sum(dim=1)\n",
    "        test_preds = np.concatenate([test_preds, preds_.data.cpu().numpy()], axis = 0)\n",
    "\n",
    "    preds = test_preds[xt_ids_not_extended]\n",
    "    for k in range(len(ids)):\n",
    "        sub[int(ids[k])] += preds[k]\n",
    "    \n",
    "for k in range(len(ids)):\n",
    "    sub[int(ids[k])] = sub[int(ids[k])]/TTA_N\n",
    "\n",
    "learner.model = learner.model.module\n",
    "data.batch_size = old_bs\n",
    "\n",
    "sub_df = pd.DataFrame(sub.items(), columns=['id', 'scalar_coupling_constant'])\n",
    "sub_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submit to Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "csv_fname = f'{sub_fname}_tta{TTA_N}.csv'\n",
    "sub_df.to_csv(csv_fname, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "comp = 'champs-scalar-coupling'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!kaggle competitions submit -c {comp} -f {csv_fname} -m 'QM9 tta {TTA_N} {ext}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "time.sleep(60)\n",
    "!kaggle competitions submissions -c {comp} -v > submissions-{comp}.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submissions = pd.read_csv(f'submissions-{comp}.csv')\n",
    "submissions.iloc[0].publicScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (root)",
   "language": "python",
   "name": "root"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
