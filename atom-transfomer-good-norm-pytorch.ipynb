{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%load_ext autoreload\n",
    "#%autoreload 2\n",
    "import os\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1,2\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.utils.data\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from RAdam.radam import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.nn import Parameter\n",
    "\n",
    "def _load_from_state_dict(self, state_dict, prefix, local_metadata, strict,\n",
    "                          missing_keys, unexpected_keys, error_msgs):\n",
    "    r\"\"\"Copies parameters and buffers from :attr:`state_dict` into only\n",
    "    this module, but not its descendants. This is called on every submodule\n",
    "    in :meth:`~torch.nn.Module.load_state_dict`. Metadata saved for this\n",
    "    module in input :attr:`state_dict` is provided as :attr:`local_metadata`.\n",
    "    For state dicts without metadata, :attr:`local_metadata` is empty.\n",
    "    Subclasses can achieve class-specific backward compatible loading using\n",
    "    the version number at `local_metadata.get(\"version\", None)`.\n",
    "\n",
    "    .. note::\n",
    "        :attr:`state_dict` is not the same object as the input\n",
    "        :attr:`state_dict` to :meth:`~torch.nn.Module.load_state_dict`. So\n",
    "        it can be modified.\n",
    "\n",
    "    Arguments:\n",
    "        state_dict (dict): a dict containing parameters and\n",
    "            persistent buffers.\n",
    "        prefix (str): the prefix for parameters and buffers used in this\n",
    "            module\n",
    "        local_metadata (dict): a dict containing the metadata for this module.\n",
    "            See\n",
    "        strict (bool): whether to strictly enforce that the keys in\n",
    "            :attr:`state_dict` with :attr:`prefix` match the names of\n",
    "            parameters and buffers in this module\n",
    "        missing_keys (list of str): if ``strict=True``, add missing keys to\n",
    "            this list\n",
    "        unexpected_keys (list of str): if ``strict=True``, add unexpected\n",
    "            keys to this list\n",
    "        error_msgs (list of str): error messages should be added to this\n",
    "            list, and will be reported together in\n",
    "            :meth:`~torch.nn.Module.load_state_dict`\n",
    "    \"\"\"\n",
    "    for hook in self._load_state_dict_pre_hooks.values():\n",
    "        hook(state_dict, prefix, local_metadata, strict, missing_keys,\n",
    "             unexpected_keys, error_msgs)\n",
    "\n",
    "    local_name_params = itertools.chain(self._parameters.items(),\n",
    "                                        self._buffers.items())\n",
    "    local_state = {k: v.data for k, v in local_name_params if v is not None}\n",
    "\n",
    "    for name, param in local_state.items():\n",
    "        key = prefix + name\n",
    "        if key in state_dict:\n",
    "            input_param = state_dict[key]\n",
    "\n",
    "            # Backward compatibility: loading 1-dim tensor from 0.3.* to version 0.4+\n",
    "            if len(param.shape) == 0 and len(input_param.shape) == 1:\n",
    "                input_param = input_param[0]\n",
    "\n",
    "            if input_param.shape != param.shape:\n",
    "                # local shape should match the one in checkpoint\n",
    "                error_msgs.append(\n",
    "                    'size mismatch for {}: copying a param with shape {} from checkpoint, '\n",
    "                    'the shape in current model is {}.'.format(\n",
    "                        key, input_param.shape, param.shape))\n",
    "                #if not strict:\n",
    "                #    continue\n",
    "\n",
    "            if isinstance(input_param, Parameter):\n",
    "                # backwards compatibility for serialized parameters\n",
    "                input_param = input_param.data\n",
    "\n",
    "            try:\n",
    "                param.copy_(input_param)\n",
    "            except Exception:\n",
    "                error_msgs.append(\n",
    "                    'While copying the parameter named \"{}\", '\n",
    "                    'whose dimensions in the model are {} and '\n",
    "                    'whose dimensions in the checkpoint are {}.'.format(\n",
    "                        key, param.size(), input_param.size()))\n",
    "                # PG load partially\n",
    "\n",
    "                if len(input_param.size()) == 3:\n",
    "                    error_msgs.append(\n",
    "                        'Partially copying the parameter named \"{}\", '\n",
    "                        'whose dimensions in the model are {} and '\n",
    "                        'whose dimensions in the checkpoint are {}. - trying {}'\n",
    "                        .format(\n",
    "                            key, param.size(), input_param.size(),\n",
    "                            param[:input_param.size()[0], :input_param.size(\n",
    "                            )[1], :input_param.size()[2]].shape))\n",
    "                else:\n",
    "                    error_msgs.append(\n",
    "                        'Partially copying the parameter named \"{}\", '\n",
    "                        'whose dimensions in the model are {} and '\n",
    "                        'whose dimensions in the checkpoint are {}. - trying {}'\n",
    "                        .format(key, param.size(), input_param.size(),\n",
    "                                param[:input_param.size()[0]].shape))\n",
    "\n",
    "                try:\n",
    "                    new_input_param = torch.empty_like(param)\n",
    "                    new_input_param = torch.nn.init.normal_(new_input_param,\n",
    "                                                            mean=input_param.mean(),\n",
    "                                                            std=input_param.std())\n",
    "\n",
    "                    if len(input_param.size()) == 3:\n",
    "                        new_input_param[:input_param.size()[0], :input_param.\n",
    "                                        size()[1], :input_param.size(\n",
    "                                        )[2]] = input_param\n",
    "                    else:\n",
    "                        new_input_param[:input_param.size()[0]] = input_param\n",
    "                    param.copy_(new_input_param)\n",
    "                except Exception as e:\n",
    "                    assert e\n",
    "                    error_msgs.append(\n",
    "                        'Failed to load weights partially {}'.format(e))\n",
    "        elif strict:\n",
    "            missing_keys.append(key)\n",
    "\n",
    "    if strict:\n",
    "        for key in state_dict.keys():\n",
    "            if key.startswith(prefix):\n",
    "                input_name = key[len(prefix):]\n",
    "                input_name = input_name.split(\n",
    "                    '.', 1)[0]  # get the name of param/buffer/child\n",
    "                if input_name not in self._modules and input_name not in local_state:\n",
    "                    unexpected_keys.append(key)\n",
    "\n",
    "def load_state_dict(self, state_dict, strict=True):\n",
    "    r\"\"\"Copies parameters and buffers from :attr:`state_dict` into\n",
    "    this module and its descendants. If :attr:`strict` is ``True``, then\n",
    "    the keys of :attr:`state_dict` must exactly match the keys returned\n",
    "    by this module's :meth:`~torch.nn.Module.state_dict` function.\n",
    "\n",
    "    Arguments:\n",
    "        state_dict (dict): a dict containing parameters and\n",
    "            persistent buffers.\n",
    "        strict (bool, optional): whether to strictly enforce that the keys\n",
    "            in :attr:`state_dict` match the keys returned by this module's\n",
    "            :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\n",
    "\n",
    "    Returns:\n",
    "        ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n",
    "            * **missing_keys** is a list of str containing the missing keys\n",
    "            * **unexpected_keys** is a list of str containing the unexpected keys\n",
    "    \"\"\"\n",
    "    missing_keys = []\n",
    "    unexpected_keys = []\n",
    "    error_msgs = []\n",
    "\n",
    "    # copy state_dict so _load_from_state_dict can modify it\n",
    "    metadata = getattr(state_dict, '_metadata', None)\n",
    "    state_dict = state_dict.copy()\n",
    "    if metadata is not None:\n",
    "        state_dict._metadata = metadata\n",
    "\n",
    "    def load(module, prefix=''):\n",
    "        local_metadata = {} if metadata is None else metadata.get(\n",
    "            prefix[:-1], {})\n",
    "        module._load_from_state_dict(state_dict, prefix, local_metadata, True,\n",
    "                                     missing_keys, unexpected_keys, error_msgs)\n",
    "        for name, child in module._modules.items():\n",
    "            if child is not None:\n",
    "                load(child, prefix + name + '.')\n",
    "                \n",
    "    load(self)\n",
    "\n",
    "    if strict:\n",
    "        if len(unexpected_keys) > 0:\n",
    "            error_msgs.insert(\n",
    "                0, 'Unexpected key(s) in state_dict: {}. '.format(', '.join(\n",
    "                    '\"{}\"'.format(k) for k in unexpected_keys)))\n",
    "        if len(missing_keys) > 0:\n",
    "            error_msgs.insert(\n",
    "                0, 'Missing key(s) in state_dict: {}. '.format(', '.join(\n",
    "                    '\"{}\"'.format(k) for k in missing_keys)))\n",
    "\n",
    "    if strict and len(error_msgs) > 0:\n",
    "        raise RuntimeError(\n",
    "            'Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n",
    "                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nn.Module._load_from_state_dict = _load_from_state_dict\n",
    "nn.Module.load_state_dict = load_state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from fastai import *\n",
    "from fastai.basic_train import *\n",
    "from fastai.basic_data import *\n",
    "from fastai.data_block import *\n",
    "from fastai.torch_core import *\n",
    "from fastai.train import *\n",
    "from fastai.callback import *\n",
    "from fastai.callbacks import *\n",
    "from fastai.distributed import *\n",
    "from fastai.layers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# deterministic\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fname_ext = lambda fname,ext: f'{str(fname)[:-4]}{ext}{str(fname)[-4:]}'\n",
    "\n",
    "def preprocess(fname, type_index=None, ext=''):\n",
    "    t  = pd.read_csv(fname_ext(fname,ext))\n",
    "    s  = pd.read_csv('structures.csv')\n",
    "    \n",
    "    has_y = 'scalar_coupling_constant' in t.columns\n",
    "\n",
    "    if has_y:\n",
    "        # atom-atom level\n",
    "        # molecule_name,atom_index_0,atom_index_1,type,fc,sd,pso,dso\n",
    "        scalar_couplings = pd.read_csv(f'scalar_coupling_contributions{ext}.csv') # fc,sd,pso,dso\n",
    "\n",
    "        # atom level\n",
    "        # molecule_name,atom_index,XX,YX,ZX,XY,YY,ZY,XZ,YZ,ZZ\n",
    "        magnetic_shielding = pd.read_csv('magnetic_shielding_tensors.csv')\n",
    "        # molecule_name,atom_index,mulliken_charge\n",
    "        mulliken_charges = pd.read_csv('mulliken_charges.csv')\n",
    "\n",
    "        # molecule level\n",
    "        # molecule_name,X,Y,Z\n",
    "        dipole_moments = pd.read_csv('dipole_moments.csv')\n",
    "        # molecule_name,potential_energy\n",
    "        potential_energy = pd.read_csv('potential_energy.csv')\n",
    "\n",
    "    t['molecule_index'] = pd.factorize(t['molecule_name'])[0] + t['id'].min()\n",
    "    # make sure we use the same indexes in train/test (test needs to provide type_index)\n",
    "    if type_index is not None:\n",
    "        t['type_idx'] = t['type'].apply(lambda x: type_index.index(x) ) # pd.factorize(pd.concat([pd.Series(type_index),t['type']]))[0][len(type_index):]\n",
    "    else:\n",
    "        t['type_idx'] = pd.factorize(t['type'])[0]\n",
    "\n",
    "    s['atom_idx'] = s['atom'].apply(lambda x: atoms.index(x) )\n",
    "\n",
    "    max_items = len(t.groupby(['molecule_name', 'atom_index_0']))# if has_y else 422550\n",
    "    max_atoms = int(s.atom_index.max() + 1)\n",
    "\n",
    "    if has_y:\n",
    "        contributions = ['fc','sd','pso','dso']\n",
    "        magnetic_tensors = ['XX','YX','ZX','XY','YY','ZY','XZ','YZ','ZZ']\n",
    "        XYZ = ['X','Y','Z']\n",
    "    xyz = ['x', 'y', 'z']\n",
    "    \n",
    "    x_xyz   = np.zeros((max_items,len(xyz),  max_atoms), dtype=np.float32)\n",
    "    x_type  = np.zeros((max_items,1,         max_atoms), dtype=np.int8)\n",
    "    x_ext   = np.zeros((max_items,1,         max_atoms), dtype=np.bool_)\n",
    "    x_atom  = np.empty((max_items,1,         max_atoms), dtype=np.int8)\n",
    "    x_atom[:] = -1\n",
    "\n",
    "    if has_y:\n",
    "        y_scalar   = np.zeros((max_items,len(contributions)   ,max_atoms), dtype=np.float32)\n",
    "        y_magnetic = np.zeros((max_items,len(magnetic_tensors),max_atoms), dtype=np.float32)\n",
    "        y_mulliken = np.zeros((max_items,1                    ,max_atoms), dtype=np.float32)\n",
    "\n",
    "        y_dipole   = np.zeros((max_items,len(XYZ)), dtype=np.float32)\n",
    "        y_potential= np.zeros((max_items,1       ), dtype=np.float32)\n",
    "\n",
    "        y_magnetic[...] = np.nan\n",
    "        y_mulliken[...] = np.nan\n",
    "    else:\n",
    "        xt_ids = np.zeros((max_items, max_atoms), dtype=np.int32)\n",
    "\n",
    "    m = np.zeros((max_items,), dtype=np.int32)\n",
    "    i = j = 0\n",
    "    \n",
    "    for (m_name, m_index) ,m_group in tqdm(t.groupby(['molecule_name', 'molecule_index'])):\n",
    "        ss = s[s.molecule_name==m_name]\n",
    "        n_atoms = len(ss)\n",
    "        if has_y:\n",
    "            magnetic = magnetic_shielding[\n",
    "                    (magnetic_shielding['molecule_name']==m_name)][magnetic_tensors].values.T\n",
    "\n",
    "            mulliken = mulliken_charges[\n",
    "                    (mulliken_charges['molecule_name']==m_name)]['mulliken_charge'].values.T\n",
    "\n",
    "            scs = scalar_couplings[scalar_couplings['molecule_name']==m_name]\n",
    "            \n",
    "            y_dipole[j,:]= dipole_moments[dipole_moments['molecule_name']==m_name][XYZ].values\n",
    "            y_potential[j,:]=potential_energy[\n",
    "                potential_energy['molecule_name']==m_name]['potential_energy'].values\n",
    "        \n",
    "        for a_name,a_group in m_group.groupby('atom_index_0'):\n",
    "            \n",
    "            ref_a = ss[ss['atom_index']==a_name]\n",
    "            \n",
    "            x_xyz[i] = 0.\n",
    "            x_type[i] = -1\n",
    "            x_ext[i] =  True\n",
    "            \n",
    "            x_xyz[i,:,:n_atoms] = (ss[xyz].values-ref_a[xyz].values).T  # xyz \n",
    "            x_type[i,0,a_group['atom_index_1']] = a_group['type_idx']  # type \n",
    "            x_ext[i,0,a_group['atom_index_1']] = a_group['ext']  # ext \n",
    "            x_atom[i,:,:n_atoms] = ss['atom_idx'].T                \n",
    "\n",
    "            if has_y:\n",
    "                y_scalar[i,:,a_group['atom_index_1']] = scs[scs['atom_index_0']==a_name][contributions]\n",
    "                y_magnetic[i,:,:n_atoms] = magnetic\n",
    "                y_mulliken[i,:,:n_atoms] = mulliken\n",
    "            else:\n",
    "                xt_ids[i,a_group['atom_index_1']] = a_group['id']  \n",
    "\n",
    "            m[i] = m_index\n",
    "            i+=1\n",
    "        j += 1\n",
    "    assert i == max_items\n",
    "    print(i,max_items)\n",
    "    if has_y:\n",
    "        return x_xyz,x_type,x_ext,x_atom, m, y_scalar, y_magnetic, y_mulliken, y_dipole, y_potential\n",
    "    else:\n",
    "        return x_xyz,x_type,x_ext,x_atom, m, xt_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define where you want to use original training set '' or extended ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ext = '_ext' # or ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load preprocessed or preprocess and save for later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_fname = Path('train.npz')\n",
    "types = ['1JHC', '2JHH', '1JHN', '2JHN', '2JHC', '3JHH', '3JHC', '3JHN'] \n",
    "atoms = 'CFHNO'\n",
    "\n",
    "try:\n",
    "    npzfile = np.load(fname_ext(train_fname, ext))\n",
    "    x_xyz   = npzfile['x_xyz']\n",
    "    x_type  = npzfile['x_type']\n",
    "    x_ext   = npzfile['x_ext']\n",
    "    x_atom  = npzfile['x_atom']\n",
    "\n",
    "    y_scalar    = npzfile['y_scalar']\n",
    "    y_magnetic  = npzfile['y_magnetic']\n",
    "    y_mulliken  = npzfile['y_mulliken']\n",
    "    y_dipole    = npzfile['y_dipole']\n",
    "    y_potential = npzfile['y_potential']\n",
    "    m = npzfile['m']\n",
    "    max_items, max_atoms = x_xyz.shape[0], x_xyz.shape[-1]\n",
    "except:\n",
    "    x_xyz,x_type,x_ext,x_atom, m, y_scalar, y_magnetic, y_mulliken, y_dipole, y_potential = \\\n",
    "        preprocess(train_fname.with_suffix('.csv'), type_index=types, ext=ext)\n",
    "    np.savez(fname_ext(train_fname, ext), \n",
    "             x_xyz=x_xyz,\n",
    "             x_type=x_type,\n",
    "             x_ext=x_ext,\n",
    "             x_atom=x_atom,\n",
    "             y_scalar=y_scalar,\n",
    "             y_magnetic=y_magnetic,\n",
    "             y_mulliken=y_mulliken,\n",
    "             y_dipole=y_dipole,\n",
    "             y_potential=y_potential,\n",
    "             m=m)\n",
    "n_types = int(x_type[~np.isnan(x_type)].max() + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "use_memmap = True\n",
    "load_fn = np.load if not use_memmap else partial(np.lib.format.open_memmap, mode='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_qm9_mulliken = load_fn(f'x_qm9_mulliken{ext}.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1405126, 3, 29),\n",
       " (1405126, 1, 29),\n",
       " (1405126, 1, 29),\n",
       " (1405126, 1, 29),\n",
       " (1405126, 1, 29),\n",
       " (1405126, 4, 29),\n",
       " (1405126, 9, 29),\n",
       " (1405126, 1, 29),\n",
       " (1405126, 3),\n",
       " (1405126, 1),\n",
       " (1405126,)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[v.shape for v in [x_xyz,x_type,x_ext,x_atom,x_qm9_mulliken, \n",
    "                   y_scalar, y_magnetic, y_mulliken, y_dipole, y_potential, m]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1405126, 29)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_atom_mask = (x_atom != -1).swapaxes(1,2).squeeze(-1)\n",
    "real_atom_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_xyz_masked = x_xyz.swapaxes(1,2)[real_atom_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26154396, 3)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_xyz_masked.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# BAD mean/std\n",
    "#x_xyz_mean, x_xyz_std = Tensor(x_xyz.mean(axis=(0,2),keepdims=True)), Tensor(x_xyz.std(axis=(0,2),keepdims=True))\n",
    "#x_xyz_mean = Tensor(x_xyz).mean(dim=(0,2),keepdim=True)\n",
    "#x_xyz_std  = Tensor(x_xyz).std(dim=(0,2), keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GOOD mean/std\n",
    "x_xyz_mean = Tensor(x_xyz_masked).mean(dim=(0),keepdim=True).unsqueeze(-1)\n",
    "x_xyz_std  = Tensor(x_xyz_masked).std(dim=(0), keepdim=True).unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.0094],\n",
       "          [-0.0328],\n",
       "          [ 0.0032]]]), tensor([[[1.8703],\n",
       "          [2.2832],\n",
       "          [1.7753]]]), torch.Size([1, 3, 1]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_xyz_mean, x_xyz_std, x_xyz_std.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#x_xyz = (x_xyz - x_xyz_mean.numpy() ) / x_xyz_std.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_qm9_mulliken_mean = Tensor(x_qm9_mulliken.mean(axis=(0,2),keepdims=True))\n",
    "x_qm9_mulliken_std  = Tensor(x_qm9_mulliken.std( axis=(0,2),keepdims=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 1])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_xyz_std.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fastai classes (this should should be done into its own `application` but who has time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MoleculeItem(ItemBase):\n",
    "    def __init__(self,i,xyz,type,ext,atom,qm9_mulliken): \n",
    "        self.i,self.xyz,self.type,self.ext, self.atom,self.qm9_mulliken = \\\n",
    "            i,xyz,type,ext,atom,qm9_mulliken\n",
    "        self.data = [Tensor(xyz), LongTensor((type)), \n",
    "                     Tensor(ext), LongTensor((atom)),Tensor(qm9_mulliken)]\n",
    "    def __str__(self):\n",
    "        # TODO: count n_atoms correctly. \n",
    "        n_atoms = np.count_nonzero(np.sum(np.absolute(self.xyz), axis=0))+1\n",
    "        n_couplings = np.sum((self.type!=-1))\n",
    "        return f'{self.i} {n_atoms} atoms {n_couplings} couplings'\n",
    "    \n",
    "    def apply_tfms(self, tfms:Collection, **kwargs):\n",
    "        x = self.clone()\n",
    "        for t in tfms:\n",
    "            if t: x.data = t(x.data)\n",
    "        return x\n",
    "    \n",
    "    def clone(self):\n",
    "        return self.__class__(self.i,self.xyz,self.type,self.ext,self.atom,self.qm9_mulliken)\n",
    "    \n",
    "class ScalarCouplingItem(ItemBase):\n",
    "    def __init__(self,scalar,**kwargs): \n",
    "        self.scalar = scalar#,magnetic,mulliken,dipole,potential\n",
    "        self.data = Tensor(scalar) #, Tensor(magnetic), Tensor(dipole), Tensor(potential))\n",
    "    def __str__(self):\n",
    "        #res, spacer, n_couplings = '', '', 0\n",
    "        return f'{self.data}'\n",
    "    def apply_tfms(self, tfms:Collection, **kwargs):\n",
    "        y = self.clone()\n",
    "        for t in tfms:\n",
    "            if 'label_smoothing' == t.__name__:\n",
    "                if t: y.data = t(y.data)\n",
    "        return y\n",
    "    def clone(self): return self.__class__(self.scalar)#,self.magnetic,self.mulliken,self.dipole,self.potential)\n",
    "    def __hash__(self): return hash(str(self))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#y_scalar.mean(axis=(0,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LMAEMaskedLoss(Module):\n",
    "    def __init__(self, contrib_w=0, types_w = [1]*n_types, return_all=False, proxy_log=torch.log, exclude_ext=False):\n",
    "        self.contrib_w = contrib_w\n",
    "        self.types_w = types_w\n",
    "        self.return_all = return_all\n",
    "        self.proxy_log = proxy_log\n",
    "        self.exclude_ext = exclude_ext\n",
    "        self.loss = torch.nn.L1Loss()\n",
    "\n",
    "    def forward(self, from_forward, t_scalar):\n",
    "        valid_couplings_mask,type_masked,ext_masked,p_scalar_masked = from_forward\n",
    "        loss, n, j_loss = 0, 0, [0] * n_types\n",
    "        #p_scalar_masked = p_scalar_masked#.squeeze(-1)\n",
    "        t_scalar_masked = t_scalar[valid_couplings_mask.to(dtype=torch.bool)]\n",
    "        for t in range(n_types):\n",
    "            type_mask = (type_masked == t) if not self.exclude_ext else ((type_masked == t) & (ext_masked == 0))\n",
    "            if type_mask.sum() > 0:\n",
    "                _output,_target = p_scalar_masked[type_mask], t_scalar_masked[type_mask]\n",
    "                s_loss = self.proxy_log(torch.nn.L1Loss()(_output,_target)+1e-9)\n",
    "                loss += self.types_w[t] * s_loss\n",
    "                j_loss[t] += s_loss\n",
    "                n+=1\n",
    "        loss /= n\n",
    "        \n",
    "        return loss if not self.return_all else (loss, *j_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ScalarCouplingList(ItemList):\n",
    "    def __init__(self, items:Iterator, **kwargs):\n",
    "        super().__init__(items, **kwargs)\n",
    "        self.loss_func = LMAEMaskedLoss\n",
    "\n",
    "    def get(self, i):\n",
    "        o = super().get(i)\n",
    "        return ScalarCouplingItem(np.array(o, dtype=np.float32))\n",
    "\n",
    "    def reconstruct(self,t): return 0; # TODO for viz !!!! ScalarCouplingItem(t.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quaterions allow us to rotate 3d points randoming with a nice uniform distribution of 3 numbers hece we use them, however it's still to be seen if are useful here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#https://github.com/facebookresearch/QuaterNet/blob/master/common/quaternion.py\n",
    "def qrot(q, v):\n",
    "    \"\"\"\n",
    "    Rotate vector(s) v about the rotation described by quaternion(s) q.\n",
    "    Expects a tensor of shape (*, 4) for q and a tensor of shape (*, 3) for v,\n",
    "    where * denotes any number of dimensions.\n",
    "    Returns a tensor of shape (*, 3).\n",
    "    \"\"\"\n",
    "    assert q.shape[-1] == 4\n",
    "    assert v.shape[-1] == 3\n",
    "    assert q.shape[:-1] == v.shape[:-1]\n",
    "    \n",
    "    original_shape = list(v.shape)\n",
    "    q = q.view(-1, 4)\n",
    "    v = v.view(-1, 3)\n",
    "    \n",
    "    qvec = q[:, 1:]\n",
    "    uv = torch.cross(qvec, v, dim=1)\n",
    "    uuv = torch.cross(qvec, uv, dim=1)\n",
    "    return (v + 2 * (q[:, :1] * uv + uuv)).view(original_shape)\n",
    "\n",
    "def random_rotation(data):\n",
    "    x_xyz = data[0].transpose(0,1)\n",
    "    r = torch.rand(3)\n",
    "    sq1_v1,sqv1,v2_2pi,v3_2pi = torch.sqrt(1-r[:1]),torch.sqrt(r[:1]),2*math.pi*r[1:2],2*math.pi*r[2:3]\n",
    "    q = torch.cat([sq1_v1*torch.sin(v2_2pi), sq1_v1*torch.cos(v2_2pi), \n",
    "                   sqv1  *torch.sin(v3_2pi), sqv1  *torch.cos(v3_2pi)], dim=0).unsqueeze(0)\n",
    "    x_xyz = qrot(q.expand(x_xyz.shape[0],-1), x_xyz).squeeze(0).transpose(0,1)\n",
    "    return (x_xyz, *data[1:])\n",
    "\n",
    "def normalize(data):\n",
    "    sq = False\n",
    "    if data[0].ndim < 3:\n",
    "        data[0].unsqueeze_(0)\n",
    "        data[4].unsqueeze_(0)\n",
    "        sq = True\n",
    "    x_xyz      = (data[0] - x_xyz_mean)          / x_xyz_std\n",
    "    x_mulliken = (data[4] - x_qm9_mulliken_mean) / x_qm9_mulliken_std\n",
    "    if sq:\n",
    "        x_xyz.squeeze_(0)\n",
    "        x_mulliken.squeeze_(0)\n",
    "    return (x_xyz, data[1],data[2],data[3],x_mulliken)\n",
    "\n",
    "def canonize(data):\n",
    "    xyz,type,ext,atom,mulliken = data\n",
    "    mask = (atom == -1).squeeze(0)\n",
    "    i_max_atom = torch.nonzero(atom != -1).max() + 1\n",
    "    mask_atoms = torch.ones ((max_atoms, ), dtype=torch.uint8)\n",
    "    zeros      = torch.zeros((i_max_atom,), dtype=torch.uint8)\n",
    "    mask_atoms[:zeros.shape[0],] = zeros\n",
    "    n_atoms = i_max_atom\n",
    "    xyz[:,mask], type[:,mask],ext[:,mask],atom[:,mask],mulliken[:,mask] = 0,-1,1,-1,0\n",
    "    return (xyz,type,ext,atom,mulliken, mask_atoms, n_atoms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "THRESHOLD=0.05\n",
    "def label_smoothing(y):\n",
    "    r = y[0] + torch.zeros_like(y[0]).uniform_(-THRESHOLD, THRESHOLD).masked_fill(y[0]==0, 0.)\n",
    "    return (r, y[1], y[2], y[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build `data` bunch etc. for fastai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = ItemList(items=(MoleculeItem(i,*v) for i,v in \n",
    "                       enumerate(zip(x_xyz,x_type,x_ext,x_atom,x_qm9_mulliken))),label_cls=ScalarCouplingItem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "_, idx_valid_split = train_test_split(range(m.max()+1), test_size=0.1, random_state=13)\n",
    "idx_valid_split = np.argwhere(np.isin(m, idx_valid_split)).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true,
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [],
   "source": [
    "data = data.split_by_idx(idx_valid_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_scalar[0].sum(axis=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def l(o): return np.array(y_scalar[o.i].sum(axis=0), dtype=np.float32)\n",
    "data = data.label_from_func(func=l,label_cls=ScalarCouplingList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfms = [normalize, canonize]\n",
    "tta_tfms = list(tfms)\n",
    "#tta_tfms.insert(0,random_rotation)\n",
    "data = data.transform((tta_tfms, tfms))#.transform_y(([label_smoothing], None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data=data.databunch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelList (78217 items)\n",
       "x: ItemList\n",
       "16 8 atoms 7 couplings,17 8 atoms 7 couplings,18 8 atoms 7 couplings,19 8 atoms 7 couplings,20 8 atoms 7 couplings\n",
       "y: ScalarCouplingList\n",
       "tensor([ 83.5430,  -2.3783,   0.0000, -11.7004, -11.6979,   3.2528,  13.6913,\n",
       "          3.2521,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "          0.0000]),tensor([ 83.5417,  -2.3786, -11.7004,   0.0000, -11.6996,  13.6924,   3.2525,\n",
       "          3.2527,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "          0.0000]),tensor([ 83.5484,  -2.3772, -11.6979, -11.6996,   0.0000,   3.2524,   3.2524,\n",
       "         13.6921,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "          0.0000]),tensor([ -2.3788,  83.5418,   3.2528,  13.6924,   3.2524,   0.0000, -11.7004,\n",
       "        -11.6993,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "          0.0000]),tensor([ -2.3785,  83.5430,  13.6913,   3.2525,   3.2524, -11.7004,   0.0000,\n",
       "        -11.6976,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "          0.0000])\n",
       "Path: ."
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove ext\n",
    "data.train_ds.filter_by_func(lambda item, _: len(item.data[2].cpu().numpy()[item.data[2].cpu().numpy()==0]) == 0)\n",
    "data.valid_ds.filter_by_func(lambda item, _: len(item.data[2].cpu().numpy()[item.data[2].cpu().numpy()==0]) == 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "Whole model here, self-contained (needs some cleanup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LMAEMetric(LearnerCallback):\n",
    "    _order=-20 # Needs to run before the recorder\n",
    "    def __init__(self, learn, val_only=True):\n",
    "        super().__init__(learn)\n",
    "        self.val_only=val_only\n",
    "        self.metric = LMAEMaskedLoss(return_all=True, exclude_ext=True)\n",
    "\n",
    "    def on_train_begin(self, **kwargs):\n",
    "        if not self.val_only: self.learn.recorder.add_metric_names(['tLMAE'])\n",
    "        self.learn.recorder.add_metric_names(['👉🏻LMAE👈🏻'] + [f'{i} {types[i]}' for i in range(n_types)])\n",
    "            \n",
    "    def on_batch_end(self, train, last_output, last_target, **kwargs):\n",
    "        if self.val_only and train: return \n",
    "        preds,targs = self.preds[int(train)], self.targs[int(train)] # 0 val 1 train\n",
    "        \n",
    "        if preds is None:\n",
    "            targs, preds = last_target.detach().cpu(),[t.detach().cpu() for t in listify(last_output)]\n",
    "        else:\n",
    "            targs = torch.cat([targs,last_target.detach().cpu()], dim=0)\n",
    "            for i,(o,t) in enumerate(zip(last_output, last_target)):\n",
    "                preds[i] = torch.cat([preds[i], o.detach().cpu()], dim=0)\n",
    "        self.preds[int(train)], self.targs[int(train)] = preds,targs\n",
    "        \n",
    "    def on_epoch_begin(self, **kwargs):\n",
    "        self.targs, self.preds = [None, None], [None, None]\n",
    "\n",
    "    def on_epoch_end(self, last_metrics, **kwargs):\n",
    "        if True: #(kwargs['epoch'] % max_atoms) == 0:\n",
    "            mets = []\n",
    "            if self.preds[1]: mets.append(self.metric.forward(self.preds[1], self.targs[1])[0]) # just tLMAE\n",
    "            if self.preds[0]: \n",
    "                mets.extend(self.metric.forward(self.preds[0], self.targs[0]))\n",
    "            return add_metrics(last_metrics, mets) if mets else None\n",
    "        else: return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DummyDecoder(Module):\n",
    "    def __init__(self,dropout:float=0):\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self,tgt, memory, tgt_mask=None, memory_mask=None, \n",
    "                tgt_key_padding_mask=None, memory_key_padding_mask=None):\n",
    "        return self.dropout(memory)\n",
    "\n",
    "class AtomTorchTransformer(Module):\n",
    "    def __init__(self,n_layers,n_heads,d_model,d_inner,embed_p:float=0,\n",
    "                 encoder_dropout:float=0,decoder_dropout:float=0,\n",
    "                 d_head=None,deep_decoder=False,dense_out=False, **kwargs):\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        d_head = ifnone(d_head, d_model//n_heads)        \n",
    "        \n",
    "        self.transformer = nn.Transformer(num_encoder_layers=n_layers,\n",
    "                                          nhead=n_heads,d_model=d_model,dim_feedforward=d_inner,\n",
    "                                          dropout = encoder_dropout, \n",
    "                                          custom_decoder=DummyDecoder(dropout=decoder_dropout))\n",
    "        \n",
    "        channels_out = d_model*n_layers if dense_out else d_model\n",
    "        channels_out_scalar = channels_out + n_types + 1\n",
    "        if deep_decoder:\n",
    "            sl = [int(channels_out_scalar/(2**d)) for d in range(int(math.ceil(np.log2(channels_out_scalar/4)-1)))]\n",
    "            self.scalar = nn.Sequential(*(list(itertools.chain.from_iterable(\n",
    "                [[nn.Conv1d(sl[i],sl[i+1],1),nn.ReLU(),nn.BatchNorm1d(sl[i+1])] for i in range(len(sl)-1)])) + \n",
    "                [nn.Conv1d(sl[-1], 1, 1)]))\n",
    "        else:\n",
    "            self.scalar = nn.Conv1d(channels_out_scalar, 1, 1)\n",
    "        \n",
    "        n_atom_embedding = d_model//2\n",
    "        n_type_embedding = d_model - n_atom_embedding - 3 #- 1 - 1 -1 \n",
    "        self.type_embedding = nn.Embedding(len(types)+1,n_type_embedding)\n",
    "        self.atom_embedding = nn.Embedding(len(atoms)+1,n_atom_embedding)\n",
    "        self.drop_type, self.drop_atom = nn.Dropout(embed_p), nn.Dropout(embed_p)\n",
    "            \n",
    "    def forward(self,xyz,type,ext,atom,mulliken,mask_atoms,n_atoms):\n",
    "        bs, _, n_pts = xyz.shape        \n",
    "        t = self.drop_type(self.type_embedding((type+1).squeeze(1)))\n",
    "        a = self.drop_atom(self.atom_embedding((atom+1).squeeze(1)))\n",
    "        \n",
    "        x = xyz\n",
    "        x = torch.cat([x.transpose(1,2), t, a], dim=-1) * math.sqrt(self.d_model) # B,N(29),d_model\n",
    "\n",
    "        mask = mask_atoms.to(dtype=torch.bool) \n",
    "        x = x.transpose(0,1)\n",
    "        x = self.transformer(x, x, src_key_padding_mask=mask).permute(1,2,0)\n",
    "            \n",
    "        t_one_hot = torch.zeros(bs,n_types+1,n_pts,device=type.device,dtype=x.dtype).scatter_(1,type+1, 1.)\n",
    "        scalar    = self.scalar(torch.cat([x, t_one_hot], dim=1)).squeeze(1)\n",
    "        \n",
    "        type = type.squeeze(1)\n",
    "        ext  = ext.squeeze(1)\n",
    "        valid_couplings_mask = (type != -1)\n",
    "        return valid_couplings_mask,type[valid_couplings_mask],ext[valid_couplings_mask],scalar[valid_couplings_mask]\n",
    "                    \n",
    "    def reset(self): pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This callback allows to insert multiple stateful (not averaged) metrics in one pass. Addditionally we could add metrics for train if we want to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model instantiation: where's all your TPUs/GPUs when you need a decent hyperparam sweep?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ShowGraph(LearnerCallback):\n",
    "    \"Update a graph of learner stats and metrics after each epoch.\"\n",
    "    def on_epoch_end(self, n_epochs:int, last_metrics:MetricsList, **kwargs)->bool:\n",
    "        \"If we have `last_metrics` plot them in our pbar graph\"\n",
    "        if last_metrics is not None and last_metrics[0] is not None:\n",
    "            rec = self.learn.recorder\n",
    "            iters = range_of(rec.losses)\n",
    "            val_iter = np.array(rec.nb_batches).cumsum()\n",
    "            x_bounds = (0, (n_epochs - len(rec.nb_batches)) * rec.nb_batches[-1] + len(rec.losses))\n",
    "            y_bounds = (min((min(Tensor(rec.losses)), min(Tensor(rec.val_losses)))), \n",
    "                        max((max(Tensor(rec.losses)), max(Tensor(rec.val_losses)))))\n",
    "            rec.pbar.update_graph([(iters, rec.losses), (val_iter, rec.val_losses)], x_bounds, y_bounds)\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "net, learner = None,None\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "n_layers=24 #6\n",
    "n_heads=16 #4\n",
    "d_model=256 #2048\n",
    "d_inner=2048*4 # 2048*2\n",
    "decoder_dropout=0.0\n",
    "\n",
    "deep_decoder = False\n",
    "dense_out    = False\n",
    "\n",
    "net = AtomTorchTransformer(n_layers=n_layers, n_heads=n_heads,d_model=d_model,d_inner=d_inner,\n",
    "                      embed_p=0, encoder_dropout=0., decoder_dropout=decoder_dropout,\n",
    "                      deep_decoder=deep_decoder, dense_out=dense_out)\n",
    "\n",
    "learner = Learner(data,net,loss_func=LMAEMaskedLoss(),callback_fns=ShowGraph).to_fp32()\n",
    "learner.callbacks.extend([\n",
    "    SaveModelCallback(learner, monitor='👉🏻LMAE👈🏻', mode='min'),\n",
    "    LMAEMetric(learner)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AtomTorchTransformer\n",
       "======================================================================\n",
       "Layer (type)         Output Shape         Param #    Trainable \n",
       "======================================================================\n",
       "Linear               [1, 8192]            2,105,344  True      \n",
       "______________________________________________________________________\n",
       "Dropout              [1, 8192]            0          False     \n",
       "______________________________________________________________________\n",
       "Linear               [1, 256]             2,097,408  True      \n",
       "______________________________________________________________________\n",
       "LayerNorm            [1, 256]             512        True      \n",
       "______________________________________________________________________\n",
       "LayerNorm            [1, 256]             512        True      \n",
       "______________________________________________________________________\n",
       "Dropout              [1, 256]             0          False     \n",
       "______________________________________________________________________\n",
       "Dropout              [1, 256]             0          False     \n",
       "______________________________________________________________________\n",
       "Linear               [1, 8192]            2,105,344  True      \n",
       "______________________________________________________________________\n",
       "Dropout              [1, 8192]            0          False     \n",
       "______________________________________________________________________\n",
       "Linear               [1, 256]             2,097,408  True      \n",
       "______________________________________________________________________\n",
       "LayerNorm            [1, 256]             512        True      \n",
       "______________________________________________________________________\n",
       "LayerNorm            [1, 256]             512        True      \n",
       "______________________________________________________________________\n",
       "Dropout              [1, 256]             0          False     \n",
       "______________________________________________________________________\n",
       "Dropout              [1, 256]             0          False     \n",
       "______________________________________________________________________\n",
       "Linear               [1, 8192]            2,105,344  True      \n",
       "______________________________________________________________________\n",
       "Dropout              [1, 8192]            0          False     \n",
       "______________________________________________________________________\n",
       "Linear               [1, 256]             2,097,408  True      \n",
       "______________________________________________________________________\n",
       "LayerNorm            [1, 256]             512        True      \n",
       "______________________________________________________________________\n",
       "LayerNorm            [1, 256]             512        True      \n",
       "______________________________________________________________________\n",
       "Dropout              [1, 256]             0          False     \n",
       "______________________________________________________________________\n",
       "Dropout              [1, 256]             0          False     \n",
       "______________________________________________________________________\n",
       "Linear               [1, 8192]            2,105,344  True      \n",
       "______________________________________________________________________\n",
       "Dropout              [1, 8192]            0          False     \n",
       "______________________________________________________________________\n",
       "Linear               [1, 256]             2,097,408  True      \n",
       "______________________________________________________________________\n",
       "LayerNorm            [1, 256]             512        True      \n",
       "______________________________________________________________________\n",
       "LayerNorm            [1, 256]             512        True      \n",
       "______________________________________________________________________\n",
       "Dropout              [1, 256]             0          False     \n",
       "______________________________________________________________________\n",
       "Dropout              [1, 256]             0          False     \n",
       "______________________________________________________________________\n",
       "Linear               [1, 8192]            2,105,344  True      \n",
       "______________________________________________________________________\n",
       "Dropout              [1, 8192]            0          False     \n",
       "______________________________________________________________________\n",
       "Linear               [1, 256]             2,097,408  True      \n",
       "______________________________________________________________________\n",
       "LayerNorm            [1, 256]             512        True      \n",
       "______________________________________________________________________\n",
       "LayerNorm            [1, 256]             512        True      \n",
       "______________________________________________________________________\n",
       "Dropout              [1, 256]             0          False     \n",
       "______________________________________________________________________\n",
       "Dropout              [1, 256]             0          False     \n",
       "______________________________________________________________________\n",
       "Linear               [1, 8192]            2,105,344  True      \n",
       "______________________________________________________________________\n",
       "Dropout              [1, 8192]            0          False     \n",
       "______________________________________________________________________\n",
       "Linear               [1, 256]             2,097,408  True      \n",
       "______________________________________________________________________\n",
       "LayerNorm            [1, 256]             512        True      \n",
       "______________________________________________________________________\n",
       "LayerNorm            [1, 256]             512        True      \n",
       "______________________________________________________________________\n",
       "Dropout              [1, 256]             0          False     \n",
       "______________________________________________________________________\n",
       "Dropout              [1, 256]             0          False     \n",
       "______________________________________________________________________\n",
       "Linear               [1, 8192]            2,105,344  True      \n",
       "______________________________________________________________________\n",
       "Dropout              [1, 8192]            0          False     \n",
       "______________________________________________________________________\n",
       "Linear               [1, 256]             2,097,408  True      \n",
       "______________________________________________________________________\n",
       "LayerNorm            [1, 256]             512        True      \n",
       "______________________________________________________________________\n",
       "LayerNorm            [1, 256]             512        True      \n",
       "______________________________________________________________________\n",
       "Dropout              [1, 256]             0          False     \n",
       "______________________________________________________________________\n",
       "Dropout              [1, 256]             0          False     \n",
       "______________________________________________________________________\n",
       "Linear               [1, 8192]            2,105,344  True      \n",
       "______________________________________________________________________\n",
       "Dropout              [1, 8192]            0          False     \n",
       "______________________________________________________________________\n",
       "Linear               [1, 256]             2,097,408  True      \n",
       "______________________________________________________________________\n",
       "LayerNorm            [1, 256]             512        True      \n",
       "______________________________________________________________________\n",
       "LayerNorm            [1, 256]             512        True      \n",
       "______________________________________________________________________\n",
       "Dropout              [1, 256]             0          False     \n",
       "______________________________________________________________________\n",
       "Dropout              [1, 256]             0          False     \n",
       "______________________________________________________________________\n",
       "Linear               [1, 8192]            2,105,344  True      \n",
       "______________________________________________________________________\n",
       "Dropout              [1, 8192]            0          False     \n",
       "______________________________________________________________________\n",
       "Linear               [1, 256]             2,097,408  True      \n",
       "______________________________________________________________________\n",
       "LayerNorm            [1, 256]             512        True      \n",
       "______________________________________________________________________\n",
       "LayerNorm            [1, 256]             512        True      \n",
       "______________________________________________________________________\n",
       "Dropout              [1, 256]             0          False     \n",
       "______________________________________________________________________\n",
       "Dropout              [1, 256]             0          False     \n",
       "______________________________________________________________________\n",
       "Linear               [1, 8192]            2,105,344  True      \n",
       "______________________________________________________________________\n",
       "Dropout              [1, 8192]            0          False     \n",
       "______________________________________________________________________\n",
       "Linear               [1, 256]             2,097,408  True      \n",
       "______________________________________________________________________\n",
       "LayerNorm            [1, 256]             512        True      \n",
       "______________________________________________________________________\n",
       "LayerNorm            [1, 256]             512        True      \n",
       "______________________________________________________________________\n",
       "Dropout              [1, 256]             0          False     \n",
       "______________________________________________________________________\n",
       "Dropout              [1, 256]             0          False     \n",
       "______________________________________________________________________\n",
       "Linear               [1, 8192]            2,105,344  True      \n",
       "______________________________________________________________________\n",
       "Dropout              [1, 8192]            0          False     \n",
       "______________________________________________________________________\n",
       "Linear               [1, 256]             2,097,408  True      \n",
       "______________________________________________________________________\n",
       "LayerNorm            [1, 256]             512        True      \n",
       "______________________________________________________________________\n",
       "LayerNorm            [1, 256]             512        True      \n",
       "______________________________________________________________________\n",
       "Dropout              [1, 256]             0          False     \n",
       "______________________________________________________________________\n",
       "Dropout              [1, 256]             0          False     \n",
       "______________________________________________________________________\n",
       "Linear               [1, 8192]            2,105,344  True      \n",
       "______________________________________________________________________\n",
       "Dropout              [1, 8192]            0          False     \n",
       "______________________________________________________________________\n",
       "Linear               [1, 256]             2,097,408  True      \n",
       "______________________________________________________________________\n",
       "LayerNorm            [1, 256]             512        True      \n",
       "______________________________________________________________________\n",
       "LayerNorm            [1, 256]             512        True      \n",
       "______________________________________________________________________\n",
       "Dropout              [1, 256]             0          False     \n",
       "______________________________________________________________________\n",
       "Dropout              [1, 256]             0          False     \n",
       "______________________________________________________________________\n",
       "Linear               [1, 8192]            2,105,344  True      \n",
       "______________________________________________________________________\n",
       "Dropout              [1, 8192]            0          False     \n",
       "______________________________________________________________________\n",
       "Linear               [1, 256]             2,097,408  True      \n",
       "______________________________________________________________________\n",
       "LayerNorm            [1, 256]             512        True      \n",
       "______________________________________________________________________\n",
       "LayerNorm            [1, 256]             512        True      \n",
       "______________________________________________________________________\n",
       "Dropout              [1, 256]             0          False     \n",
       "______________________________________________________________________\n",
       "Dropout              [1, 256]             0          False     \n",
       "______________________________________________________________________\n",
       "Linear               [1, 8192]            2,105,344  True      \n",
       "______________________________________________________________________\n",
       "Dropout              [1, 8192]            0          False     \n",
       "______________________________________________________________________\n",
       "Linear               [1, 256]             2,097,408  True      \n",
       "______________________________________________________________________\n",
       "LayerNorm            [1, 256]             512        True      \n",
       "______________________________________________________________________\n",
       "LayerNorm            [1, 256]             512        True      \n",
       "______________________________________________________________________\n",
       "Dropout              [1, 256]             0          False     \n",
       "______________________________________________________________________\n",
       "Dropout              [1, 256]             0          False     \n",
       "______________________________________________________________________\n",
       "Linear               [1, 8192]            2,105,344  True      \n",
       "______________________________________________________________________\n",
       "Dropout              [1, 8192]            0          False     \n",
       "______________________________________________________________________\n",
       "Linear               [1, 256]             2,097,408  True      \n",
       "______________________________________________________________________\n",
       "LayerNorm            [1, 256]             512        True      \n",
       "______________________________________________________________________\n",
       "LayerNorm            [1, 256]             512        True      \n",
       "______________________________________________________________________\n",
       "Dropout              [1, 256]             0          False     \n",
       "______________________________________________________________________\n",
       "Dropout              [1, 256]             0          False     \n",
       "______________________________________________________________________\n",
       "Linear               [1, 8192]            2,105,344  True      \n",
       "______________________________________________________________________\n",
       "Dropout              [1, 8192]            0          False     \n",
       "______________________________________________________________________\n",
       "Linear               [1, 256]             2,097,408  True      \n",
       "______________________________________________________________________\n",
       "LayerNorm            [1, 256]             512        True      \n",
       "______________________________________________________________________\n",
       "LayerNorm            [1, 256]             512        True      \n",
       "______________________________________________________________________\n",
       "Dropout              [1, 256]             0          False     \n",
       "______________________________________________________________________\n",
       "Dropout              [1, 256]             0          False     \n",
       "______________________________________________________________________\n",
       "Linear               [1, 8192]            2,105,344  True      \n",
       "______________________________________________________________________\n",
       "Dropout              [1, 8192]            0          False     \n",
       "______________________________________________________________________\n",
       "Linear               [1, 256]             2,097,408  True      \n",
       "______________________________________________________________________\n",
       "LayerNorm            [1, 256]             512        True      \n",
       "______________________________________________________________________\n",
       "LayerNorm            [1, 256]             512        True      \n",
       "______________________________________________________________________\n",
       "Dropout              [1, 256]             0          False     \n",
       "______________________________________________________________________\n",
       "Dropout              [1, 256]             0          False     \n",
       "______________________________________________________________________\n",
       "Linear               [1, 8192]            2,105,344  True      \n",
       "______________________________________________________________________\n",
       "Dropout              [1, 8192]            0          False     \n",
       "______________________________________________________________________\n",
       "Linear               [1, 256]             2,097,408  True      \n",
       "______________________________________________________________________\n",
       "LayerNorm            [1, 256]             512        True      \n",
       "______________________________________________________________________\n",
       "LayerNorm            [1, 256]             512        True      \n",
       "______________________________________________________________________\n",
       "Dropout              [1, 256]             0          False     \n",
       "______________________________________________________________________\n",
       "Dropout              [1, 256]             0          False     \n",
       "______________________________________________________________________\n",
       "Linear               [1, 8192]            2,105,344  True      \n",
       "______________________________________________________________________\n",
       "Dropout              [1, 8192]            0          False     \n",
       "______________________________________________________________________\n",
       "Linear               [1, 256]             2,097,408  True      \n",
       "______________________________________________________________________\n",
       "LayerNorm            [1, 256]             512        True      \n",
       "______________________________________________________________________\n",
       "LayerNorm            [1, 256]             512        True      \n",
       "______________________________________________________________________\n",
       "Dropout              [1, 256]             0          False     \n",
       "______________________________________________________________________\n",
       "Dropout              [1, 256]             0          False     \n",
       "______________________________________________________________________\n",
       "Linear               [1, 8192]            2,105,344  True      \n",
       "______________________________________________________________________\n",
       "Dropout              [1, 8192]            0          False     \n",
       "______________________________________________________________________\n",
       "Linear               [1, 256]             2,097,408  True      \n",
       "______________________________________________________________________\n",
       "LayerNorm            [1, 256]             512        True      \n",
       "______________________________________________________________________\n",
       "LayerNorm            [1, 256]             512        True      \n",
       "______________________________________________________________________\n",
       "Dropout              [1, 256]             0          False     \n",
       "______________________________________________________________________\n",
       "Dropout              [1, 256]             0          False     \n",
       "______________________________________________________________________\n",
       "Linear               [1, 8192]            2,105,344  True      \n",
       "______________________________________________________________________\n",
       "Dropout              [1, 8192]            0          False     \n",
       "______________________________________________________________________\n",
       "Linear               [1, 256]             2,097,408  True      \n",
       "______________________________________________________________________\n",
       "LayerNorm            [1, 256]             512        True      \n",
       "______________________________________________________________________\n",
       "LayerNorm            [1, 256]             512        True      \n",
       "______________________________________________________________________\n",
       "Dropout              [1, 256]             0          False     \n",
       "______________________________________________________________________\n",
       "Dropout              [1, 256]             0          False     \n",
       "______________________________________________________________________\n",
       "Linear               [1, 8192]            2,105,344  True      \n",
       "______________________________________________________________________\n",
       "Dropout              [1, 8192]            0          False     \n",
       "______________________________________________________________________\n",
       "Linear               [1, 256]             2,097,408  True      \n",
       "______________________________________________________________________\n",
       "LayerNorm            [1, 256]             512        True      \n",
       "______________________________________________________________________\n",
       "LayerNorm            [1, 256]             512        True      \n",
       "______________________________________________________________________\n",
       "Dropout              [1, 256]             0          False     \n",
       "______________________________________________________________________\n",
       "Dropout              [1, 256]             0          False     \n",
       "______________________________________________________________________\n",
       "Linear               [1, 8192]            2,105,344  True      \n",
       "______________________________________________________________________\n",
       "Dropout              [1, 8192]            0          False     \n",
       "______________________________________________________________________\n",
       "Linear               [1, 256]             2,097,408  True      \n",
       "______________________________________________________________________\n",
       "LayerNorm            [1, 256]             512        True      \n",
       "______________________________________________________________________\n",
       "LayerNorm            [1, 256]             512        True      \n",
       "______________________________________________________________________\n",
       "Dropout              [1, 256]             0          False     \n",
       "______________________________________________________________________\n",
       "Dropout              [1, 256]             0          False     \n",
       "______________________________________________________________________\n",
       "Linear               [1, 8192]            2,105,344  True      \n",
       "______________________________________________________________________\n",
       "Dropout              [1, 8192]            0          False     \n",
       "______________________________________________________________________\n",
       "Linear               [1, 256]             2,097,408  True      \n",
       "______________________________________________________________________\n",
       "LayerNorm            [1, 256]             512        True      \n",
       "______________________________________________________________________\n",
       "LayerNorm            [1, 256]             512        True      \n",
       "______________________________________________________________________\n",
       "Dropout              [1, 256]             0          False     \n",
       "______________________________________________________________________\n",
       "Dropout              [1, 256]             0          False     \n",
       "______________________________________________________________________\n",
       "LayerNorm            [1, 256]             512        True      \n",
       "______________________________________________________________________\n",
       "Dropout              [1, 256]             0          False     \n",
       "______________________________________________________________________\n",
       "Conv1d               [1, 29]              266        True      \n",
       "______________________________________________________________________\n",
       "Embedding            [29, 125]            1,125      True      \n",
       "______________________________________________________________________\n",
       "Embedding            [29, 128]            768        True      \n",
       "______________________________________________________________________\n",
       "Dropout              [29, 125]            0          False     \n",
       "______________________________________________________________________\n",
       "Dropout              [29, 128]            0          False     \n",
       "______________________________________________________________________\n",
       "\n",
       "Total params: 100,893,295\n",
       "Total trainable params: 100,893,295\n",
       "Total non-trainable params: 0\n",
       "Optimized with 'torch.optim.adam.Adam', betas=(0.9, 0.99)\n",
       "Using true weight decay as discussed in https://www.fast.ai/2018/07/02/adam-weight-decay/ \n",
       "Loss function : LMAEMaskedLoss\n",
       "======================================================================\n",
       "Callbacks functions applied \n",
       "    SaveModelCallback\n",
       "    LMAEMetric"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#learner.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sub_fname = \"d_model2048decoder_dropout0.0n_layers6n_heads4d_inner4096loss-4.1465val-2.6932\"\n",
    "sub_fname = \"bestmodel_285\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to load: bestmodel_285... Loaded\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print(f\"Attempting to load: {sub_fname}... \", end=\"\")\n",
    "    learner.load(sub_fname, strict=False,with_opt=False)\n",
    "    print(\"Loaded\")\n",
    "except Exception as e:\n",
    "    print(\"NOT loaded! \", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# neural weight manual transplant \n",
    "if False:\n",
    "    for name,child in learner.model.named_children():\n",
    "        print(\"CHILD: \",name)\n",
    "        if not (name in ['scalar', 'atom_embedding', 'type_embedding']):\n",
    "            print(\"FREEZING\")\n",
    "            for param in child.parameters(): param.requires_grad = False\n",
    "        else:\n",
    "            for name,param in child.named_parameters(): \n",
    "                param.requires_grad = True\n",
    "    learner.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from optimizers import Novograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from ranger import Ranger\n",
    "from ralamb import Ralamb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#learner.opt_func = partial(torch.optim.Adam, betas=(0.9,0.99), eps=5e-4)\n",
    "#learner.opt_func = partial(RAdam,eps=1e-8)\n",
    "#learner.opt_func = partial(Novograd,eps=1e-8)\n",
    "learner.opt_func = partial(Ralamb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learner = learner.to_parallel()#.to_fp16()\n",
    "b = 0.2 #0.35\n",
    "data.batch_size = {1: int(b*(1/3)*4096//2), 2: int(b*(2/3)*4096//2), 3: int(b*4096//2)}[torch.cuda.device_count()]\n",
    "data.batch_size *= int(int(any([isinstance(cb, MixedPrecision) for cb in learner.callbacks]))*.8)+1 # 2x if fp16\n",
    "#data.batch_size = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Real loss func. Need to test different auxiliary tasks weights: `magnetic_w`, `dipole_w`, `potential`, weights of indivial `lmae`s: `types_w`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learner.loss_func = LMAEMaskedLoss(types_w = [1] * 8)#,proxy_log=lambda x: x) #\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "losses, lrs = [], []\n",
    "wds = [None] #[1e-1,1e-2,1e-3,1e-4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove 'models/bestmodel.pth': No such file or directory\r\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n",
      "Failed to compute the gradients, there might not be enough points.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for wd in wds:\n",
    "    !rm models/bestmodel.pth\n",
    "    learner.lr_find(wd=wd)#num_it=800, stop_div=False)\n",
    "    learner.recorder.plot(suggestion=True)\n",
    "    losses.append(learner.recorder.losses)\n",
    "    lrs.append(learner.recorder.lrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainingPhase(length=519000)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 300\n",
    "b_its = len(data.train_dl)\n",
    "ph1 = (TrainingPhase(epochs*b_its).schedule_hp('wd', (1e-5,1e-1)))\n",
    "gs = GeneralScheduler(learner, (ph1,))\n",
    "ph1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded bestmodel_285\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>👉🏻LMAE👈🏻</th>\n",
       "      <th>0 1JHC</th>\n",
       "      <th>1 2JHH</th>\n",
       "      <th>2 1JHN</th>\n",
       "      <th>3 2JHN</th>\n",
       "      <th>4 2JHC</th>\n",
       "      <th>5 3JHH</th>\n",
       "      <th>6 3JHC</th>\n",
       "      <th>7 3JHN</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>286</td>\n",
       "      <td>-6.119233</td>\n",
       "      <td>-2.943255</td>\n",
       "      <td>-2.801640</td>\n",
       "      <td>-1.859598</td>\n",
       "      <td>-3.252427</td>\n",
       "      <td>-1.608047</td>\n",
       "      <td>-3.167655</td>\n",
       "      <td>-2.910917</td>\n",
       "      <td>-3.331743</td>\n",
       "      <td>-2.854678</td>\n",
       "      <td>-3.428053</td>\n",
       "      <td>23:41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>287</td>\n",
       "      <td>-6.164610</td>\n",
       "      <td>-2.942879</td>\n",
       "      <td>-2.801363</td>\n",
       "      <td>-1.858822</td>\n",
       "      <td>-3.252594</td>\n",
       "      <td>-1.607903</td>\n",
       "      <td>-3.166720</td>\n",
       "      <td>-2.910921</td>\n",
       "      <td>-3.331652</td>\n",
       "      <td>-2.854765</td>\n",
       "      <td>-3.427530</td>\n",
       "      <td>23:51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>288</td>\n",
       "      <td>-6.221425</td>\n",
       "      <td>-2.943165</td>\n",
       "      <td>-2.801473</td>\n",
       "      <td>-1.858137</td>\n",
       "      <td>-3.252691</td>\n",
       "      <td>-1.607849</td>\n",
       "      <td>-3.167724</td>\n",
       "      <td>-2.910997</td>\n",
       "      <td>-3.331673</td>\n",
       "      <td>-2.854757</td>\n",
       "      <td>-3.427957</td>\n",
       "      <td>23:51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>289</td>\n",
       "      <td>-6.260288</td>\n",
       "      <td>-2.943074</td>\n",
       "      <td>-2.801458</td>\n",
       "      <td>-1.858686</td>\n",
       "      <td>-3.252366</td>\n",
       "      <td>-1.608097</td>\n",
       "      <td>-3.167464</td>\n",
       "      <td>-2.910958</td>\n",
       "      <td>-3.331589</td>\n",
       "      <td>-2.854792</td>\n",
       "      <td>-3.427711</td>\n",
       "      <td>23:52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>-6.336713</td>\n",
       "      <td>-2.943325</td>\n",
       "      <td>-2.801579</td>\n",
       "      <td>-1.859010</td>\n",
       "      <td>-3.252597</td>\n",
       "      <td>-1.607918</td>\n",
       "      <td>-3.167485</td>\n",
       "      <td>-2.910960</td>\n",
       "      <td>-3.331961</td>\n",
       "      <td>-2.854731</td>\n",
       "      <td>-3.427974</td>\n",
       "      <td>23:54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>291</td>\n",
       "      <td>-6.409475</td>\n",
       "      <td>-2.943153</td>\n",
       "      <td>-2.801540</td>\n",
       "      <td>-1.859031</td>\n",
       "      <td>-3.252447</td>\n",
       "      <td>-1.607981</td>\n",
       "      <td>-3.167470</td>\n",
       "      <td>-2.910960</td>\n",
       "      <td>-3.331759</td>\n",
       "      <td>-2.854803</td>\n",
       "      <td>-3.427870</td>\n",
       "      <td>23:54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>292</td>\n",
       "      <td>-6.451546</td>\n",
       "      <td>-2.943289</td>\n",
       "      <td>-2.801628</td>\n",
       "      <td>-1.859272</td>\n",
       "      <td>-3.252625</td>\n",
       "      <td>-1.608118</td>\n",
       "      <td>-3.167526</td>\n",
       "      <td>-2.910999</td>\n",
       "      <td>-3.331874</td>\n",
       "      <td>-2.854799</td>\n",
       "      <td>-3.427815</td>\n",
       "      <td>23:55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>293</td>\n",
       "      <td>-6.521936</td>\n",
       "      <td>-2.943303</td>\n",
       "      <td>-2.801588</td>\n",
       "      <td>-1.859283</td>\n",
       "      <td>-3.252670</td>\n",
       "      <td>-1.607924</td>\n",
       "      <td>-3.167539</td>\n",
       "      <td>-2.911100</td>\n",
       "      <td>-3.331744</td>\n",
       "      <td>-2.854778</td>\n",
       "      <td>-3.427666</td>\n",
       "      <td>23:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>294</td>\n",
       "      <td>-6.570429</td>\n",
       "      <td>-2.943308</td>\n",
       "      <td>-2.801582</td>\n",
       "      <td>-1.859310</td>\n",
       "      <td>-3.252646</td>\n",
       "      <td>-1.607818</td>\n",
       "      <td>-3.167490</td>\n",
       "      <td>-2.910999</td>\n",
       "      <td>-3.331759</td>\n",
       "      <td>-2.854778</td>\n",
       "      <td>-3.427853</td>\n",
       "      <td>23:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>295</td>\n",
       "      <td>-6.635170</td>\n",
       "      <td>-2.943285</td>\n",
       "      <td>-2.801581</td>\n",
       "      <td>-1.859315</td>\n",
       "      <td>-3.252651</td>\n",
       "      <td>-1.607918</td>\n",
       "      <td>-3.167479</td>\n",
       "      <td>-2.910994</td>\n",
       "      <td>-3.331783</td>\n",
       "      <td>-2.854757</td>\n",
       "      <td>-3.427747</td>\n",
       "      <td>23:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>296</td>\n",
       "      <td>-6.694108</td>\n",
       "      <td>-2.943345</td>\n",
       "      <td>-2.801616</td>\n",
       "      <td>-1.859282</td>\n",
       "      <td>-3.252617</td>\n",
       "      <td>-1.608041</td>\n",
       "      <td>-3.167545</td>\n",
       "      <td>-2.910961</td>\n",
       "      <td>-3.331819</td>\n",
       "      <td>-2.854784</td>\n",
       "      <td>-3.427879</td>\n",
       "      <td>23:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>297</td>\n",
       "      <td>-6.750970</td>\n",
       "      <td>-2.943335</td>\n",
       "      <td>-2.801617</td>\n",
       "      <td>-1.859300</td>\n",
       "      <td>-3.252641</td>\n",
       "      <td>-1.607979</td>\n",
       "      <td>-3.167559</td>\n",
       "      <td>-2.910996</td>\n",
       "      <td>-3.331816</td>\n",
       "      <td>-2.854790</td>\n",
       "      <td>-3.427859</td>\n",
       "      <td>24:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>298</td>\n",
       "      <td>-6.787337</td>\n",
       "      <td>-2.943330</td>\n",
       "      <td>-2.801618</td>\n",
       "      <td>-1.859308</td>\n",
       "      <td>-3.252638</td>\n",
       "      <td>-1.608010</td>\n",
       "      <td>-3.167537</td>\n",
       "      <td>-2.910981</td>\n",
       "      <td>-3.331815</td>\n",
       "      <td>-2.854791</td>\n",
       "      <td>-3.427860</td>\n",
       "      <td>23:55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>299</td>\n",
       "      <td>-6.803875</td>\n",
       "      <td>-2.943336</td>\n",
       "      <td>-2.801621</td>\n",
       "      <td>-1.859308</td>\n",
       "      <td>-3.252642</td>\n",
       "      <td>-1.608024</td>\n",
       "      <td>-3.167541</td>\n",
       "      <td>-2.910988</td>\n",
       "      <td>-3.331817</td>\n",
       "      <td>-2.854790</td>\n",
       "      <td>-3.427860</td>\n",
       "      <td>23:56</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 286 with 👉🏻LMAE👈🏻 value: -2.8016397953033447.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learner.fit_one_cycle(epochs,4e-4, pct_start=0.3, start_epoch=286)#,callbacks=gs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learner.recorder.plot_losses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learner.recorder.plot_lr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learner.recorder.plot_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#learner.save(\"bestmodel_flatlineddd_dropout2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lookahead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from itertools import chain\n",
    "import warnings\n",
    "\n",
    "class Lookahead(torch.optim.Optimizer):\n",
    "    def __init__(self, optimizer, k=5, alpha=0.5):\n",
    "        self.optimizer = optimizer\n",
    "        self.k = k\n",
    "        self.alpha = alpha\n",
    "        self.param_groups = self.optimizer.param_groups\n",
    "        self.state = defaultdict(dict)\n",
    "        self.fast_state = self.optimizer.state\n",
    "        for group in self.param_groups:\n",
    "            group[\"counter\"] = 0\n",
    "    \n",
    "    def update(self, group):\n",
    "        for fast in group[\"params\"]:\n",
    "            param_state = self.state[fast]\n",
    "            if \"slow_param\" not in param_state:\n",
    "                param_state[\"slow_param\"] = torch.zeros_like(fast.data)\n",
    "                param_state[\"slow_param\"].copy_(fast.data)\n",
    "            slow = param_state[\"slow_param\"]\n",
    "            slow += (fast.data - slow) * self.alpha\n",
    "            fast.data.copy_(slow)\n",
    "    \n",
    "    def update_lookahead(self):\n",
    "        for group in self.param_groups:\n",
    "            self.update(group)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        loss = self.optimizer.step(closure)\n",
    "        for group in self.param_groups:\n",
    "            if group[\"counter\"] == 0:\n",
    "                self.update(group)\n",
    "            group[\"counter\"] += 1\n",
    "            if group[\"counter\"] >= self.k:\n",
    "                group[\"counter\"] = 0\n",
    "        return loss\n",
    "\n",
    "    def state_dict(self):\n",
    "        fast_state_dict = self.optimizer.state_dict()\n",
    "        slow_state = {\n",
    "            (id(k) if isinstance(k, torch.Tensor) else k): v\n",
    "            for k, v in self.state.items()\n",
    "        }\n",
    "        fast_state = fast_state_dict[\"state\"]\n",
    "        param_groups = fast_state_dict[\"param_groups\"]\n",
    "        return {\n",
    "            \"fast_state\": fast_state,\n",
    "            \"slow_state\": slow_state,\n",
    "            \"param_groups\": param_groups,\n",
    "        }\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        slow_state_dict = {\n",
    "            \"state\": state_dict[\"slow_state\"],\n",
    "            \"param_groups\": state_dict[\"param_groups\"],\n",
    "        }\n",
    "        fast_state_dict = {\n",
    "            \"state\": state_dict[\"fast_state\"],\n",
    "            \"param_groups\": state_dict[\"param_groups\"],\n",
    "        }\n",
    "        super(Lookahead, self).load_state_dict(slow_state_dict)\n",
    "        self.optimizer.load_state_dict(fast_state_dict)\n",
    "        self.fast_state = self.optimizer.state\n",
    "\n",
    "    def add_param_group(self, param_group):\n",
    "        param_group[\"counter\"] = 0\n",
    "        self.optimizer.add_param_group(param_group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional Fine tune regular fit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learner.callbacks.append(\n",
    "    ReduceLROnPlateauCallback(learner, monitor='train_loss', mode='min', factor=0.2, patience=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    base_opt\n",
    "except:\n",
    "    base_opt = learner.opt.opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lookahead = Lookahead(base_opt, k=5, alpha=0.5) # Initialize Lookahead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learner.opt = OptimWrapper(lookahead)\n",
    "learner.opt_func = lookahead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learner.opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learner.fit(10, 4e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learner.fit(50, 3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learner.fit(50, 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learner.fit(50, 8e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learner.fit(10, 6e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learner.fit(10, 5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learner.fit(10, 3e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learner.fit(10, 8e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learner.recorder.plot_lr(show_moms=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learner.recorder.plot_losses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learner.recorder.plot_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learner.fit(10,1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 0 with 👉🏻LMAE👈🏻 value: -2.801621198654175.\n",
      "tensor(-2.8016)\n"
     ]
    }
   ],
   "source": [
    "learner.to_fp32()\n",
    "val = learner.validate()[1]\n",
    "print(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d_model256decoder_dropout0.0n_layers24n_heads16d_inner8192loss-6.8039val-2.8016\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    sub_fname = f'd_model{d_model}decoder_dropout{decoder_dropout}n_layers{n_layers}n_heads{n_heads}d_inner{d_inner}loss{learner.recorder.losses[-1]:.04f}val{val:.04f}'\n",
    "except:\n",
    "    sub_fname = f'd_model{d_model}val{val:.04f}'\n",
    "learner.save(sub_fname)\n",
    "print(sub_fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation predictions (for L2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'d_model256decoder_dropout0.0n_layers24n_heads16d_inner8192loss-6.8039val-2.8016_val'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.data.valid_dl = data.valid_dl.new(shuffle=False)\n",
    "TTA_N =1                                          \n",
    "sub = defaultdict(int)\n",
    "targets = defaultdict(int)\n",
    "targets_types = defaultdict(int)\n",
    "\n",
    "mb = master_bar(range(TTA_N))\n",
    "for tta in mb:\n",
    "    test_preds = np.zeros((0, 29), dtype=np.float32)\n",
    "    test_exts = np.zeros((0, 29), dtype=np.float32)\n",
    "    test_types = np.zeros((0, 29), dtype=np.float32)\n",
    "    test_targets = np.zeros((0, 29), dtype=np.float32)\n",
    "\n",
    "    for batch_idx, batch in progress_bar(\n",
    "        enumerate(learner.dl(DatasetType.Valid)), total=len(learner.dl(DatasetType.Valid)), parent=mb):\n",
    "        true_mask, types_true, ext_true, preds_true = learner.pred_batch(ds_type=DatasetType.Valid, batch=batch)\n",
    "        types_ = torch.empty_like(true_mask, dtype=types_true.dtype)\n",
    "        types_[:] = -1\n",
    "        types_[true_mask] = types_true\n",
    "        ext_ = torch.ones_like(true_mask, dtype=ext_true.dtype)\n",
    "        ext_[true_mask] = ext_true\n",
    "        preds_ = torch.zeros_like(true_mask, dtype=preds_true.dtype)\n",
    "        preds_[true_mask] = preds_true\n",
    "        targets_ = batch[1]\n",
    "                \n",
    "        test_preds   = np.concatenate([test_preds,   preds_.data.cpu().numpy()],   axis = 0)\n",
    "        test_targets = np.concatenate([test_targets, targets_.data.cpu().numpy()], axis = 0)        \n",
    "        test_types   = np.concatenate([test_types,   types_.data.cpu().numpy()],   axis = 0)\n",
    "        test_exts    = np.concatenate([test_exts,    ext_.data.cpu().numpy()],     axis = 0)\n",
    "    \n",
    "    test_preds   = test_preds.flatten()\n",
    "    test_types   = test_types.flatten()\n",
    "    test_targets = test_targets.flatten()\n",
    "    test_exts    = test_exts.flatten()\n",
    "    \n",
    "    mask = (test_types != -1) & (test_exts == 0)\n",
    "    preds = test_preds[mask]\n",
    "    test_targets = test_targets[mask]\n",
    "    test_types = test_types[mask]\n",
    "    for k in range(len(preds)):\n",
    "        sub[k] += preds[k]\n",
    "        targets[k] = test_targets[k]\n",
    "        targets_types[k] = test_types[k]\n",
    "    \n",
    "for k in range(len(sub.keys())):\n",
    "    sub[k] = sub[k]/TTA_N\n",
    "\n",
    "sub_df = pd.DataFrame(sub.items(), columns=['id', 'scalar_coupling_constant'])\n",
    "sub_df.head()\n",
    "sub_df.to_csv(sub_fname + '_val', index=False)\n",
    "sub_fname + '_val'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference\n",
    "\n",
    "Make sure `tranforms` are activated to test set otherwise TTA > 1 will be as TTA =1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_fname = Path('test.npz')\n",
    "try:\n",
    "    npzfile  = np.load(fname_ext(test_fname, ext))\n",
    "    xt_xyz   = npzfile['x_xyz']\n",
    "    xt_type  = npzfile['x_type']\n",
    "    xt_ext   = npzfile['x_ext']\n",
    "    xt_atom  = npzfile['x_atom']\n",
    "    mt = npzfile['m']\n",
    "    xt_ids = npzfile['x_ids']\n",
    "except:\n",
    "    xt_xyz,xt_type,xt_ext,xt_atom,mt,xt_ids = \\\n",
    "        preprocess(test_fname.with_suffix('.csv'), type_index=types,ext=ext)\n",
    "    np.savez(fname_ext('_'+test_fname, ext), \n",
    "             x_xyz  = xt_xyz,\n",
    "             x_type = xt_type,\n",
    "             x_ext  = xt_ext,\n",
    "             x_atom = xt_atom,\n",
    "             m=mt,\n",
    "             x_ids=xt_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xt_qm9_mulliken = load_fn(f'xt_qm9_mulliken{ext}.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(756113, 3, 29),\n",
       " (756113, 1, 29),\n",
       " (756113, 1, 29),\n",
       " (756113, 1, 29),\n",
       " (756113, 1, 29),\n",
       " (756113, 29),\n",
       " (756113,)]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[v.shape for v in [xt_xyz,xt_type,xt_ext,xt_atom, xt_qm9_mulliken,xt_ids, mt]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learner.data.add_test(ItemList(items=(MoleculeItem(i,*v) for i,v in \n",
    "                              enumerate(zip(xt_xyz,xt_type,xt_ext,xt_atom,xt_qm9_mulliken)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TTA_N = 1\n",
    "learner.data.test_ds.tfms = tta_tfms if TTA_N > 1 else tfms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>scalar_coupling_constant</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4658147</td>\n",
       "      <td>17.692743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4658148</td>\n",
       "      <td>196.137146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4658149</td>\n",
       "      <td>5.326140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4658150</td>\n",
       "      <td>196.120132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4658151</td>\n",
       "      <td>17.708519</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  scalar_coupling_constant\n",
       "0  4658147                 17.692743\n",
       "1  4658148                196.137146\n",
       "2  4658149                  5.326140\n",
       "3  4658150                196.120132\n",
       "4  4658151                 17.708519"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_bs = data.batch_size\n",
    "data.batch_size *= 2\n",
    "\n",
    "sub = defaultdict(int)\n",
    "xt_ids_not_extended = (xt_ids!=0) & (xt_ids<=7163688) # TODO\n",
    "ids = xt_ids[xt_ids_not_extended]\n",
    "\n",
    "mb = master_bar(range(TTA_N))\n",
    "for tta in mb:\n",
    "    test_preds = np.zeros((0, 29), dtype=np.float32)\n",
    "\n",
    "    for batch_idx, batch in progress_bar(\n",
    "        enumerate(learner.dl(DatasetType.Test)), total=len(learner.dl(DatasetType.Test)), parent=mb):\n",
    "        true_mask, _, _, preds_true = learner.pred_batch(ds_type=DatasetType.Test, batch=batch)\n",
    "        preds_ = torch.zeros_like(true_mask, dtype=preds_true.dtype)\n",
    "        preds_[true_mask] = preds_true\n",
    "        test_preds = np.concatenate([test_preds, preds_.data.cpu().numpy()], axis = 0)\n",
    "\n",
    "    preds = test_preds[xt_ids_not_extended]\n",
    "    for k in range(len(ids)):\n",
    "        sub[int(ids[k])] += preds[k]\n",
    "    \n",
    "for k in range(len(ids)):\n",
    "    sub[int(ids[k])] = sub[int(ids[k])]/TTA_N\n",
    "\n",
    "data.batch_size = old_bs\n",
    "\n",
    "sub_df = pd.DataFrame(sub.items(), columns=['id', 'scalar_coupling_constant'])\n",
    "sub_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submit to Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "csv_fname = f'{sub_fname}_tta{TTA_N}.csv'\n",
    "sub_df.to_csv(csv_fname, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_fname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "comp = 'champs-scalar-coupling'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!kaggle competitions submit -c {comp} -f {csv_fname} -m 'QM9 tta {TTA_N} {ext}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "time.sleep(60)\n",
    "!kaggle competitions submissions -c {comp} -v > submissions-{comp}.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submissions = pd.read_csv(f'submissions-{comp}.csv')\n",
    "submissions.iloc[0].publicScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (root)",
   "language": "python",
   "name": "root"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
